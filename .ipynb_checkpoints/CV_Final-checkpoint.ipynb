{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDNN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loadmat('./dataset.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__globals__', 'test_data', 'test_label', '__version__', 'train_label', 'train_data']\n"
     ]
    }
   ],
   "source": [
    "print list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 林敬翔 之銘言：\n",
    "跟助教要的 dataset\n",
    "\n",
    "第一維是數量 第二維是時間序 第三維是資料量\n",
    "\n",
    "Label 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ....\n",
    "\n",
    "1-10左手腕 11-20右手腕 21-30左手臂 31-40右手臂\n",
    "\n",
    "220代表frame數\n",
    "\n",
    "我是取一秒25frame\n",
    "\n",
    "動作數量是以動作為單位沒錯 1*220*40表是某一個動作的完整資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下資料維度\n",
    "\n",
    "data: (?, 220, 40) = (幾筆資料, 時間步, 資料維度)\n",
    "\n",
    "label: (?, 1) = (幾筆資料, 動作label)\n",
    "\n",
    "動作 label: 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: __header__\n",
      "not an array\n",
      "key: __globals__\n",
      "not an array\n",
      "key: test_data\n",
      "(130, 220, 40)\n",
      "key: test_label\n",
      "(130, 1)\n",
      "key: __version__\n",
      "not an array\n",
      "key: train_label\n",
      "(600, 1)\n",
      "key: train_data\n",
      "(600, 220, 40)\n"
     ]
    }
   ],
   "source": [
    "for key, value in data.iteritems():\n",
    "    print 'key: %s'%key\n",
    "    if hasattr(value , 'shape'):\n",
    "        print value.shape\n",
    "    else:\n",
    "        print 'not an array'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列資料 + 預測 -> RNN? / HMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Keras CuDNNLSTM\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "import keras\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Flatten, Conv1D, MaxPool1D\n",
    "from keras.layers import LSTM, CuDNNLSTM, RepeatVector, TimeDistributed, Bidirectional\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.utils import to_categorical # one-hot encoding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, actions=40, cell_size=128, lstm_layer_n = 3, conv_filters=64, \n",
    "                conv_kernel=7, pooling_step=2, learning_rate=0.01, dropout_r=0.14, \n",
    "                optimizer=keras.optimizers.RMSprop, clipnorm=1., \n",
    "                tLSTM=LSTM, use_temporal_subsampling=False, gaussian_noise_std=1.):\n",
    "    if lstm_layer_n<1: raise ValueError('lstm_layer_n must >= 1')\n",
    "    optimizer = optimizer(lr=learning_rate, clipnorm=clipnorm)\n",
    "    model = Sequential()\n",
    "    model.add(GaussianNoise(gaussian_noise_std, input_shape=input_shape))\n",
    "    if use_temporal_subsampling:\n",
    "        model.add(Conv1D(filters=conv_filters, kernel_size=conv_kernel, padding='same', activation='relu'))\n",
    "        model.add(MaxPool1D(pool_size=pooling_step))\n",
    "        model.add(Dropout(dropout_r))\n",
    "    for _ in xrange(1, lstm_layer_n):\n",
    "        model.add(#Bidirectional(\n",
    "                                tLSTM(cell_size, return_sequences=True,\n",
    "                                      unit_forget_bias=True, recurrent_regularizer=regularizers.l2(0.001))#,\n",
    "                                #merge_mode='sum'\n",
    "                               )#)\n",
    "        model.add(Dropout(dropout_r))\n",
    "    model.add(#Bidirectional(\n",
    "                            tLSTM(cell_size, return_sequences=False, unit_forget_bias=True, \n",
    "                                  recurrent_regularizer=regularizers.l2(0.001))#,\n",
    "                            #merge_mode='sum'\n",
    "                           )#)\n",
    "    model.add(Dropout(dropout_r))\n",
    "    model.add(Dense(actions, activation='softmax'))\n",
    "    model.compile(\n",
    "            loss = 'categorical_crossentropy',\n",
    "            optimizer=optimizer, metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info for 0 pca: 1.00\n",
      "info for 1 pca: 1.00\n",
      "info for 2 pca: 1.00\n",
      "info for 3 pca: 1.00\n",
      "info for 4 pca: 1.00\n",
      "info for 5 pca: 1.00\n",
      "info for 6 pca: 1.00\n",
      "info for 7 pca: 1.00\n",
      "info for 8 pca: 1.00\n",
      "info for 9 pca: 1.00\n",
      "info for 10 pca: 1.00\n",
      "info for 11 pca: 1.00\n",
      "info for 12 pca: 1.00\n",
      "info for 13 pca: 1.00\n",
      "info for 14 pca: 1.00\n",
      "info for 15 pca: 1.00\n",
      "info for 16 pca: 1.00\n",
      "info for 17 pca: 1.00\n",
      "info for 18 pca: 1.00\n",
      "info for 19 pca: 1.00\n",
      "info for 20 pca: 1.00\n",
      "info for 21 pca: 1.00\n",
      "info for 22 pca: 1.00\n",
      "info for 23 pca: 1.00\n",
      "info for 24 pca: 1.00\n",
      "info for 25 pca: 1.00\n",
      "info for 26 pca: 1.00\n",
      "info for 27 pca: 1.00\n",
      "info for 28 pca: 1.00\n",
      "info for 29 pca: 1.00\n",
      "info for 30 pca: 1.00\n",
      "info for 31 pca: 1.00\n",
      "info for 32 pca: 1.00\n",
      "info for 33 pca: 1.00\n",
      "info for 34 pca: 1.00\n",
      "info for 35 pca: 1.00\n",
      "info for 36 pca: 1.00\n",
      "info for 37 pca: 1.00\n",
      "info for 38 pca: 1.00\n",
      "info for 39 pca: 1.00\n",
      "info for 40 pca: 1.00\n",
      "info for 41 pca: 1.00\n",
      "info for 42 pca: 1.00\n",
      "info for 43 pca: 1.00\n",
      "info for 44 pca: 1.00\n",
      "info for 45 pca: 1.00\n",
      "info for 46 pca: 1.00\n",
      "info for 47 pca: 1.00\n",
      "info for 48 pca: 1.00\n",
      "info for 49 pca: 1.00\n",
      "info for 50 pca: 1.00\n",
      "info for 51 pca: 1.00\n",
      "info for 52 pca: 1.00\n",
      "info for 53 pca: 1.00\n",
      "info for 54 pca: 1.00\n",
      "info for 55 pca: 1.00\n",
      "info for 56 pca: 1.00\n",
      "info for 57 pca: 1.00\n",
      "info for 58 pca: 1.00\n",
      "info for 59 pca: 1.00\n",
      "info for 60 pca: 1.00\n",
      "info for 61 pca: 1.00\n",
      "info for 62 pca: 1.00\n",
      "info for 63 pca: 1.00\n",
      "info for 64 pca: 1.00\n",
      "info for 65 pca: 1.00\n",
      "info for 66 pca: 1.00\n",
      "info for 67 pca: 1.00\n",
      "info for 68 pca: 1.00\n",
      "info for 69 pca: 1.00\n",
      "info for 70 pca: 1.00\n",
      "info for 71 pca: 1.00\n",
      "info for 72 pca: 1.00\n",
      "info for 73 pca: 1.00\n",
      "info for 74 pca: 1.00\n",
      "info for 75 pca: 1.00\n",
      "info for 76 pca: 1.00\n",
      "info for 77 pca: 1.00\n",
      "info for 78 pca: 1.00\n",
      "info for 79 pca: 1.00\n",
      "info for 80 pca: 1.00\n",
      "info for 81 pca: 1.00\n",
      "info for 82 pca: 1.00\n",
      "info for 83 pca: 1.00\n",
      "info for 84 pca: 1.00\n",
      "info for 85 pca: 1.00\n",
      "info for 86 pca: 1.00\n",
      "info for 87 pca: 1.00\n",
      "info for 88 pca: 1.00\n",
      "info for 89 pca: 1.00\n",
      "info for 90 pca: 1.00\n",
      "info for 91 pca: 1.00\n",
      "info for 92 pca: 1.00\n",
      "info for 93 pca: 1.00\n",
      "info for 94 pca: 1.00\n",
      "info for 95 pca: 1.00\n",
      "info for 96 pca: 1.00\n",
      "info for 97 pca: 1.00\n",
      "info for 98 pca: 1.00\n",
      "info for 99 pca: 1.00\n",
      "info for 100 pca: 1.00\n",
      "info for 101 pca: 1.00\n",
      "info for 102 pca: 1.00\n",
      "info for 103 pca: 1.00\n",
      "info for 104 pca: 1.00\n",
      "info for 105 pca: 1.00\n",
      "info for 106 pca: 1.00\n",
      "info for 107 pca: 1.00\n",
      "info for 108 pca: 1.00\n",
      "info for 109 pca: 1.00\n",
      "info for 110 pca: 1.00\n",
      "info for 111 pca: 1.00\n",
      "info for 112 pca: 1.00\n",
      "info for 113 pca: 1.00\n",
      "info for 114 pca: 1.00\n",
      "info for 115 pca: 1.00\n",
      "info for 116 pca: 1.00\n",
      "info for 117 pca: 1.00\n",
      "info for 118 pca: 1.00\n",
      "info for 119 pca: 1.00\n",
      "info for 120 pca: 1.00\n",
      "info for 121 pca: 1.00\n",
      "info for 122 pca: 1.00\n",
      "info for 123 pca: 1.00\n",
      "info for 124 pca: 1.00\n",
      "info for 125 pca: 1.00\n",
      "info for 126 pca: 1.00\n",
      "info for 127 pca: 1.00\n",
      "info for 128 pca: 1.00\n",
      "info for 129 pca: 1.00\n",
      "info for 130 pca: 1.00\n",
      "info for 131 pca: 1.00\n",
      "info for 132 pca: 1.00\n",
      "info for 133 pca: 1.00\n",
      "info for 134 pca: 1.00\n",
      "info for 135 pca: 1.00\n",
      "info for 136 pca: 1.00\n",
      "info for 137 pca: 1.00\n",
      "info for 138 pca: 1.00\n",
      "info for 139 pca: 1.00\n",
      "info for 140 pca: 1.00\n",
      "info for 141 pca: 1.00\n",
      "info for 142 pca: 1.00\n",
      "info for 143 pca: 1.00\n",
      "info for 144 pca: 1.00\n",
      "info for 145 pca: 1.00\n",
      "info for 146 pca: 1.00\n",
      "info for 147 pca: 1.00\n",
      "info for 148 pca: 1.00\n",
      "info for 149 pca: 1.00\n",
      "info for 150 pca: 1.00\n",
      "info for 151 pca: 1.00\n",
      "info for 152 pca: 1.00\n",
      "info for 153 pca: 1.00\n",
      "info for 154 pca: 1.00\n",
      "info for 155 pca: 1.00\n",
      "info for 156 pca: 1.00\n",
      "info for 157 pca: 1.00\n",
      "info for 158 pca: 1.00\n",
      "info for 159 pca: 1.00\n",
      "info for 160 pca: 1.00\n",
      "info for 161 pca: 1.00\n",
      "info for 162 pca: 1.00\n",
      "info for 163 pca: 1.00\n",
      "info for 164 pca: 1.00\n",
      "info for 165 pca: 1.00\n",
      "info for 166 pca: 1.00\n",
      "info for 167 pca: 1.00\n",
      "info for 168 pca: 1.00\n",
      "info for 169 pca: 1.00\n",
      "info for 170 pca: 1.00\n",
      "info for 171 pca: 1.00\n",
      "info for 172 pca: 1.00\n",
      "info for 173 pca: 1.00\n",
      "info for 174 pca: 1.00\n",
      "info for 175 pca: 1.00\n",
      "info for 176 pca: 1.00\n",
      "info for 177 pca: 1.00\n",
      "info for 178 pca: 1.00\n",
      "info for 179 pca: 1.00\n",
      "info for 180 pca: 1.00\n",
      "info for 181 pca: 1.00\n",
      "info for 182 pca: 1.00\n",
      "info for 183 pca: 1.00\n",
      "info for 184 pca: 1.00\n",
      "info for 185 pca: 1.00\n",
      "info for 186 pca: 1.00\n",
      "info for 187 pca: 1.00\n",
      "info for 188 pca: 1.00\n",
      "info for 189 pca: 1.00\n",
      "info for 190 pca: 1.00\n",
      "info for 191 pca: 1.00\n",
      "info for 192 pca: 1.00\n",
      "info for 193 pca: 1.00\n",
      "info for 194 pca: 1.00\n",
      "info for 195 pca: 1.00\n",
      "info for 196 pca: 1.00\n",
      "info for 197 pca: 1.00\n",
      "info for 198 pca: 1.00\n",
      "info for 199 pca: 1.00\n",
      "info for 200 pca: 1.00\n",
      "info for 201 pca: 1.00\n",
      "info for 202 pca: 1.00\n",
      "info for 203 pca: 1.00\n",
      "info for 204 pca: 1.00\n",
      "info for 205 pca: 1.00\n",
      "info for 206 pca: 1.00\n",
      "info for 207 pca: 1.00\n",
      "info for 208 pca: 1.00\n",
      "info for 209 pca: 1.00\n",
      "info for 210 pca: 1.00\n",
      "info for 211 pca: 1.00\n",
      "info for 212 pca: 1.00\n",
      "info for 213 pca: 1.00\n",
      "info for 214 pca: 1.00\n",
      "info for 215 pca: 1.00\n",
      "info for 216 pca: 1.00\n",
      "info for 217 pca: 1.00\n",
      "info for 218 pca: 1.00\n",
      "info for 219 pca: 1.00\n",
      "(600, 220, 12)\n",
      "(130, 220, 12)\n",
      "(600, 12)\n",
      "(130, 12)\n",
      "69.9668046757\n",
      "1.96944773114e-05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = data['train_data'], data['test_data'], data['train_label'], data['test_label']\n",
    "label_enc = OHE(sparse=False) # One-hot encoder, which has attribute [transform, inverse_transform]\n",
    "y_test = label_enc.fit_transform(y_test)\n",
    "y_train = label_enc.transform(y_train)\n",
    "\n",
    "pcas = []\n",
    "n_comp = 12\n",
    "X_train_new = np.zeros((X_train.shape[0], X_train.shape[1], n_comp))\n",
    "X_test_new = np.zeros((X_test.shape[0], X_test.shape[1], n_comp))\n",
    "for i in xrange(X_train.shape[-2]):\n",
    "    pca = PCA(n_components=n_comp).fit(X_train[:,i,:])\n",
    "    pcas.append( (pca, np.sum(pca.explained_variance_ratio_)) )\n",
    "    X_train_new[:,i,:] = pcas[-1][0].transform(X_train[:,i,:])\n",
    "    X_test_new[:,i,:] = pcas[-1][0].transform(X_test[:,i,:])\n",
    "\n",
    "X_train = X_train_new\n",
    "X_test = X_test_new\n",
    "del X_train_new\n",
    "del X_test_new\n",
    "\n",
    "for i, pca in enumerate(pcas):\n",
    "    print 'info for %d pca: %.2f'%(i,pca[1])\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape\n",
    "print X_train.std()\n",
    "print np.abs(X_train).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_1 (GaussianNo (None, 220, 12)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 100)               45600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 46,812\n",
      "Trainable params: 46,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(input_shape=X_test.shape[1:] ,actions=y_test.shape[-1], cell_size=100, lstm_layer_n=1,\n",
    "                    learning_rate=0.0001, dropout_r=0.5,\n",
    "                    tLSTM=CuDNNLSTM if USE_CUDNN else LSTM, use_temporal_subsampling=False,\n",
    "                    gaussian_noise_std=1e-5\n",
    "                   )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 510 samples, validate on 90 samples\n",
      "Epoch 1/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 3.0836 - acc: 0.0588 - val_loss: 2.8879 - val_acc: 0.0444\n",
      "Epoch 2/3000\n",
      "510/510 [==============================] - 0s 660us/step - loss: 3.0116 - acc: 0.0451 - val_loss: 2.8369 - val_acc: 0.0667\n",
      "Epoch 3/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 2.9088 - acc: 0.0647 - val_loss: 2.7924 - val_acc: 0.0889\n",
      "Epoch 4/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 2.8509 - acc: 0.0706 - val_loss: 2.7538 - val_acc: 0.0889\n",
      "Epoch 5/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.8745 - acc: 0.0781Epoch 00005: val_loss improved from inf to 2.71391, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 624us/step - loss: 2.8708 - acc: 0.0765 - val_loss: 2.7139 - val_acc: 0.1111\n",
      "Epoch 6/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 2.8188 - acc: 0.0922 - val_loss: 2.6765 - val_acc: 0.1222\n",
      "Epoch 7/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 2.7305 - acc: 0.0863 - val_loss: 2.6426 - val_acc: 0.1333\n",
      "Epoch 8/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 2.7195 - acc: 0.0980 - val_loss: 2.6103 - val_acc: 0.1333\n",
      "Epoch 9/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 2.6888 - acc: 0.1196 - val_loss: 2.5790 - val_acc: 0.1222\n",
      "Epoch 10/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.6633 - acc: 0.1250Epoch 00010: val_loss improved from 2.71391 to 2.55240, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 699us/step - loss: 2.6675 - acc: 0.1157 - val_loss: 2.5524 - val_acc: 0.1222\n",
      "Epoch 11/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 2.5361 - acc: 0.1314 - val_loss: 2.5224 - val_acc: 0.1333\n",
      "Epoch 12/3000\n",
      "510/510 [==============================] - 0s 625us/step - loss: 2.5849 - acc: 0.1392 - val_loss: 2.4891 - val_acc: 0.1333\n",
      "Epoch 13/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 2.5486 - acc: 0.1392 - val_loss: 2.4590 - val_acc: 0.1333\n",
      "Epoch 14/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 2.5383 - acc: 0.1333 - val_loss: 2.4279 - val_acc: 0.1333\n",
      "Epoch 15/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.3904 - acc: 0.1830Epoch 00015: val_loss improved from 2.55240 to 2.40577, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 651us/step - loss: 2.4088 - acc: 0.1824 - val_loss: 2.4058 - val_acc: 0.1667\n",
      "Epoch 16/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 2.4334 - acc: 0.1725 - val_loss: 2.3747 - val_acc: 0.1889\n",
      "Epoch 17/3000\n",
      "510/510 [==============================] - 0s 637us/step - loss: 2.4146 - acc: 0.1941 - val_loss: 2.3434 - val_acc: 0.1889\n",
      "Epoch 18/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 2.3689 - acc: 0.1902 - val_loss: 2.3177 - val_acc: 0.1889\n",
      "Epoch 19/3000\n",
      "510/510 [==============================] - 0s 660us/step - loss: 2.3437 - acc: 0.1961 - val_loss: 2.2945 - val_acc: 0.2000\n",
      "Epoch 20/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.3435 - acc: 0.2254Epoch 00020: val_loss improved from 2.40577 to 2.27233, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 629us/step - loss: 2.3284 - acc: 0.2314 - val_loss: 2.2723 - val_acc: 0.2444\n",
      "Epoch 21/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 2.2773 - acc: 0.2314 - val_loss: 2.2538 - val_acc: 0.2556\n",
      "Epoch 22/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 2.2219 - acc: 0.2608 - val_loss: 2.2365 - val_acc: 0.2556\n",
      "Epoch 23/3000\n",
      "510/510 [==============================] - 0s 645us/step - loss: 2.2419 - acc: 0.2608 - val_loss: 2.2152 - val_acc: 0.2556\n",
      "Epoch 24/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 2.1973 - acc: 0.2490 - val_loss: 2.1933 - val_acc: 0.2778\n",
      "Epoch 25/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.1893 - acc: 0.2500Epoch 00025: val_loss improved from 2.27233 to 2.17501, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 653us/step - loss: 2.1947 - acc: 0.2471 - val_loss: 2.1750 - val_acc: 0.2778\n",
      "Epoch 26/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 2.1082 - acc: 0.3039 - val_loss: 2.1528 - val_acc: 0.2889\n",
      "Epoch 27/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 2.1178 - acc: 0.3216 - val_loss: 2.1335 - val_acc: 0.2889\n",
      "Epoch 28/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 2.0925 - acc: 0.3000 - val_loss: 2.1188 - val_acc: 0.2889\n",
      "Epoch 29/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 2.0600 - acc: 0.3039 - val_loss: 2.0978 - val_acc: 0.3000\n",
      "Epoch 30/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.0418 - acc: 0.3393Epoch 00030: val_loss improved from 2.17501 to 2.07977, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 636us/step - loss: 2.0314 - acc: 0.3451 - val_loss: 2.0798 - val_acc: 0.3222\n",
      "Epoch 31/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 1.9859 - acc: 0.3431 - val_loss: 2.0593 - val_acc: 0.3222\n",
      "Epoch 32/3000\n",
      "510/510 [==============================] - 0s 650us/step - loss: 2.0260 - acc: 0.3216 - val_loss: 2.0429 - val_acc: 0.3444\n",
      "Epoch 33/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 2.0200 - acc: 0.3353 - val_loss: 2.0269 - val_acc: 0.3556\n",
      "Epoch 34/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 1.9669 - acc: 0.3373 - val_loss: 2.0081 - val_acc: 0.3556\n",
      "Epoch 35/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.9431 - acc: 0.3661Epoch 00035: val_loss improved from 2.07977 to 1.98661, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 614us/step - loss: 1.9418 - acc: 0.3667 - val_loss: 1.9866 - val_acc: 0.4000\n",
      "Epoch 36/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 1.9718 - acc: 0.3294 - val_loss: 1.9710 - val_acc: 0.4000\n",
      "Epoch 37/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 1.9163 - acc: 0.3804 - val_loss: 1.9562 - val_acc: 0.4222\n",
      "Epoch 38/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 1.8770 - acc: 0.3647 - val_loss: 1.9379 - val_acc: 0.4444\n",
      "Epoch 39/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 1.8438 - acc: 0.4137 - val_loss: 1.9221 - val_acc: 0.4556\n",
      "Epoch 40/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.8448 - acc: 0.4464Epoch 00040: val_loss improved from 1.98661 to 1.90231, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 635us/step - loss: 1.8394 - acc: 0.4510 - val_loss: 1.9023 - val_acc: 0.4556\n",
      "Epoch 41/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 1.8387 - acc: 0.4039 - val_loss: 1.8900 - val_acc: 0.4667\n",
      "Epoch 42/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 1.8136 - acc: 0.4216 - val_loss: 1.8771 - val_acc: 0.4667\n",
      "Epoch 43/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 1.8304 - acc: 0.3843 - val_loss: 1.8564 - val_acc: 0.4667\n",
      "Epoch 44/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 1.7508 - acc: 0.4333 - val_loss: 1.8435 - val_acc: 0.4556\n",
      "Epoch 45/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.7583 - acc: 0.4420Epoch 00045: val_loss improved from 1.90231 to 1.83069, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 595us/step - loss: 1.7765 - acc: 0.4314 - val_loss: 1.8307 - val_acc: 0.4778\n",
      "Epoch 46/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 1.7478 - acc: 0.4451 - val_loss: 1.8172 - val_acc: 0.4667\n",
      "Epoch 47/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 1.7152 - acc: 0.4235 - val_loss: 1.8072 - val_acc: 0.4889\n",
      "Epoch 48/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 1.7136 - acc: 0.4529 - val_loss: 1.7937 - val_acc: 0.4889\n",
      "Epoch 49/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 1.7062 - acc: 0.4412 - val_loss: 1.7783 - val_acc: 0.4889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.6680 - acc: 0.4821Epoch 00050: val_loss improved from 1.83069 to 1.76292, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 586us/step - loss: 1.6774 - acc: 0.4745 - val_loss: 1.7629 - val_acc: 0.4889\n",
      "Epoch 51/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 1.6845 - acc: 0.4529 - val_loss: 1.7488 - val_acc: 0.4667\n",
      "Epoch 52/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 1.6480 - acc: 0.4667 - val_loss: 1.7381 - val_acc: 0.4667\n",
      "Epoch 53/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 1.6937 - acc: 0.4529 - val_loss: 1.7236 - val_acc: 0.4778\n",
      "Epoch 54/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 1.6257 - acc: 0.4804 - val_loss: 1.7117 - val_acc: 0.4778\n",
      "Epoch 55/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.6257 - acc: 0.4643Epoch 00055: val_loss improved from 1.76292 to 1.70197, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 588us/step - loss: 1.6311 - acc: 0.4588 - val_loss: 1.7020 - val_acc: 0.4889\n",
      "Epoch 56/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 1.6154 - acc: 0.4549 - val_loss: 1.6910 - val_acc: 0.4889\n",
      "Epoch 57/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 1.5825 - acc: 0.4824 - val_loss: 1.6766 - val_acc: 0.4889\n",
      "Epoch 58/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 1.6034 - acc: 0.4843 - val_loss: 1.6629 - val_acc: 0.4889\n",
      "Epoch 59/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 1.5852 - acc: 0.4882 - val_loss: 1.6540 - val_acc: 0.5000\n",
      "Epoch 60/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.5430 - acc: 0.5246Epoch 00060: val_loss improved from 1.70197 to 1.64235, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 643us/step - loss: 1.5480 - acc: 0.5255 - val_loss: 1.6424 - val_acc: 0.5000\n",
      "Epoch 61/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 1.5619 - acc: 0.5039 - val_loss: 1.6279 - val_acc: 0.5222\n",
      "Epoch 62/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 1.5349 - acc: 0.4941 - val_loss: 1.6244 - val_acc: 0.5111\n",
      "Epoch 63/3000\n",
      "510/510 [==============================] - 0s 546us/step - loss: 1.5424 - acc: 0.5137 - val_loss: 1.6135 - val_acc: 0.5222\n",
      "Epoch 64/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 1.5094 - acc: 0.5275 - val_loss: 1.6009 - val_acc: 0.5333\n",
      "Epoch 65/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.5010 - acc: 0.5045Epoch 00065: val_loss improved from 1.64235 to 1.59253, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 643us/step - loss: 1.4995 - acc: 0.5059 - val_loss: 1.5925 - val_acc: 0.5222\n",
      "Epoch 66/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 1.4355 - acc: 0.5510 - val_loss: 1.5773 - val_acc: 0.5333\n",
      "Epoch 67/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 1.4688 - acc: 0.5353 - val_loss: 1.5646 - val_acc: 0.5222\n",
      "Epoch 68/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 1.4592 - acc: 0.5059 - val_loss: 1.5508 - val_acc: 0.5111\n",
      "Epoch 69/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 1.4518 - acc: 0.5510 - val_loss: 1.5432 - val_acc: 0.5333\n",
      "Epoch 70/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.4573 - acc: 0.5513Epoch 00070: val_loss improved from 1.59253 to 1.53481, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 587us/step - loss: 1.4404 - acc: 0.5588 - val_loss: 1.5348 - val_acc: 0.5333\n",
      "Epoch 71/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 1.4257 - acc: 0.5549 - val_loss: 1.5227 - val_acc: 0.5222\n",
      "Epoch 72/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 1.3977 - acc: 0.5588 - val_loss: 1.5148 - val_acc: 0.5222\n",
      "Epoch 73/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 1.4238 - acc: 0.5529 - val_loss: 1.5064 - val_acc: 0.5333\n",
      "Epoch 74/3000\n",
      "510/510 [==============================] - 0s 561us/step - loss: 1.4055 - acc: 0.5765 - val_loss: 1.4960 - val_acc: 0.5333\n",
      "Epoch 75/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.3763 - acc: 0.5491Epoch 00075: val_loss improved from 1.53481 to 1.48434, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 623us/step - loss: 1.3923 - acc: 0.5353 - val_loss: 1.4843 - val_acc: 0.5333\n",
      "Epoch 76/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 1.4064 - acc: 0.5431 - val_loss: 1.4795 - val_acc: 0.5333\n",
      "Epoch 77/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 1.3851 - acc: 0.5451 - val_loss: 1.4740 - val_acc: 0.5333\n",
      "Epoch 78/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 1.3625 - acc: 0.5588 - val_loss: 1.4630 - val_acc: 0.5333\n",
      "Epoch 79/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 1.4051 - acc: 0.5412 - val_loss: 1.4593 - val_acc: 0.5444\n",
      "Epoch 80/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.3620 - acc: 0.5938Epoch 00080: val_loss improved from 1.48434 to 1.45327, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 606us/step - loss: 1.3592 - acc: 0.5922 - val_loss: 1.4533 - val_acc: 0.5333\n",
      "Epoch 81/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 1.3545 - acc: 0.5529 - val_loss: 1.4428 - val_acc: 0.5444\n",
      "Epoch 82/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 1.3395 - acc: 0.5804 - val_loss: 1.4343 - val_acc: 0.5333\n",
      "Epoch 83/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 1.3097 - acc: 0.5941 - val_loss: 1.4259 - val_acc: 0.5444\n",
      "Epoch 84/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 1.3671 - acc: 0.5745 - val_loss: 1.4230 - val_acc: 0.5778\n",
      "Epoch 85/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.3451 - acc: 0.5268Epoch 00085: val_loss improved from 1.45327 to 1.41544, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 599us/step - loss: 1.3293 - acc: 0.5412 - val_loss: 1.4154 - val_acc: 0.5778\n",
      "Epoch 86/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 1.3391 - acc: 0.5725 - val_loss: 1.4091 - val_acc: 0.5778\n",
      "Epoch 87/3000\n",
      "510/510 [==============================] - 0s 550us/step - loss: 1.3001 - acc: 0.5882 - val_loss: 1.4038 - val_acc: 0.5556\n",
      "Epoch 88/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 1.3252 - acc: 0.5784 - val_loss: 1.3981 - val_acc: 0.5556\n",
      "Epoch 89/3000\n",
      "510/510 [==============================] - 0s 555us/step - loss: 1.2385 - acc: 0.6431 - val_loss: 1.3868 - val_acc: 0.5556\n",
      "Epoch 90/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.3240 - acc: 0.5536Epoch 00090: val_loss improved from 1.41544 to 1.38186, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 609us/step - loss: 1.3226 - acc: 0.5529 - val_loss: 1.3819 - val_acc: 0.5667\n",
      "Epoch 91/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 1.2841 - acc: 0.5882 - val_loss: 1.3765 - val_acc: 0.5667\n",
      "Epoch 92/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 1.2607 - acc: 0.6098 - val_loss: 1.3685 - val_acc: 0.5667\n",
      "Epoch 93/3000\n",
      "510/510 [==============================] - 0s 545us/step - loss: 1.2508 - acc: 0.6137 - val_loss: 1.3608 - val_acc: 0.5778\n",
      "Epoch 94/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 1.2656 - acc: 0.5843 - val_loss: 1.3574 - val_acc: 0.5778\n",
      "Epoch 95/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2942 - acc: 0.5469Epoch 00095: val_loss improved from 1.38186 to 1.35577, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 571us/step - loss: 1.2994 - acc: 0.5549 - val_loss: 1.3558 - val_acc: 0.5778\n",
      "Epoch 96/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 1.2569 - acc: 0.5882 - val_loss: 1.3514 - val_acc: 0.5667\n",
      "Epoch 97/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 1.2158 - acc: 0.6353 - val_loss: 1.3487 - val_acc: 0.5667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 1.2101 - acc: 0.6157 - val_loss: 1.3444 - val_acc: 0.5778\n",
      "Epoch 99/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 1.2376 - acc: 0.6020 - val_loss: 1.3320 - val_acc: 0.5778\n",
      "Epoch 100/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2292 - acc: 0.5804Epoch 00100: val_loss improved from 1.35577 to 1.33075, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 569us/step - loss: 1.2471 - acc: 0.5745 - val_loss: 1.3308 - val_acc: 0.5889\n",
      "Epoch 101/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 1.2284 - acc: 0.5863 - val_loss: 1.3259 - val_acc: 0.5889\n",
      "Epoch 102/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 1.2400 - acc: 0.5902 - val_loss: 1.3195 - val_acc: 0.5778\n",
      "Epoch 103/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 1.2412 - acc: 0.5961 - val_loss: 1.3215 - val_acc: 0.5667\n",
      "Epoch 104/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 1.2140 - acc: 0.5843 - val_loss: 1.3158 - val_acc: 0.5889\n",
      "Epoch 105/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2031 - acc: 0.6004Epoch 00105: val_loss improved from 1.33075 to 1.30972, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 574us/step - loss: 1.1974 - acc: 0.5961 - val_loss: 1.3097 - val_acc: 0.5889\n",
      "Epoch 106/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 1.1752 - acc: 0.6353 - val_loss: 1.3035 - val_acc: 0.5889\n",
      "Epoch 107/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 1.1794 - acc: 0.6255 - val_loss: 1.3004 - val_acc: 0.6000\n",
      "Epoch 108/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 1.1998 - acc: 0.6157 - val_loss: 1.2957 - val_acc: 0.6000\n",
      "Epoch 109/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 1.1748 - acc: 0.6059 - val_loss: 1.2914 - val_acc: 0.6000\n",
      "Epoch 110/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2143 - acc: 0.5848Epoch 00110: val_loss improved from 1.30972 to 1.28788, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 593us/step - loss: 1.2086 - acc: 0.5882 - val_loss: 1.2879 - val_acc: 0.6000\n",
      "Epoch 111/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 1.1820 - acc: 0.6176 - val_loss: 1.2831 - val_acc: 0.6000\n",
      "Epoch 112/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 1.1611 - acc: 0.6314 - val_loss: 1.2766 - val_acc: 0.6000\n",
      "Epoch 113/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 1.1807 - acc: 0.5941 - val_loss: 1.2732 - val_acc: 0.6000\n",
      "Epoch 114/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 1.1333 - acc: 0.6353 - val_loss: 1.2685 - val_acc: 0.6000\n",
      "Epoch 115/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.1827 - acc: 0.6094Epoch 00115: val_loss improved from 1.28788 to 1.26568, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 624us/step - loss: 1.1727 - acc: 0.6157 - val_loss: 1.2657 - val_acc: 0.6000\n",
      "Epoch 116/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 1.1526 - acc: 0.6059 - val_loss: 1.2642 - val_acc: 0.6000\n",
      "Epoch 117/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 1.1257 - acc: 0.6471 - val_loss: 1.2590 - val_acc: 0.6000\n",
      "Epoch 118/3000\n",
      "510/510 [==============================] - 0s 561us/step - loss: 1.1240 - acc: 0.6412 - val_loss: 1.2524 - val_acc: 0.6111\n",
      "Epoch 119/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 1.1058 - acc: 0.6451 - val_loss: 1.2490 - val_acc: 0.6111\n",
      "Epoch 120/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0911 - acc: 0.6451Epoch 00120: val_loss improved from 1.26568 to 1.24521, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 576us/step - loss: 1.0912 - acc: 0.6490 - val_loss: 1.2452 - val_acc: 0.6111\n",
      "Epoch 121/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 1.1499 - acc: 0.6000 - val_loss: 1.2404 - val_acc: 0.6000\n",
      "Epoch 122/3000\n",
      "510/510 [==============================] - 0s 540us/step - loss: 1.1087 - acc: 0.6451 - val_loss: 1.2374 - val_acc: 0.6000\n",
      "Epoch 123/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 1.1259 - acc: 0.6412 - val_loss: 1.2302 - val_acc: 0.6000\n",
      "Epoch 124/3000\n",
      "510/510 [==============================] - 0s 554us/step - loss: 1.1097 - acc: 0.6314 - val_loss: 1.2309 - val_acc: 0.6000\n",
      "Epoch 125/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0958 - acc: 0.6540Epoch 00125: val_loss improved from 1.24521 to 1.22268, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 600us/step - loss: 1.0801 - acc: 0.6686 - val_loss: 1.2227 - val_acc: 0.6000\n",
      "Epoch 126/3000\n",
      "510/510 [==============================] - 0s 625us/step - loss: 1.1142 - acc: 0.6235 - val_loss: 1.2201 - val_acc: 0.6111\n",
      "Epoch 127/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 1.0735 - acc: 0.6569 - val_loss: 1.2202 - val_acc: 0.6000\n",
      "Epoch 128/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 1.0743 - acc: 0.6608 - val_loss: 1.2182 - val_acc: 0.6000\n",
      "Epoch 129/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 1.1004 - acc: 0.6176 - val_loss: 1.2124 - val_acc: 0.6111\n",
      "Epoch 130/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0548 - acc: 0.6496Epoch 00130: val_loss improved from 1.22268 to 1.21063, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 590us/step - loss: 1.0453 - acc: 0.6667 - val_loss: 1.2106 - val_acc: 0.6111\n",
      "Epoch 131/3000\n",
      "510/510 [==============================] - 0s 552us/step - loss: 1.0821 - acc: 0.6490 - val_loss: 1.2081 - val_acc: 0.6111\n",
      "Epoch 132/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 1.0314 - acc: 0.6784 - val_loss: 1.2030 - val_acc: 0.6000\n",
      "Epoch 133/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 1.0906 - acc: 0.6510 - val_loss: 1.2026 - val_acc: 0.6000\n",
      "Epoch 134/3000\n",
      "510/510 [==============================] - 0s 552us/step - loss: 1.0583 - acc: 0.6667 - val_loss: 1.1984 - val_acc: 0.6111\n",
      "Epoch 135/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0697 - acc: 0.6629Epoch 00135: val_loss improved from 1.21063 to 1.19773, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 592us/step - loss: 1.0700 - acc: 0.6588 - val_loss: 1.1977 - val_acc: 0.6000\n",
      "Epoch 136/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 1.0220 - acc: 0.6784 - val_loss: 1.1938 - val_acc: 0.6111\n",
      "Epoch 137/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 1.0577 - acc: 0.6471 - val_loss: 1.1871 - val_acc: 0.6111\n",
      "Epoch 138/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 1.0262 - acc: 0.7000 - val_loss: 1.1806 - val_acc: 0.6111\n",
      "Epoch 139/3000\n",
      "510/510 [==============================] - 0s 556us/step - loss: 1.0630 - acc: 0.6451 - val_loss: 1.1831 - val_acc: 0.6000\n",
      "Epoch 140/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0366 - acc: 0.6518Epoch 00140: val_loss improved from 1.19773 to 1.18032, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 571us/step - loss: 1.0345 - acc: 0.6569 - val_loss: 1.1803 - val_acc: 0.6000\n",
      "Epoch 141/3000\n",
      "510/510 [==============================] - 0s 552us/step - loss: 1.0887 - acc: 0.6314 - val_loss: 1.1740 - val_acc: 0.6111\n",
      "Epoch 142/3000\n",
      "510/510 [==============================] - 0s 556us/step - loss: 1.0275 - acc: 0.6647 - val_loss: 1.1670 - val_acc: 0.6111\n",
      "Epoch 143/3000\n",
      "510/510 [==============================] - 0s 547us/step - loss: 1.0223 - acc: 0.6765 - val_loss: 1.1677 - val_acc: 0.6000\n",
      "Epoch 144/3000\n",
      "510/510 [==============================] - 0s 541us/step - loss: 0.9934 - acc: 0.6961 - val_loss: 1.1637 - val_acc: 0.6000\n",
      "Epoch 145/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0123 - acc: 0.6808Epoch 00145: val_loss improved from 1.18032 to 1.16216, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 565us/step - loss: 1.0210 - acc: 0.6765 - val_loss: 1.1622 - val_acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 1.0072 - acc: 0.6902 - val_loss: 1.1566 - val_acc: 0.6000\n",
      "Epoch 147/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 1.0029 - acc: 0.6725 - val_loss: 1.1539 - val_acc: 0.6000\n",
      "Epoch 148/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 1.0183 - acc: 0.6667 - val_loss: 1.1499 - val_acc: 0.6111\n",
      "Epoch 149/3000\n",
      "510/510 [==============================] - 0s 556us/step - loss: 1.0218 - acc: 0.6784 - val_loss: 1.1435 - val_acc: 0.6333\n",
      "Epoch 150/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9914 - acc: 0.6763Epoch 00150: val_loss improved from 1.16216 to 1.14755, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.9824 - acc: 0.6863 - val_loss: 1.1476 - val_acc: 0.6333\n",
      "Epoch 151/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.9835 - acc: 0.6882 - val_loss: 1.1479 - val_acc: 0.6111\n",
      "Epoch 152/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.9851 - acc: 0.6902 - val_loss: 1.1445 - val_acc: 0.6000\n",
      "Epoch 153/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.9897 - acc: 0.6745 - val_loss: 1.1416 - val_acc: 0.6000\n",
      "Epoch 154/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.9839 - acc: 0.7059 - val_loss: 1.1412 - val_acc: 0.6111\n",
      "Epoch 155/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9911 - acc: 0.6652Epoch 00155: val_loss improved from 1.14755 to 1.13938, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.9857 - acc: 0.6647 - val_loss: 1.1394 - val_acc: 0.6111\n",
      "Epoch 156/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.9871 - acc: 0.6863 - val_loss: 1.1363 - val_acc: 0.6111\n",
      "Epoch 157/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.9551 - acc: 0.7137 - val_loss: 1.1399 - val_acc: 0.6000\n",
      "Epoch 158/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.9268 - acc: 0.7294 - val_loss: 1.1382 - val_acc: 0.6111\n",
      "Epoch 159/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.9445 - acc: 0.6745 - val_loss: 1.1364 - val_acc: 0.6111\n",
      "Epoch 160/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9972 - acc: 0.6875Epoch 00160: val_loss improved from 1.13938 to 1.13464, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.9982 - acc: 0.6863 - val_loss: 1.1346 - val_acc: 0.6000\n",
      "Epoch 161/3000\n",
      "510/510 [==============================] - 0s 544us/step - loss: 0.9532 - acc: 0.7020 - val_loss: 1.1289 - val_acc: 0.6000\n",
      "Epoch 162/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.9595 - acc: 0.6980 - val_loss: 1.1243 - val_acc: 0.6111\n",
      "Epoch 163/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.9604 - acc: 0.6824 - val_loss: 1.1251 - val_acc: 0.6111\n",
      "Epoch 164/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.9606 - acc: 0.6784 - val_loss: 1.1222 - val_acc: 0.6111\n",
      "Epoch 165/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9295 - acc: 0.7009Epoch 00165: val_loss improved from 1.13464 to 1.11611, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.9198 - acc: 0.7098 - val_loss: 1.1161 - val_acc: 0.6222\n",
      "Epoch 166/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.9199 - acc: 0.7196 - val_loss: 1.1136 - val_acc: 0.6333\n",
      "Epoch 167/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.9310 - acc: 0.6961 - val_loss: 1.1093 - val_acc: 0.6222\n",
      "Epoch 168/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.9637 - acc: 0.6686 - val_loss: 1.1103 - val_acc: 0.6333\n",
      "Epoch 169/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.9078 - acc: 0.7157 - val_loss: 1.1064 - val_acc: 0.6333\n",
      "Epoch 170/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9388 - acc: 0.6942Epoch 00170: val_loss improved from 1.11611 to 1.10646, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.9248 - acc: 0.7039 - val_loss: 1.1065 - val_acc: 0.6444\n",
      "Epoch 171/3000\n",
      "510/510 [==============================] - 0s 561us/step - loss: 0.9692 - acc: 0.6745 - val_loss: 1.1042 - val_acc: 0.6222\n",
      "Epoch 172/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.9371 - acc: 0.6863 - val_loss: 1.1054 - val_acc: 0.6222\n",
      "Epoch 173/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.9167 - acc: 0.7098 - val_loss: 1.1027 - val_acc: 0.6333\n",
      "Epoch 174/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.9367 - acc: 0.6745 - val_loss: 1.1035 - val_acc: 0.6111\n",
      "Epoch 175/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9633 - acc: 0.6585Epoch 00175: val_loss improved from 1.10646 to 1.10314, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.9606 - acc: 0.6647 - val_loss: 1.1031 - val_acc: 0.6222\n",
      "Epoch 176/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.9090 - acc: 0.7098 - val_loss: 1.1042 - val_acc: 0.6222\n",
      "Epoch 177/3000\n",
      "510/510 [==============================] - 0s 543us/step - loss: 0.9074 - acc: 0.7039 - val_loss: 1.1005 - val_acc: 0.6222\n",
      "Epoch 178/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.9113 - acc: 0.7059 - val_loss: 1.0992 - val_acc: 0.6111\n",
      "Epoch 179/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.9112 - acc: 0.7039 - val_loss: 1.1007 - val_acc: 0.6222\n",
      "Epoch 180/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8851 - acc: 0.7054Epoch 00180: val_loss improved from 1.10314 to 1.09883, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.9092 - acc: 0.6961 - val_loss: 1.0988 - val_acc: 0.6333\n",
      "Epoch 181/3000\n",
      "510/510 [==============================] - 0s 554us/step - loss: 0.9148 - acc: 0.7020 - val_loss: 1.0974 - val_acc: 0.6444\n",
      "Epoch 182/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.8955 - acc: 0.7137 - val_loss: 1.0949 - val_acc: 0.6444\n",
      "Epoch 183/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.8584 - acc: 0.7412 - val_loss: 1.0914 - val_acc: 0.6444\n",
      "Epoch 184/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.8998 - acc: 0.6961 - val_loss: 1.0929 - val_acc: 0.6444\n",
      "Epoch 185/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8662 - acc: 0.7143Epoch 00185: val_loss improved from 1.09883 to 1.09073, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.8545 - acc: 0.7216 - val_loss: 1.0907 - val_acc: 0.6444\n",
      "Epoch 186/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.8784 - acc: 0.7294 - val_loss: 1.0888 - val_acc: 0.6222\n",
      "Epoch 187/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.9007 - acc: 0.6863 - val_loss: 1.0899 - val_acc: 0.6333\n",
      "Epoch 188/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.8664 - acc: 0.7510 - val_loss: 1.0868 - val_acc: 0.6333\n",
      "Epoch 189/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.9148 - acc: 0.6784 - val_loss: 1.0861 - val_acc: 0.6333\n",
      "Epoch 190/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8794 - acc: 0.7232Epoch 00190: val_loss improved from 1.09073 to 1.07956, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.8762 - acc: 0.7235 - val_loss: 1.0796 - val_acc: 0.6333\n",
      "Epoch 191/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.8659 - acc: 0.7275 - val_loss: 1.0728 - val_acc: 0.6556\n",
      "Epoch 192/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.9114 - acc: 0.7078 - val_loss: 1.0731 - val_acc: 0.6444\n",
      "Epoch 193/3000\n",
      "510/510 [==============================] - 0s 556us/step - loss: 0.8753 - acc: 0.7412 - val_loss: 1.0694 - val_acc: 0.6333\n",
      "Epoch 194/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 556us/step - loss: 0.8850 - acc: 0.7118 - val_loss: 1.0701 - val_acc: 0.6333\n",
      "Epoch 195/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8336 - acc: 0.7500Epoch 00195: val_loss improved from 1.07956 to 1.06879, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.8460 - acc: 0.7353 - val_loss: 1.0688 - val_acc: 0.6444\n",
      "Epoch 196/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.8626 - acc: 0.7255 - val_loss: 1.0656 - val_acc: 0.6444\n",
      "Epoch 197/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.8280 - acc: 0.7510 - val_loss: 1.0629 - val_acc: 0.6444\n",
      "Epoch 198/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.8785 - acc: 0.7098 - val_loss: 1.0605 - val_acc: 0.6444\n",
      "Epoch 199/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.8551 - acc: 0.7157 - val_loss: 1.0582 - val_acc: 0.6444\n",
      "Epoch 200/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8425 - acc: 0.7478Epoch 00200: val_loss improved from 1.06879 to 1.05739, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.8452 - acc: 0.7431 - val_loss: 1.0574 - val_acc: 0.6444\n",
      "Epoch 201/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.8537 - acc: 0.7314 - val_loss: 1.0543 - val_acc: 0.6444\n",
      "Epoch 202/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.8311 - acc: 0.7471 - val_loss: 1.0565 - val_acc: 0.6444\n",
      "Epoch 203/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.8226 - acc: 0.7451 - val_loss: 1.0582 - val_acc: 0.6333\n",
      "Epoch 204/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.8225 - acc: 0.7235 - val_loss: 1.0543 - val_acc: 0.6333\n",
      "Epoch 205/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8113 - acc: 0.7321Epoch 00205: val_loss improved from 1.05739 to 1.05090, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.8297 - acc: 0.7235 - val_loss: 1.0509 - val_acc: 0.6444\n",
      "Epoch 206/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.8399 - acc: 0.7275 - val_loss: 1.0484 - val_acc: 0.6556\n",
      "Epoch 207/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.8260 - acc: 0.7353 - val_loss: 1.0509 - val_acc: 0.6556\n",
      "Epoch 208/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.8504 - acc: 0.7373 - val_loss: 1.0482 - val_acc: 0.6556\n",
      "Epoch 209/3000\n",
      "510/510 [==============================] - 0s 556us/step - loss: 0.8128 - acc: 0.7431 - val_loss: 1.0452 - val_acc: 0.6667\n",
      "Epoch 210/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8421 - acc: 0.7188Epoch 00210: val_loss improved from 1.05090 to 1.04708, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.8643 - acc: 0.7000 - val_loss: 1.0471 - val_acc: 0.6556\n",
      "Epoch 211/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.8134 - acc: 0.7569 - val_loss: 1.0444 - val_acc: 0.6556\n",
      "Epoch 212/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.8004 - acc: 0.7353 - val_loss: 1.0426 - val_acc: 0.6556\n",
      "Epoch 213/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.8279 - acc: 0.7333 - val_loss: 1.0392 - val_acc: 0.6444\n",
      "Epoch 214/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.8360 - acc: 0.7412 - val_loss: 1.0352 - val_acc: 0.6556\n",
      "Epoch 215/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8207 - acc: 0.7277Epoch 00215: val_loss improved from 1.04708 to 1.03605, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.8297 - acc: 0.7216 - val_loss: 1.0360 - val_acc: 0.6667\n",
      "Epoch 216/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.8185 - acc: 0.7235 - val_loss: 1.0333 - val_acc: 0.6444\n",
      "Epoch 217/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.8101 - acc: 0.7569 - val_loss: 1.0297 - val_acc: 0.6444\n",
      "Epoch 218/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.8284 - acc: 0.7216 - val_loss: 1.0305 - val_acc: 0.6444\n",
      "Epoch 219/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.8227 - acc: 0.7373 - val_loss: 1.0282 - val_acc: 0.6556\n",
      "Epoch 220/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7706 - acc: 0.7433Epoch 00220: val_loss improved from 1.03605 to 1.02292, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.7854 - acc: 0.7333 - val_loss: 1.0229 - val_acc: 0.6556\n",
      "Epoch 221/3000\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.8111 - acc: 0.7529 - val_loss: 1.0183 - val_acc: 0.6556\n",
      "Epoch 222/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.8104 - acc: 0.7431 - val_loss: 1.0176 - val_acc: 0.6667\n",
      "Epoch 223/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.7813 - acc: 0.7667 - val_loss: 1.0178 - val_acc: 0.6556\n",
      "Epoch 224/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.7798 - acc: 0.7725 - val_loss: 1.0175 - val_acc: 0.6444\n",
      "Epoch 225/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7717 - acc: 0.7321Epoch 00225: val_loss improved from 1.02292 to 1.01738, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.7839 - acc: 0.7216 - val_loss: 1.0174 - val_acc: 0.6556\n",
      "Epoch 226/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.7917 - acc: 0.7490 - val_loss: 1.0124 - val_acc: 0.6444\n",
      "Epoch 227/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.8209 - acc: 0.7451 - val_loss: 1.0158 - val_acc: 0.6444\n",
      "Epoch 228/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.7618 - acc: 0.7980 - val_loss: 1.0155 - val_acc: 0.6444\n",
      "Epoch 229/3000\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.7751 - acc: 0.7471 - val_loss: 1.0152 - val_acc: 0.6444\n",
      "Epoch 230/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8115 - acc: 0.7232Epoch 00230: val_loss did not improve\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.8159 - acc: 0.7235 - val_loss: 1.0176 - val_acc: 0.6444\n",
      "Epoch 231/3000\n",
      "510/510 [==============================] - 0s 549us/step - loss: 0.7744 - acc: 0.7569 - val_loss: 1.0172 - val_acc: 0.6556\n",
      "Epoch 232/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.7867 - acc: 0.7451 - val_loss: 1.0144 - val_acc: 0.6556\n",
      "Epoch 233/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.7929 - acc: 0.7471 - val_loss: 1.0251 - val_acc: 0.6444\n",
      "Epoch 234/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.7955 - acc: 0.7471 - val_loss: 1.0188 - val_acc: 0.6556\n",
      "Epoch 235/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7823 - acc: 0.7366Epoch 00235: val_loss did not improve\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.7876 - acc: 0.7333 - val_loss: 1.0198 - val_acc: 0.6444\n",
      "Epoch 236/3000\n",
      "510/510 [==============================] - 0s 554us/step - loss: 0.7801 - acc: 0.7686 - val_loss: 1.0201 - val_acc: 0.6333\n",
      "Epoch 237/3000\n",
      "510/510 [==============================] - 0s 545us/step - loss: 0.7538 - acc: 0.7510 - val_loss: 1.0141 - val_acc: 0.6444\n",
      "Epoch 238/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.7403 - acc: 0.7627 - val_loss: 1.0134 - val_acc: 0.6556\n",
      "Epoch 239/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.7440 - acc: 0.7941 - val_loss: 1.0116 - val_acc: 0.6444\n",
      "Epoch 240/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7616 - acc: 0.765 - ETA: 0s - loss: 0.7693 - acc: 0.7634Epoch 00240: val_loss improved from 1.01738 to 1.01361, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.7699 - acc: 0.7549 - val_loss: 1.0136 - val_acc: 0.6222\n",
      "Epoch 241/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.7735 - acc: 0.7490 - val_loss: 1.0099 - val_acc: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.7485 - acc: 0.7608 - val_loss: 1.0112 - val_acc: 0.6444\n",
      "Epoch 243/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.8042 - acc: 0.7216 - val_loss: 1.0126 - val_acc: 0.6444\n",
      "Epoch 244/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.7563 - acc: 0.7647 - val_loss: 1.0068 - val_acc: 0.6556\n",
      "Epoch 245/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7386 - acc: 0.7679Epoch 00245: val_loss improved from 1.01361 to 1.01007, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 737us/step - loss: 0.7484 - acc: 0.7627 - val_loss: 1.0101 - val_acc: 0.6444\n",
      "Epoch 246/3000\n",
      "510/510 [==============================] - 0s 776us/step - loss: 0.7337 - acc: 0.7706 - val_loss: 1.0075 - val_acc: 0.6333\n",
      "Epoch 247/3000\n",
      "510/510 [==============================] - 0s 656us/step - loss: 0.7455 - acc: 0.7843 - val_loss: 1.0053 - val_acc: 0.6333\n",
      "Epoch 248/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.7612 - acc: 0.7569 - val_loss: 1.0050 - val_acc: 0.6333\n",
      "Epoch 249/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.7432 - acc: 0.7804 - val_loss: 0.9977 - val_acc: 0.6667\n",
      "Epoch 250/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7492 - acc: 0.7612Epoch 00250: val_loss improved from 1.01007 to 0.99728, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.7615 - acc: 0.7608 - val_loss: 0.9973 - val_acc: 0.6667\n",
      "Epoch 251/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.7659 - acc: 0.7510 - val_loss: 0.9949 - val_acc: 0.6667\n",
      "Epoch 252/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.7532 - acc: 0.7431 - val_loss: 0.9975 - val_acc: 0.6556\n",
      "Epoch 253/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.7566 - acc: 0.7431 - val_loss: 0.9934 - val_acc: 0.6667\n",
      "Epoch 254/3000\n",
      "510/510 [==============================] - 0s 637us/step - loss: 0.7553 - acc: 0.7510 - val_loss: 0.9952 - val_acc: 0.6444\n",
      "Epoch 255/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7343 - acc: 0.7612Epoch 00255: val_loss improved from 0.99728 to 0.99467, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 690us/step - loss: 0.7331 - acc: 0.7667 - val_loss: 0.9947 - val_acc: 0.6444\n",
      "Epoch 256/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.7299 - acc: 0.7902 - val_loss: 0.9930 - val_acc: 0.6444\n",
      "Epoch 257/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.7487 - acc: 0.7725 - val_loss: 0.9938 - val_acc: 0.6222\n",
      "Epoch 258/3000\n",
      "510/510 [==============================] - 0s 659us/step - loss: 0.7506 - acc: 0.7510 - val_loss: 0.9911 - val_acc: 0.6222\n",
      "Epoch 259/3000\n",
      "510/510 [==============================] - 0s 652us/step - loss: 0.7093 - acc: 0.8000 - val_loss: 0.9901 - val_acc: 0.6111\n",
      "Epoch 260/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7422 - acc: 0.7455Epoch 00260: val_loss improved from 0.99467 to 0.98603, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.7344 - acc: 0.7529 - val_loss: 0.9860 - val_acc: 0.6333\n",
      "Epoch 261/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.7003 - acc: 0.7902 - val_loss: 0.9861 - val_acc: 0.6333\n",
      "Epoch 262/3000\n",
      "510/510 [==============================] - 0s 676us/step - loss: 0.7436 - acc: 0.7569 - val_loss: 0.9793 - val_acc: 0.6333\n",
      "Epoch 263/3000\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.6852 - acc: 0.7863 - val_loss: 0.9824 - val_acc: 0.6333\n",
      "Epoch 264/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.7119 - acc: 0.7745 - val_loss: 0.9816 - val_acc: 0.6333\n",
      "Epoch 265/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7043 - acc: 0.7879Epoch 00265: val_loss improved from 0.98603 to 0.97859, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.7159 - acc: 0.7745 - val_loss: 0.9786 - val_acc: 0.6333\n",
      "Epoch 266/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.7133 - acc: 0.7667 - val_loss: 0.9767 - val_acc: 0.6444\n",
      "Epoch 267/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.7179 - acc: 0.7863 - val_loss: 0.9777 - val_acc: 0.6333\n",
      "Epoch 268/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.7021 - acc: 0.8020 - val_loss: 0.9783 - val_acc: 0.6222\n",
      "Epoch 269/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.7237 - acc: 0.7784 - val_loss: 0.9734 - val_acc: 0.6333\n",
      "Epoch 270/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7426 - acc: 0.7746Epoch 00270: val_loss improved from 0.97859 to 0.96330, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.7263 - acc: 0.7804 - val_loss: 0.9633 - val_acc: 0.6556\n",
      "Epoch 271/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.6892 - acc: 0.7980 - val_loss: 0.9721 - val_acc: 0.6333\n",
      "Epoch 272/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.7164 - acc: 0.7824 - val_loss: 0.9713 - val_acc: 0.6333\n",
      "Epoch 273/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.7025 - acc: 0.7922 - val_loss: 0.9755 - val_acc: 0.6222\n",
      "Epoch 274/3000\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.7121 - acc: 0.7784 - val_loss: 0.9738 - val_acc: 0.6333\n",
      "Epoch 275/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6855 - acc: 0.8013Epoch 00275: val_loss did not improve\n",
      "510/510 [==============================] - 0s 661us/step - loss: 0.6897 - acc: 0.7961 - val_loss: 0.9684 - val_acc: 0.6333\n",
      "Epoch 276/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.6780 - acc: 0.8039 - val_loss: 0.9683 - val_acc: 0.6333\n",
      "Epoch 277/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.6801 - acc: 0.7980 - val_loss: 0.9687 - val_acc: 0.6444\n",
      "Epoch 278/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.6834 - acc: 0.7843 - val_loss: 0.9715 - val_acc: 0.6444\n",
      "Epoch 279/3000\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.7172 - acc: 0.7686 - val_loss: 0.9672 - val_acc: 0.6333\n",
      "Epoch 280/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7146 - acc: 0.7746Epoch 00280: val_loss did not improve\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.7180 - acc: 0.7784 - val_loss: 0.9657 - val_acc: 0.6333\n",
      "Epoch 281/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.6988 - acc: 0.7902 - val_loss: 0.9641 - val_acc: 0.6333\n",
      "Epoch 282/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.6719 - acc: 0.7961 - val_loss: 0.9643 - val_acc: 0.6333\n",
      "Epoch 283/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.7028 - acc: 0.7804 - val_loss: 0.9578 - val_acc: 0.6333\n",
      "Epoch 284/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.7202 - acc: 0.7549 - val_loss: 0.9555 - val_acc: 0.6333\n",
      "Epoch 285/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7222 - acc: 0.7701Epoch 00285: val_loss improved from 0.96330 to 0.96006, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.7190 - acc: 0.7706 - val_loss: 0.9601 - val_acc: 0.6222\n",
      "Epoch 286/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.6814 - acc: 0.7902 - val_loss: 0.9598 - val_acc: 0.6444\n",
      "Epoch 287/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.6722 - acc: 0.7961 - val_loss: 0.9564 - val_acc: 0.6444\n",
      "Epoch 288/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.6576 - acc: 0.7922 - val_loss: 0.9599 - val_acc: 0.6556\n",
      "Epoch 289/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.7018 - acc: 0.7745 - val_loss: 0.9574 - val_acc: 0.6333\n",
      "Epoch 290/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6653 - acc: 0.7924Epoch 00290: val_loss improved from 0.96006 to 0.95323, saving model to top_weight.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 633us/step - loss: 0.6720 - acc: 0.7882 - val_loss: 0.9532 - val_acc: 0.6444\n",
      "Epoch 291/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.6764 - acc: 0.8059 - val_loss: 0.9575 - val_acc: 0.6333\n",
      "Epoch 292/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.6671 - acc: 0.7980 - val_loss: 0.9574 - val_acc: 0.6333\n",
      "Epoch 293/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.6706 - acc: 0.8098 - val_loss: 0.9602 - val_acc: 0.6222\n",
      "Epoch 294/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.6679 - acc: 0.8137 - val_loss: 0.9580 - val_acc: 0.6222\n",
      "Epoch 295/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6426 - acc: 0.8192Epoch 00295: val_loss did not improve\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.6434 - acc: 0.8216 - val_loss: 0.9542 - val_acc: 0.6222\n",
      "Epoch 296/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.6473 - acc: 0.7804 - val_loss: 0.9507 - val_acc: 0.6222\n",
      "Epoch 297/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.6802 - acc: 0.7902 - val_loss: 0.9514 - val_acc: 0.6222\n",
      "Epoch 298/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.6522 - acc: 0.7667 - val_loss: 0.9438 - val_acc: 0.6333\n",
      "Epoch 299/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.6607 - acc: 0.7667 - val_loss: 0.9463 - val_acc: 0.6111\n",
      "Epoch 300/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6352 - acc: 0.8237Epoch 00300: val_loss improved from 0.95323 to 0.94224, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.6325 - acc: 0.8255 - val_loss: 0.9422 - val_acc: 0.6333\n",
      "Epoch 301/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.6709 - acc: 0.7804 - val_loss: 0.9430 - val_acc: 0.6333\n",
      "Epoch 302/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.6764 - acc: 0.7843 - val_loss: 0.9470 - val_acc: 0.6222\n",
      "Epoch 303/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 0.6711 - acc: 0.7922 - val_loss: 0.9432 - val_acc: 0.6333\n",
      "Epoch 304/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.6379 - acc: 0.8216 - val_loss: 0.9464 - val_acc: 0.6222\n",
      "Epoch 305/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6766 - acc: 0.8036Epoch 00305: val_loss did not improve\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.6690 - acc: 0.8157 - val_loss: 0.9440 - val_acc: 0.6222\n",
      "Epoch 306/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.6710 - acc: 0.7980 - val_loss: 0.9406 - val_acc: 0.6333\n",
      "Epoch 307/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.6353 - acc: 0.7922 - val_loss: 0.9397 - val_acc: 0.6444\n",
      "Epoch 308/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.6502 - acc: 0.8039 - val_loss: 0.9482 - val_acc: 0.6333\n",
      "Epoch 309/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.6727 - acc: 0.7863 - val_loss: 0.9438 - val_acc: 0.6333\n",
      "Epoch 310/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6630 - acc: 0.7924Epoch 00310: val_loss improved from 0.94224 to 0.93798, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.6628 - acc: 0.7922 - val_loss: 0.9380 - val_acc: 0.6333\n",
      "Epoch 311/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.6598 - acc: 0.7882 - val_loss: 0.9441 - val_acc: 0.6444\n",
      "Epoch 312/3000\n",
      "510/510 [==============================] - 0s 541us/step - loss: 0.6944 - acc: 0.7784 - val_loss: 0.9394 - val_acc: 0.6333\n",
      "Epoch 313/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.6741 - acc: 0.7706 - val_loss: 0.9385 - val_acc: 0.6222\n",
      "Epoch 314/3000\n",
      "510/510 [==============================] - 0s 543us/step - loss: 0.6383 - acc: 0.8118 - val_loss: 0.9351 - val_acc: 0.6444\n",
      "Epoch 315/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6278 - acc: 0.8192Epoch 00315: val_loss improved from 0.93798 to 0.93656, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.6435 - acc: 0.8118 - val_loss: 0.9366 - val_acc: 0.6444\n",
      "Epoch 316/3000\n",
      "510/510 [==============================] - 0s 556us/step - loss: 0.6838 - acc: 0.7706 - val_loss: 0.9335 - val_acc: 0.6444\n",
      "Epoch 317/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.6379 - acc: 0.7941 - val_loss: 0.9379 - val_acc: 0.6444\n",
      "Epoch 318/3000\n",
      "510/510 [==============================] - 0s 549us/step - loss: 0.6403 - acc: 0.7941 - val_loss: 0.9350 - val_acc: 0.6444\n",
      "Epoch 319/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.6251 - acc: 0.7961 - val_loss: 0.9372 - val_acc: 0.6556\n",
      "Epoch 320/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6528 - acc: 0.8192Epoch 00320: val_loss improved from 0.93656 to 0.93293, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.6593 - acc: 0.8157 - val_loss: 0.9329 - val_acc: 0.6444\n",
      "Epoch 321/3000\n",
      "510/510 [==============================] - 0s 545us/step - loss: 0.6377 - acc: 0.7961 - val_loss: 0.9247 - val_acc: 0.6444\n",
      "Epoch 322/3000\n",
      "510/510 [==============================] - 0s 534us/step - loss: 0.6469 - acc: 0.7882 - val_loss: 0.9294 - val_acc: 0.6556\n",
      "Epoch 323/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.6245 - acc: 0.8176 - val_loss: 0.9344 - val_acc: 0.6444\n",
      "Epoch 324/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 0.6310 - acc: 0.7980 - val_loss: 0.9270 - val_acc: 0.6444\n",
      "Epoch 325/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6520 - acc: 0.7969Epoch 00325: val_loss improved from 0.93293 to 0.93016, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.6611 - acc: 0.7922 - val_loss: 0.9302 - val_acc: 0.6333\n",
      "Epoch 326/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.6195 - acc: 0.8020 - val_loss: 0.9282 - val_acc: 0.6333\n",
      "Epoch 327/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.6590 - acc: 0.7863 - val_loss: 0.9265 - val_acc: 0.6556\n",
      "Epoch 328/3000\n",
      "510/510 [==============================] - 0s 551us/step - loss: 0.6292 - acc: 0.7961 - val_loss: 0.9304 - val_acc: 0.6333\n",
      "Epoch 329/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.6348 - acc: 0.7961 - val_loss: 0.9288 - val_acc: 0.6333\n",
      "Epoch 330/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6256 - acc: 0.7946Epoch 00330: val_loss improved from 0.93016 to 0.92913, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.6258 - acc: 0.7922 - val_loss: 0.9291 - val_acc: 0.6444\n",
      "Epoch 331/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.6253 - acc: 0.8196 - val_loss: 0.9300 - val_acc: 0.6333\n",
      "Epoch 332/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.6660 - acc: 0.8000 - val_loss: 0.9222 - val_acc: 0.6333\n",
      "Epoch 333/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.6345 - acc: 0.8000 - val_loss: 0.9209 - val_acc: 0.6444\n",
      "Epoch 334/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.6250 - acc: 0.7922 - val_loss: 0.9272 - val_acc: 0.6333\n",
      "Epoch 335/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6272 - acc: 0.8080Epoch 00335: val_loss did not improve\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.6290 - acc: 0.8078 - val_loss: 0.9331 - val_acc: 0.6444\n",
      "Epoch 336/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.5993 - acc: 0.8098 - val_loss: 0.9272 - val_acc: 0.6333\n",
      "Epoch 337/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.5965 - acc: 0.8176 - val_loss: 0.9238 - val_acc: 0.6444\n",
      "Epoch 338/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.6191 - acc: 0.8235 - val_loss: 0.9235 - val_acc: 0.6444\n",
      "Epoch 339/3000\n",
      "510/510 [==============================] - 0s 552us/step - loss: 0.6293 - acc: 0.8157 - val_loss: 0.9183 - val_acc: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6243 - acc: 0.7902Epoch 00340: val_loss improved from 0.92913 to 0.92042, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.6226 - acc: 0.7941 - val_loss: 0.9204 - val_acc: 0.6333\n",
      "Epoch 341/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.6106 - acc: 0.8000 - val_loss: 0.9254 - val_acc: 0.6222\n",
      "Epoch 342/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.6270 - acc: 0.7922 - val_loss: 0.9243 - val_acc: 0.6444\n",
      "Epoch 343/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.6199 - acc: 0.8176 - val_loss: 0.9197 - val_acc: 0.6222\n",
      "Epoch 344/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.6033 - acc: 0.7941 - val_loss: 0.9211 - val_acc: 0.6333\n",
      "Epoch 345/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6016 - acc: 0.8348Epoch 00345: val_loss improved from 0.92042 to 0.91731, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.6048 - acc: 0.8235 - val_loss: 0.9173 - val_acc: 0.6444\n",
      "Epoch 346/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.5789 - acc: 0.8255 - val_loss: 0.9185 - val_acc: 0.6444\n",
      "Epoch 347/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.6293 - acc: 0.8000 - val_loss: 0.9219 - val_acc: 0.6333\n",
      "Epoch 348/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.6053 - acc: 0.8078 - val_loss: 0.9229 - val_acc: 0.6333\n",
      "Epoch 349/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.5808 - acc: 0.8255 - val_loss: 0.9214 - val_acc: 0.6333\n",
      "Epoch 350/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5754 - acc: 0.8438Epoch 00350: val_loss did not improve\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.5816 - acc: 0.8373 - val_loss: 0.9205 - val_acc: 0.6333\n",
      "Epoch 351/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.5973 - acc: 0.8314 - val_loss: 0.9221 - val_acc: 0.6444\n",
      "Epoch 352/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.6189 - acc: 0.8137 - val_loss: 0.9209 - val_acc: 0.6444\n",
      "Epoch 353/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.5984 - acc: 0.7980 - val_loss: 0.9147 - val_acc: 0.6556\n",
      "Epoch 354/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.6099 - acc: 0.8196 - val_loss: 0.9094 - val_acc: 0.6556\n",
      "Epoch 355/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6446 - acc: 0.7746Epoch 00355: val_loss improved from 0.91731 to 0.91526, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.6219 - acc: 0.7863 - val_loss: 0.9153 - val_acc: 0.6444\n",
      "Epoch 356/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.5932 - acc: 0.8333 - val_loss: 0.9175 - val_acc: 0.6444\n",
      "Epoch 357/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.5872 - acc: 0.8255 - val_loss: 0.9167 - val_acc: 0.6444\n",
      "Epoch 358/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.5798 - acc: 0.8353 - val_loss: 0.9148 - val_acc: 0.6444\n",
      "Epoch 359/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.5936 - acc: 0.8333 - val_loss: 0.9172 - val_acc: 0.6444\n",
      "Epoch 360/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5863 - acc: 0.8103Epoch 00360: val_loss improved from 0.91526 to 0.91030, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.5996 - acc: 0.7980 - val_loss: 0.9103 - val_acc: 0.6333\n",
      "Epoch 361/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.5779 - acc: 0.8275 - val_loss: 0.9076 - val_acc: 0.6444\n",
      "Epoch 362/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.5825 - acc: 0.8314 - val_loss: 0.9089 - val_acc: 0.6556\n",
      "Epoch 363/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.5957 - acc: 0.7961 - val_loss: 0.9098 - val_acc: 0.6667\n",
      "Epoch 364/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.5733 - acc: 0.8098 - val_loss: 0.9068 - val_acc: 0.6556\n",
      "Epoch 365/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6008 - acc: 0.8147Epoch 00365: val_loss improved from 0.91030 to 0.91001, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.5972 - acc: 0.8196 - val_loss: 0.9100 - val_acc: 0.6333\n",
      "Epoch 366/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.5861 - acc: 0.8235 - val_loss: 0.9082 - val_acc: 0.6444\n",
      "Epoch 367/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 0.5661 - acc: 0.8098 - val_loss: 0.8995 - val_acc: 0.6667\n",
      "Epoch 368/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.5938 - acc: 0.8118 - val_loss: 0.9022 - val_acc: 0.6444\n",
      "Epoch 369/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.5817 - acc: 0.8314 - val_loss: 0.8985 - val_acc: 0.6444\n",
      "Epoch 370/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6069 - acc: 0.8214Epoch 00370: val_loss improved from 0.91001 to 0.90452, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.5887 - acc: 0.8255 - val_loss: 0.9045 - val_acc: 0.6333\n",
      "Epoch 371/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.5897 - acc: 0.8196 - val_loss: 0.8933 - val_acc: 0.6444\n",
      "Epoch 372/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.5719 - acc: 0.8255 - val_loss: 0.8932 - val_acc: 0.6556\n",
      "Epoch 373/3000\n",
      "510/510 [==============================] - 0s 550us/step - loss: 0.6034 - acc: 0.7980 - val_loss: 0.8953 - val_acc: 0.6444\n",
      "Epoch 374/3000\n",
      "510/510 [==============================] - 0s 535us/step - loss: 0.5721 - acc: 0.8235 - val_loss: 0.8937 - val_acc: 0.6444\n",
      "Epoch 375/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5686 - acc: 0.8482Epoch 00375: val_loss improved from 0.90452 to 0.89156, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.5537 - acc: 0.8529 - val_loss: 0.8916 - val_acc: 0.6444\n",
      "Epoch 376/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.5732 - acc: 0.8176 - val_loss: 0.8893 - val_acc: 0.6556\n",
      "Epoch 377/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.5790 - acc: 0.8294 - val_loss: 0.8880 - val_acc: 0.6556\n",
      "Epoch 378/3000\n",
      "510/510 [==============================] - 0s 568us/step - loss: 0.5734 - acc: 0.8255 - val_loss: 0.8873 - val_acc: 0.6333\n",
      "Epoch 379/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.5618 - acc: 0.8157 - val_loss: 0.8773 - val_acc: 0.6889\n",
      "Epoch 380/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5971 - acc: 0.7969Epoch 00380: val_loss improved from 0.89156 to 0.87614, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.6020 - acc: 0.8000 - val_loss: 0.8761 - val_acc: 0.7000\n",
      "Epoch 381/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.5753 - acc: 0.8333 - val_loss: 0.8838 - val_acc: 0.6667\n",
      "Epoch 382/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.5898 - acc: 0.8157 - val_loss: 0.8807 - val_acc: 0.6667\n",
      "Epoch 383/3000\n",
      "510/510 [==============================] - 0s 552us/step - loss: 0.5519 - acc: 0.8314 - val_loss: 0.8849 - val_acc: 0.6556\n",
      "Epoch 384/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.6024 - acc: 0.8020 - val_loss: 0.8902 - val_acc: 0.6444\n",
      "Epoch 385/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5604 - acc: 0.8125Epoch 00385: val_loss did not improve\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.5535 - acc: 0.8196 - val_loss: 0.8845 - val_acc: 0.6556\n",
      "Epoch 386/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.5793 - acc: 0.8098 - val_loss: 0.8783 - val_acc: 0.6667\n",
      "Epoch 387/3000\n",
      "510/510 [==============================] - 0s 537us/step - loss: 0.5497 - acc: 0.8314 - val_loss: 0.8706 - val_acc: 0.7000\n",
      "Epoch 388/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 571us/step - loss: 0.5683 - acc: 0.8275 - val_loss: 0.8730 - val_acc: 0.7000\n",
      "Epoch 389/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.5562 - acc: 0.8235 - val_loss: 0.8708 - val_acc: 0.7000\n",
      "Epoch 390/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5456 - acc: 0.8214Epoch 00390: val_loss did not improve\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.5420 - acc: 0.8255 - val_loss: 0.8776 - val_acc: 0.6778\n",
      "Epoch 391/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.5642 - acc: 0.8196 - val_loss: 0.8709 - val_acc: 0.7000\n",
      "Epoch 392/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.5575 - acc: 0.8196 - val_loss: 0.8743 - val_acc: 0.6778\n",
      "Epoch 393/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.5757 - acc: 0.8078 - val_loss: 0.8742 - val_acc: 0.6667\n",
      "Epoch 394/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.5391 - acc: 0.8490 - val_loss: 0.8735 - val_acc: 0.6778\n",
      "Epoch 395/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5036 - acc: 0.8638Epoch 00395: val_loss did not improve\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.5234 - acc: 0.8529 - val_loss: 0.8799 - val_acc: 0.6556\n",
      "Epoch 396/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.5535 - acc: 0.8333 - val_loss: 0.8775 - val_acc: 0.6667\n",
      "Epoch 397/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.5502 - acc: 0.8392 - val_loss: 0.8827 - val_acc: 0.6667\n",
      "Epoch 398/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.5463 - acc: 0.8490 - val_loss: 0.8760 - val_acc: 0.6667\n",
      "Epoch 399/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.5648 - acc: 0.8294 - val_loss: 0.8699 - val_acc: 0.6667\n",
      "Epoch 400/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5578 - acc: 0.8281Epoch 00400: val_loss improved from 0.87614 to 0.87447, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.5572 - acc: 0.8275 - val_loss: 0.8745 - val_acc: 0.6556\n",
      "Epoch 401/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.5412 - acc: 0.8275 - val_loss: 0.8722 - val_acc: 0.6556\n",
      "Epoch 402/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.5522 - acc: 0.8294 - val_loss: 0.8729 - val_acc: 0.6444\n",
      "Epoch 403/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.5372 - acc: 0.8235 - val_loss: 0.8730 - val_acc: 0.6667\n",
      "Epoch 404/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.5338 - acc: 0.8216 - val_loss: 0.8749 - val_acc: 0.6778\n",
      "Epoch 405/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5481 - acc: 0.8371Epoch 00405: val_loss improved from 0.87447 to 0.87351, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.5404 - acc: 0.8353 - val_loss: 0.8735 - val_acc: 0.6556\n",
      "Epoch 406/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.5550 - acc: 0.8216 - val_loss: 0.8709 - val_acc: 0.6778\n",
      "Epoch 407/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.5476 - acc: 0.8490 - val_loss: 0.8666 - val_acc: 0.6667\n",
      "Epoch 408/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.5325 - acc: 0.8373 - val_loss: 0.8671 - val_acc: 0.6667\n",
      "Epoch 409/3000\n",
      "510/510 [==============================] - 0s 720us/step - loss: 0.5508 - acc: 0.8216 - val_loss: 0.8628 - val_acc: 0.6778\n",
      "Epoch 410/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5079 - acc: 0.8549Epoch 00410: val_loss improved from 0.87351 to 0.86491, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 694us/step - loss: 0.5311 - acc: 0.8412 - val_loss: 0.8649 - val_acc: 0.6889\n",
      "Epoch 411/3000\n",
      "510/510 [==============================] - 0s 662us/step - loss: 0.5490 - acc: 0.8451 - val_loss: 0.8730 - val_acc: 0.6889\n",
      "Epoch 412/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.5256 - acc: 0.8490 - val_loss: 0.8711 - val_acc: 0.6778\n",
      "Epoch 413/3000\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.5576 - acc: 0.8255 - val_loss: 0.8714 - val_acc: 0.6778\n",
      "Epoch 414/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.5554 - acc: 0.8216 - val_loss: 0.8760 - val_acc: 0.6667\n",
      "Epoch 415/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5220 - acc: 0.8504Epoch 00415: val_loss did not improve\n",
      "510/510 [==============================] - 0s 637us/step - loss: 0.5275 - acc: 0.8490 - val_loss: 0.8693 - val_acc: 0.6667\n",
      "Epoch 416/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.5365 - acc: 0.8431 - val_loss: 0.8690 - val_acc: 0.6778\n",
      "Epoch 417/3000\n",
      "510/510 [==============================] - 0s 657us/step - loss: 0.5200 - acc: 0.8431 - val_loss: 0.8601 - val_acc: 0.6889\n",
      "Epoch 418/3000\n",
      "510/510 [==============================] - 0s 637us/step - loss: 0.5013 - acc: 0.8608 - val_loss: 0.8686 - val_acc: 0.6889\n",
      "Epoch 419/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.5387 - acc: 0.8392 - val_loss: 0.8621 - val_acc: 0.7111\n",
      "Epoch 420/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4991 - acc: 0.8616Epoch 00420: val_loss did not improve\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.5041 - acc: 0.8471 - val_loss: 0.8669 - val_acc: 0.7000\n",
      "Epoch 421/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.5375 - acc: 0.8529 - val_loss: 0.8672 - val_acc: 0.7000\n",
      "Epoch 422/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.5218 - acc: 0.8529 - val_loss: 0.8688 - val_acc: 0.7000\n",
      "Epoch 423/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.5286 - acc: 0.8373 - val_loss: 0.8708 - val_acc: 0.6778\n",
      "Epoch 424/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.5321 - acc: 0.8373 - val_loss: 0.8667 - val_acc: 0.6889\n",
      "Epoch 425/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5214 - acc: 0.8214Epoch 00425: val_loss improved from 0.86491 to 0.86095, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 657us/step - loss: 0.5219 - acc: 0.8216 - val_loss: 0.8610 - val_acc: 0.6889\n",
      "Epoch 426/3000\n",
      "510/510 [==============================] - 0s 673us/step - loss: 0.5450 - acc: 0.8392 - val_loss: 0.8647 - val_acc: 0.6889\n",
      "Epoch 427/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.5493 - acc: 0.8059 - val_loss: 0.8626 - val_acc: 0.7000\n",
      "Epoch 428/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.5172 - acc: 0.8353 - val_loss: 0.8633 - val_acc: 0.6889\n",
      "Epoch 429/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.5224 - acc: 0.8333 - val_loss: 0.8634 - val_acc: 0.6889\n",
      "Epoch 430/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5204 - acc: 0.8504Epoch 00430: val_loss did not improve\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.5257 - acc: 0.8471 - val_loss: 0.8661 - val_acc: 0.6889\n",
      "Epoch 431/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.5247 - acc: 0.8549 - val_loss: 0.8655 - val_acc: 0.6889\n",
      "Epoch 432/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.5218 - acc: 0.8412 - val_loss: 0.8599 - val_acc: 0.6778\n",
      "Epoch 433/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.5298 - acc: 0.8275 - val_loss: 0.8626 - val_acc: 0.6889\n",
      "Epoch 434/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.5091 - acc: 0.8510 - val_loss: 0.8577 - val_acc: 0.6889\n",
      "Epoch 435/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5186 - acc: 0.8504Epoch 00435: val_loss improved from 0.86095 to 0.85880, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.5160 - acc: 0.8510 - val_loss: 0.8588 - val_acc: 0.6889\n",
      "Epoch 436/3000\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.5467 - acc: 0.8431 - val_loss: 0.8549 - val_acc: 0.6889\n",
      "Epoch 437/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 591us/step - loss: 0.5489 - acc: 0.8176 - val_loss: 0.8509 - val_acc: 0.7000\n",
      "Epoch 438/3000\n",
      "510/510 [==============================] - 0s 658us/step - loss: 0.5112 - acc: 0.8529 - val_loss: 0.8504 - val_acc: 0.6889\n",
      "Epoch 439/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.5068 - acc: 0.8549 - val_loss: 0.8454 - val_acc: 0.6889\n",
      "Epoch 440/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5225 - acc: 0.8326Epoch 00440: val_loss improved from 0.85880 to 0.84617, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.5177 - acc: 0.8431 - val_loss: 0.8462 - val_acc: 0.6889\n",
      "Epoch 441/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.4990 - acc: 0.8412 - val_loss: 0.8496 - val_acc: 0.6889\n",
      "Epoch 442/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.5016 - acc: 0.8549 - val_loss: 0.8458 - val_acc: 0.6778\n",
      "Epoch 443/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.4970 - acc: 0.8588 - val_loss: 0.8486 - val_acc: 0.6889\n",
      "Epoch 444/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.4855 - acc: 0.8549 - val_loss: 0.8499 - val_acc: 0.6778\n",
      "Epoch 445/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5466 - acc: 0.8103Epoch 00445: val_loss did not improve\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.5388 - acc: 0.8098 - val_loss: 0.8489 - val_acc: 0.6778\n",
      "Epoch 446/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.4871 - acc: 0.8784 - val_loss: 0.8437 - val_acc: 0.7000\n",
      "Epoch 447/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.5170 - acc: 0.8255 - val_loss: 0.8438 - val_acc: 0.7000\n",
      "Epoch 448/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.4970 - acc: 0.8392 - val_loss: 0.8414 - val_acc: 0.7000\n",
      "Epoch 449/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.4859 - acc: 0.8569 - val_loss: 0.8414 - val_acc: 0.7000\n",
      "Epoch 450/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5015 - acc: 0.8415Epoch 00450: val_loss improved from 0.84617 to 0.83852, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.5048 - acc: 0.8373 - val_loss: 0.8385 - val_acc: 0.7000\n",
      "Epoch 451/3000\n",
      "510/510 [==============================] - 0s 673us/step - loss: 0.5096 - acc: 0.8412 - val_loss: 0.8403 - val_acc: 0.7000\n",
      "Epoch 452/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.5101 - acc: 0.8529 - val_loss: 0.8408 - val_acc: 0.7000\n",
      "Epoch 453/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.4867 - acc: 0.8431 - val_loss: 0.8343 - val_acc: 0.7000\n",
      "Epoch 454/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.5043 - acc: 0.8549 - val_loss: 0.8354 - val_acc: 0.7000\n",
      "Epoch 455/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5178 - acc: 0.8438Epoch 00455: val_loss improved from 0.83852 to 0.83404, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.5150 - acc: 0.8490 - val_loss: 0.8340 - val_acc: 0.7000\n",
      "Epoch 456/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.4945 - acc: 0.8431 - val_loss: 0.8381 - val_acc: 0.7000\n",
      "Epoch 457/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.5053 - acc: 0.8392 - val_loss: 0.8366 - val_acc: 0.7000\n",
      "Epoch 458/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.4986 - acc: 0.8490 - val_loss: 0.8370 - val_acc: 0.7000\n",
      "Epoch 459/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.4767 - acc: 0.8667 - val_loss: 0.8322 - val_acc: 0.7000\n",
      "Epoch 460/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4992 - acc: 0.8438Epoch 00460: val_loss improved from 0.83404 to 0.82978, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 660us/step - loss: 0.5009 - acc: 0.8392 - val_loss: 0.8298 - val_acc: 0.7111\n",
      "Epoch 461/3000\n",
      "510/510 [==============================] - 0s 650us/step - loss: 0.4896 - acc: 0.8431 - val_loss: 0.8323 - val_acc: 0.7000\n",
      "Epoch 462/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.4588 - acc: 0.8510 - val_loss: 0.8345 - val_acc: 0.6889\n",
      "Epoch 463/3000\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.4843 - acc: 0.8549 - val_loss: 0.8334 - val_acc: 0.7111\n",
      "Epoch 464/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.4856 - acc: 0.8529 - val_loss: 0.8347 - val_acc: 0.7111\n",
      "Epoch 465/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5373 - acc: 0.8393Epoch 00465: val_loss did not improve\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.5383 - acc: 0.8333 - val_loss: 0.8384 - val_acc: 0.7111\n",
      "Epoch 466/3000\n",
      "510/510 [==============================] - 0s 652us/step - loss: 0.4882 - acc: 0.8686 - val_loss: 0.8426 - val_acc: 0.7111\n",
      "Epoch 467/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.4853 - acc: 0.8510 - val_loss: 0.8377 - val_acc: 0.7000\n",
      "Epoch 468/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.4946 - acc: 0.8510 - val_loss: 0.8268 - val_acc: 0.7000\n",
      "Epoch 469/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.4780 - acc: 0.8549 - val_loss: 0.8226 - val_acc: 0.7111\n",
      "Epoch 470/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4684 - acc: 0.8571Epoch 00470: val_loss did not improve\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.4893 - acc: 0.8471 - val_loss: 0.8304 - val_acc: 0.7000\n",
      "Epoch 471/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.4762 - acc: 0.8569 - val_loss: 0.8251 - val_acc: 0.7000\n",
      "Epoch 472/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.5102 - acc: 0.8255 - val_loss: 0.8306 - val_acc: 0.6889\n",
      "Epoch 473/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.4896 - acc: 0.8549 - val_loss: 0.8335 - val_acc: 0.7000\n",
      "Epoch 474/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.4833 - acc: 0.8569 - val_loss: 0.8347 - val_acc: 0.7000\n",
      "Epoch 475/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5084 - acc: 0.8549Epoch 00475: val_loss improved from 0.82978 to 0.82644, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.5097 - acc: 0.8490 - val_loss: 0.8264 - val_acc: 0.7000\n",
      "Epoch 476/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.4916 - acc: 0.8392 - val_loss: 0.8263 - val_acc: 0.7000\n",
      "Epoch 477/3000\n",
      "510/510 [==============================] - 0s 647us/step - loss: 0.5219 - acc: 0.8255 - val_loss: 0.8272 - val_acc: 0.7000\n",
      "Epoch 478/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.5112 - acc: 0.8608 - val_loss: 0.8272 - val_acc: 0.7000\n",
      "Epoch 479/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.4744 - acc: 0.8471 - val_loss: 0.8270 - val_acc: 0.7000\n",
      "Epoch 480/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4752 - acc: 0.8661Epoch 00480: val_loss did not improve\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.4770 - acc: 0.8608 - val_loss: 0.8273 - val_acc: 0.7000\n",
      "Epoch 481/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.4612 - acc: 0.8588 - val_loss: 0.8271 - val_acc: 0.7000\n",
      "Epoch 482/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.4568 - acc: 0.8765 - val_loss: 0.8270 - val_acc: 0.7000\n",
      "Epoch 483/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.4846 - acc: 0.8490 - val_loss: 0.8275 - val_acc: 0.7111\n",
      "Epoch 484/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.4812 - acc: 0.8608 - val_loss: 0.8273 - val_acc: 0.7111\n",
      "Epoch 485/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4737 - acc: 0.8549Epoch 00485: val_loss did not improve\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.4722 - acc: 0.8569 - val_loss: 0.8324 - val_acc: 0.7000\n",
      "Epoch 486/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 587us/step - loss: 0.4707 - acc: 0.8490 - val_loss: 0.8269 - val_acc: 0.7111\n",
      "Epoch 487/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.4798 - acc: 0.8412 - val_loss: 0.8278 - val_acc: 0.7111\n",
      "Epoch 488/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.4677 - acc: 0.8549 - val_loss: 0.8288 - val_acc: 0.7111\n",
      "Epoch 489/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.4636 - acc: 0.8647 - val_loss: 0.8284 - val_acc: 0.7111\n",
      "Epoch 490/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4511 - acc: 0.8661Epoch 00490: val_loss improved from 0.82644 to 0.82214, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 651us/step - loss: 0.4597 - acc: 0.8667 - val_loss: 0.8221 - val_acc: 0.7111\n",
      "Epoch 491/3000\n",
      "510/510 [==============================] - 0s 547us/step - loss: 0.4197 - acc: 0.8843 - val_loss: 0.8253 - val_acc: 0.7111\n",
      "Epoch 492/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.4765 - acc: 0.8667 - val_loss: 0.8231 - val_acc: 0.7000\n",
      "Epoch 493/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.4625 - acc: 0.8588 - val_loss: 0.8255 - val_acc: 0.7000\n",
      "Epoch 494/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.4695 - acc: 0.8745 - val_loss: 0.8270 - val_acc: 0.7000\n",
      "Epoch 495/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4900 - acc: 0.8460Epoch 00495: val_loss did not improve\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.4886 - acc: 0.8373 - val_loss: 0.8283 - val_acc: 0.7000\n",
      "Epoch 496/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.4811 - acc: 0.8431 - val_loss: 0.8289 - val_acc: 0.7111\n",
      "Epoch 497/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.4771 - acc: 0.8686 - val_loss: 0.8208 - val_acc: 0.7111\n",
      "Epoch 498/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.4716 - acc: 0.8706 - val_loss: 0.8234 - val_acc: 0.7111\n",
      "Epoch 499/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.4745 - acc: 0.8471 - val_loss: 0.8235 - val_acc: 0.7111\n",
      "Epoch 500/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.4295 - acc: 0.8802Epoch 00500: val_loss did not improve\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.4500 - acc: 0.8706 - val_loss: 0.8316 - val_acc: 0.7000\n",
      "Epoch 501/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.4537 - acc: 0.8706 - val_loss: 0.8226 - val_acc: 0.7000\n",
      "Epoch 502/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.4827 - acc: 0.8490 - val_loss: 0.8245 - val_acc: 0.7000\n",
      "Epoch 503/3000\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.4678 - acc: 0.8608 - val_loss: 0.8211 - val_acc: 0.7111\n",
      "Epoch 504/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.4664 - acc: 0.8608 - val_loss: 0.8258 - val_acc: 0.7000\n",
      "Epoch 505/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4546 - acc: 0.8728Epoch 00505: val_loss improved from 0.82214 to 0.82067, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 674us/step - loss: 0.4538 - acc: 0.8706 - val_loss: 0.8207 - val_acc: 0.7111\n",
      "Epoch 506/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.4960 - acc: 0.8451 - val_loss: 0.8201 - val_acc: 0.7111\n",
      "Epoch 507/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.4657 - acc: 0.8549 - val_loss: 0.8227 - val_acc: 0.7000\n",
      "Epoch 508/3000\n",
      "510/510 [==============================] - 0s 652us/step - loss: 0.4751 - acc: 0.8627 - val_loss: 0.8217 - val_acc: 0.7111\n",
      "Epoch 509/3000\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.4501 - acc: 0.8627 - val_loss: 0.8163 - val_acc: 0.7111\n",
      "Epoch 510/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4524 - acc: 0.8661Epoch 00510: val_loss improved from 0.82067 to 0.81757, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.4510 - acc: 0.8686 - val_loss: 0.8176 - val_acc: 0.7111\n",
      "Epoch 511/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.4319 - acc: 0.8824 - val_loss: 0.8142 - val_acc: 0.7111\n",
      "Epoch 512/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.4626 - acc: 0.8588 - val_loss: 0.8141 - val_acc: 0.7111\n",
      "Epoch 513/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.4586 - acc: 0.8725 - val_loss: 0.8202 - val_acc: 0.7111\n",
      "Epoch 514/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.4660 - acc: 0.8627 - val_loss: 0.8187 - val_acc: 0.7111\n",
      "Epoch 515/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4423 - acc: 0.8750Epoch 00515: val_loss improved from 0.81757 to 0.81609, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 673us/step - loss: 0.4402 - acc: 0.8765 - val_loss: 0.8161 - val_acc: 0.7111\n",
      "Epoch 516/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.4733 - acc: 0.8529 - val_loss: 0.8178 - val_acc: 0.6889\n",
      "Epoch 517/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.4527 - acc: 0.8588 - val_loss: 0.8178 - val_acc: 0.6889\n",
      "Epoch 518/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.4560 - acc: 0.8647 - val_loss: 0.8191 - val_acc: 0.6889\n",
      "Epoch 519/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.4669 - acc: 0.8549 - val_loss: 0.8162 - val_acc: 0.6889\n",
      "Epoch 520/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4540 - acc: 0.8616Epoch 00520: val_loss improved from 0.81609 to 0.81248, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 656us/step - loss: 0.4433 - acc: 0.8647 - val_loss: 0.8125 - val_acc: 0.7000\n",
      "Epoch 521/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.4202 - acc: 0.8980 - val_loss: 0.8142 - val_acc: 0.7000\n",
      "Epoch 522/3000\n",
      "510/510 [==============================] - 0s 647us/step - loss: 0.4339 - acc: 0.8725 - val_loss: 0.8159 - val_acc: 0.7000\n",
      "Epoch 523/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.4513 - acc: 0.8529 - val_loss: 0.8180 - val_acc: 0.7000\n",
      "Epoch 524/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.4390 - acc: 0.8647 - val_loss: 0.8152 - val_acc: 0.6778\n",
      "Epoch 525/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4371 - acc: 0.8638Epoch 00525: val_loss did not improve\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.4442 - acc: 0.8588 - val_loss: 0.8206 - val_acc: 0.6667\n",
      "Epoch 526/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.4212 - acc: 0.8902 - val_loss: 0.8275 - val_acc: 0.6667\n",
      "Epoch 527/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.4349 - acc: 0.8765 - val_loss: 0.8244 - val_acc: 0.6778\n",
      "Epoch 528/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.4473 - acc: 0.8569 - val_loss: 0.8229 - val_acc: 0.6778\n",
      "Epoch 529/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.4511 - acc: 0.8667 - val_loss: 0.8231 - val_acc: 0.6778\n",
      "Epoch 530/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4516 - acc: 0.8460Epoch 00530: val_loss did not improve\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.4589 - acc: 0.8431 - val_loss: 0.8174 - val_acc: 0.6778\n",
      "Epoch 531/3000\n",
      "510/510 [==============================] - 0s 673us/step - loss: 0.4489 - acc: 0.8510 - val_loss: 0.8208 - val_acc: 0.6778\n",
      "Epoch 532/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.4294 - acc: 0.8902 - val_loss: 0.8241 - val_acc: 0.6889\n",
      "Epoch 533/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.4392 - acc: 0.8784 - val_loss: 0.8279 - val_acc: 0.6778\n",
      "Epoch 534/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.4318 - acc: 0.8843 - val_loss: 0.8208 - val_acc: 0.7000\n",
      "Epoch 535/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4462 - acc: 0.8549Epoch 00535: val_loss did not improve\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.4455 - acc: 0.8569 - val_loss: 0.8164 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 536/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.4450 - acc: 0.8608 - val_loss: 0.8169 - val_acc: 0.6889\n",
      "Epoch 537/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.4270 - acc: 0.8588 - val_loss: 0.8217 - val_acc: 0.6889\n",
      "Epoch 538/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.4669 - acc: 0.8725 - val_loss: 0.8223 - val_acc: 0.6889\n",
      "Epoch 539/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.4466 - acc: 0.8549 - val_loss: 0.8288 - val_acc: 0.6778\n",
      "Epoch 540/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4353 - acc: 0.8594Epoch 00540: val_loss did not improve\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.4433 - acc: 0.8608 - val_loss: 0.8240 - val_acc: 0.6889\n",
      "Epoch 541/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.4674 - acc: 0.8549 - val_loss: 0.8293 - val_acc: 0.6778\n",
      "Epoch 542/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.4178 - acc: 0.8686 - val_loss: 0.8245 - val_acc: 0.7000\n",
      "Epoch 543/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.4106 - acc: 0.8843 - val_loss: 0.8211 - val_acc: 0.7000\n",
      "Epoch 544/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.4101 - acc: 0.8961 - val_loss: 0.8134 - val_acc: 0.7111\n",
      "Epoch 545/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4170 - acc: 0.8906Epoch 00545: val_loss did not improve\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.4137 - acc: 0.8961 - val_loss: 0.8164 - val_acc: 0.7000\n",
      "Epoch 546/3000\n",
      "510/510 [==============================] - 0s 720us/step - loss: 0.4420 - acc: 0.8529 - val_loss: 0.8241 - val_acc: 0.6778\n",
      "Epoch 547/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.4487 - acc: 0.8608 - val_loss: 0.8106 - val_acc: 0.7000\n",
      "Epoch 548/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.4155 - acc: 0.8980 - val_loss: 0.8111 - val_acc: 0.6889\n",
      "Epoch 549/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.4253 - acc: 0.8706 - val_loss: 0.8183 - val_acc: 0.7000\n",
      "Epoch 550/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4311 - acc: 0.8795Epoch 00550: val_loss did not improve\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.4301 - acc: 0.8804 - val_loss: 0.8236 - val_acc: 0.6889\n",
      "Epoch 551/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.4453 - acc: 0.8627 - val_loss: 0.8180 - val_acc: 0.7000\n",
      "Epoch 552/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.4213 - acc: 0.8765 - val_loss: 0.8231 - val_acc: 0.7000\n",
      "Epoch 553/3000\n",
      "510/510 [==============================] - 0s 658us/step - loss: 0.4012 - acc: 0.8863 - val_loss: 0.8260 - val_acc: 0.7000\n",
      "Epoch 554/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.4270 - acc: 0.8608 - val_loss: 0.8244 - val_acc: 0.7000\n",
      "Epoch 555/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4480 - acc: 0.8705Epoch 00555: val_loss did not improve\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.4422 - acc: 0.8706 - val_loss: 0.8259 - val_acc: 0.6889\n",
      "Epoch 556/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.4065 - acc: 0.8941 - val_loss: 0.8234 - val_acc: 0.7000\n",
      "Epoch 557/3000\n",
      "510/510 [==============================] - 0s 665us/step - loss: 0.4136 - acc: 0.8784 - val_loss: 0.8238 - val_acc: 0.7000\n",
      "Epoch 558/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.4503 - acc: 0.8667 - val_loss: 0.8224 - val_acc: 0.6889\n",
      "Epoch 559/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.4266 - acc: 0.8824 - val_loss: 0.8261 - val_acc: 0.6889\n",
      "Epoch 560/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4266 - acc: 0.8661Epoch 00560: val_loss did not improve\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.4166 - acc: 0.8745 - val_loss: 0.8210 - val_acc: 0.7000\n",
      "Epoch 561/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.4446 - acc: 0.8686 - val_loss: 0.8211 - val_acc: 0.7000\n",
      "Epoch 562/3000\n",
      "510/510 [==============================] - 0s 660us/step - loss: 0.4047 - acc: 0.9059 - val_loss: 0.8168 - val_acc: 0.7000\n",
      "Epoch 563/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.4343 - acc: 0.8549 - val_loss: 0.8190 - val_acc: 0.7000\n",
      "Epoch 564/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.4458 - acc: 0.8647 - val_loss: 0.8213 - val_acc: 0.7000\n",
      "Epoch 565/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4119 - acc: 0.8705Epoch 00565: val_loss did not improve\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.4179 - acc: 0.8667 - val_loss: 0.8126 - val_acc: 0.7000\n",
      "Epoch 566/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.3899 - acc: 0.8922 - val_loss: 0.8163 - val_acc: 0.7000\n",
      "Epoch 567/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.4278 - acc: 0.8686 - val_loss: 0.8215 - val_acc: 0.7000\n",
      "Epoch 568/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.3932 - acc: 0.8804 - val_loss: 0.8196 - val_acc: 0.7111\n",
      "Epoch 569/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.4208 - acc: 0.8843 - val_loss: 0.8196 - val_acc: 0.7000\n",
      "Epoch 570/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.4163 - acc: 0.8802Epoch 00570: val_loss did not improve\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.4171 - acc: 0.8863 - val_loss: 0.8147 - val_acc: 0.7000\n",
      "Epoch 571/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.3995 - acc: 0.8902 - val_loss: 0.8242 - val_acc: 0.7000\n",
      "Epoch 572/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.4124 - acc: 0.8725 - val_loss: 0.8233 - val_acc: 0.7000\n",
      "Epoch 573/3000\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.4449 - acc: 0.8627 - val_loss: 0.8262 - val_acc: 0.7000\n",
      "Epoch 574/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.4054 - acc: 0.9039 - val_loss: 0.8168 - val_acc: 0.7000\n",
      "Epoch 575/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3915 - acc: 0.8683Epoch 00575: val_loss did not improve\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.4075 - acc: 0.8627 - val_loss: 0.8175 - val_acc: 0.7111\n",
      "Epoch 576/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.4212 - acc: 0.8922 - val_loss: 0.8144 - val_acc: 0.7111\n",
      "Epoch 577/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.4243 - acc: 0.8902 - val_loss: 0.8121 - val_acc: 0.7000\n",
      "Epoch 578/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.3941 - acc: 0.8863 - val_loss: 0.8116 - val_acc: 0.6889\n",
      "Epoch 579/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.4449 - acc: 0.8686 - val_loss: 0.8165 - val_acc: 0.6889\n",
      "Epoch 580/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4074 - acc: 0.8661Epoch 00580: val_loss improved from 0.81248 to 0.80980, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 669us/step - loss: 0.4122 - acc: 0.8627 - val_loss: 0.8098 - val_acc: 0.7000\n",
      "Epoch 581/3000\n",
      "510/510 [==============================] - 0s 652us/step - loss: 0.4359 - acc: 0.8569 - val_loss: 0.8025 - val_acc: 0.7222\n",
      "Epoch 582/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.4260 - acc: 0.8745 - val_loss: 0.8097 - val_acc: 0.7111\n",
      "Epoch 583/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.4035 - acc: 0.8863 - val_loss: 0.8058 - val_acc: 0.7222\n",
      "Epoch 584/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.4381 - acc: 0.8765 - val_loss: 0.8033 - val_acc: 0.7222\n",
      "Epoch 585/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3902 - acc: 0.8951Epoch 00585: val_loss improved from 0.80980 to 0.80636, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 649us/step - loss: 0.3909 - acc: 0.8922 - val_loss: 0.8064 - val_acc: 0.7000\n",
      "Epoch 586/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 583us/step - loss: 0.4142 - acc: 0.8882 - val_loss: 0.8086 - val_acc: 0.7222\n",
      "Epoch 587/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.4166 - acc: 0.8725 - val_loss: 0.8113 - val_acc: 0.7000\n",
      "Epoch 588/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.4126 - acc: 0.8804 - val_loss: 0.8164 - val_acc: 0.7000\n",
      "Epoch 589/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.4387 - acc: 0.8627 - val_loss: 0.8123 - val_acc: 0.7111\n",
      "Epoch 590/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4082 - acc: 0.8705Epoch 00590: val_loss did not improve\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.4116 - acc: 0.8725 - val_loss: 0.8189 - val_acc: 0.7000\n",
      "Epoch 591/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.4128 - acc: 0.8784 - val_loss: 0.8268 - val_acc: 0.7000\n",
      "Epoch 592/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.4007 - acc: 0.8961 - val_loss: 0.8250 - val_acc: 0.6889\n",
      "Epoch 593/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.4162 - acc: 0.8824 - val_loss: 0.8099 - val_acc: 0.7000\n",
      "Epoch 594/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.4032 - acc: 0.8686 - val_loss: 0.8123 - val_acc: 0.7000\n",
      "Epoch 595/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4277 - acc: 0.8817Epoch 00595: val_loss did not improve\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.4132 - acc: 0.8922 - val_loss: 0.8110 - val_acc: 0.6889\n",
      "Epoch 596/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.4109 - acc: 0.8784 - val_loss: 0.8060 - val_acc: 0.6889\n",
      "Epoch 597/3000\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.3920 - acc: 0.8843 - val_loss: 0.8027 - val_acc: 0.7111\n",
      "Epoch 598/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.4125 - acc: 0.8804 - val_loss: 0.8061 - val_acc: 0.7111\n",
      "Epoch 599/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.4061 - acc: 0.8647 - val_loss: 0.8049 - val_acc: 0.7111\n",
      "Epoch 600/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3752 - acc: 0.8929Epoch 00600: val_loss improved from 0.80636 to 0.80114, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.3795 - acc: 0.8882 - val_loss: 0.8011 - val_acc: 0.7111\n",
      "Epoch 601/3000\n",
      "510/510 [==============================] - 0s 653us/step - loss: 0.4151 - acc: 0.8843 - val_loss: 0.7987 - val_acc: 0.7111\n",
      "Epoch 602/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.4051 - acc: 0.8922 - val_loss: 0.8038 - val_acc: 0.7111\n",
      "Epoch 603/3000\n",
      "510/510 [==============================] - 0s 656us/step - loss: 0.3894 - acc: 0.8961 - val_loss: 0.8070 - val_acc: 0.7000\n",
      "Epoch 604/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.3843 - acc: 0.8941 - val_loss: 0.8072 - val_acc: 0.6889\n",
      "Epoch 605/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4063 - acc: 0.8817Epoch 00605: val_loss did not improve\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.4037 - acc: 0.8863 - val_loss: 0.8074 - val_acc: 0.7111\n",
      "Epoch 606/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.4070 - acc: 0.8765 - val_loss: 0.8063 - val_acc: 0.7000\n",
      "Epoch 607/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.3965 - acc: 0.8843 - val_loss: 0.8046 - val_acc: 0.7111\n",
      "Epoch 608/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.4159 - acc: 0.8706 - val_loss: 0.8050 - val_acc: 0.7111\n",
      "Epoch 609/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.3780 - acc: 0.8941 - val_loss: 0.8005 - val_acc: 0.7111\n",
      "Epoch 610/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3693 - acc: 0.9018Epoch 00610: val_loss did not improve\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.3775 - acc: 0.8980 - val_loss: 0.8038 - val_acc: 0.7111\n",
      "Epoch 611/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.4087 - acc: 0.8784 - val_loss: 0.8055 - val_acc: 0.7000\n",
      "Epoch 612/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.3917 - acc: 0.8863 - val_loss: 0.8088 - val_acc: 0.7000\n",
      "Epoch 613/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.3814 - acc: 0.9000 - val_loss: 0.8126 - val_acc: 0.7000\n",
      "Epoch 614/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.4029 - acc: 0.8980 - val_loss: 0.8111 - val_acc: 0.6889\n",
      "Epoch 615/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4223 - acc: 0.8795Epoch 00615: val_loss did not improve\n",
      "510/510 [==============================] - 0s 663us/step - loss: 0.4134 - acc: 0.8863 - val_loss: 0.8012 - val_acc: 0.7000\n",
      "Epoch 616/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.4022 - acc: 0.8824 - val_loss: 0.7999 - val_acc: 0.7000\n",
      "Epoch 617/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.3917 - acc: 0.8882 - val_loss: 0.7973 - val_acc: 0.7111\n",
      "Epoch 618/3000\n",
      "510/510 [==============================] - 0s 661us/step - loss: 0.3962 - acc: 0.8882 - val_loss: 0.8018 - val_acc: 0.7222\n",
      "Epoch 619/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.3978 - acc: 0.8784 - val_loss: 0.7994 - val_acc: 0.7222\n",
      "Epoch 620/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3956 - acc: 0.8951Epoch 00620: val_loss improved from 0.80114 to 0.79503, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 670us/step - loss: 0.3881 - acc: 0.8980 - val_loss: 0.7950 - val_acc: 0.7222\n",
      "Epoch 621/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.3826 - acc: 0.8843 - val_loss: 0.7970 - val_acc: 0.7111\n",
      "Epoch 622/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.3689 - acc: 0.9059 - val_loss: 0.7973 - val_acc: 0.7222\n",
      "Epoch 623/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.3719 - acc: 0.9039 - val_loss: 0.7969 - val_acc: 0.7111\n",
      "Epoch 624/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.3995 - acc: 0.8706 - val_loss: 0.7995 - val_acc: 0.7222\n",
      "Epoch 625/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3986 - acc: 0.8750Epoch 00625: val_loss did not improve\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.4062 - acc: 0.8765 - val_loss: 0.8008 - val_acc: 0.7111\n",
      "Epoch 626/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.4049 - acc: 0.8745 - val_loss: 0.7962 - val_acc: 0.7222\n",
      "Epoch 627/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.3748 - acc: 0.8980 - val_loss: 0.7871 - val_acc: 0.7111\n",
      "Epoch 628/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.3943 - acc: 0.8686 - val_loss: 0.7882 - val_acc: 0.7000\n",
      "Epoch 629/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.4092 - acc: 0.8725 - val_loss: 0.7983 - val_acc: 0.7111\n",
      "Epoch 630/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3845 - acc: 0.9018Epoch 00630: val_loss improved from 0.79503 to 0.79106, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.3952 - acc: 0.8961 - val_loss: 0.7911 - val_acc: 0.7111\n",
      "Epoch 631/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.3993 - acc: 0.8882 - val_loss: 0.7851 - val_acc: 0.7000\n",
      "Epoch 632/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.3606 - acc: 0.9137 - val_loss: 0.7883 - val_acc: 0.7111\n",
      "Epoch 633/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.3861 - acc: 0.8882 - val_loss: 0.7878 - val_acc: 0.7000\n",
      "Epoch 634/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.3696 - acc: 0.9039 - val_loss: 0.7955 - val_acc: 0.7222\n",
      "Epoch 635/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.3361 - acc: 0.9219Epoch 00635: val_loss did not improve\n",
      "510/510 [==============================] - 0s 662us/step - loss: 0.3464 - acc: 0.9118 - val_loss: 0.7979 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 636/3000\n",
      "510/510 [==============================] - 0s 543us/step - loss: 0.3746 - acc: 0.8980 - val_loss: 0.8009 - val_acc: 0.6778\n",
      "Epoch 637/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.4009 - acc: 0.8824 - val_loss: 0.8025 - val_acc: 0.6889\n",
      "Epoch 638/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.4078 - acc: 0.8804 - val_loss: 0.7983 - val_acc: 0.6889\n",
      "Epoch 639/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.3727 - acc: 0.9000 - val_loss: 0.7911 - val_acc: 0.7000\n",
      "Epoch 640/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3922 - acc: 0.8839Epoch 00640: val_loss improved from 0.79106 to 0.78855, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.3931 - acc: 0.8824 - val_loss: 0.7885 - val_acc: 0.7000\n",
      "Epoch 641/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.4008 - acc: 0.8725 - val_loss: 0.7951 - val_acc: 0.7000\n",
      "Epoch 642/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.3615 - acc: 0.9157 - val_loss: 0.7956 - val_acc: 0.7000\n",
      "Epoch 643/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.3897 - acc: 0.8882 - val_loss: 0.7905 - val_acc: 0.7000\n",
      "Epoch 644/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.3879 - acc: 0.8784 - val_loss: 0.7901 - val_acc: 0.6889\n",
      "Epoch 645/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3520 - acc: 0.9085Epoch 00645: val_loss improved from 0.78855 to 0.78380, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 682us/step - loss: 0.3666 - acc: 0.8980 - val_loss: 0.7838 - val_acc: 0.7000\n",
      "Epoch 646/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.3665 - acc: 0.9000 - val_loss: 0.7862 - val_acc: 0.7000\n",
      "Epoch 647/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.3810 - acc: 0.9020 - val_loss: 0.7882 - val_acc: 0.7000\n",
      "Epoch 648/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.3643 - acc: 0.8980 - val_loss: 0.7860 - val_acc: 0.6889\n",
      "Epoch 649/3000\n",
      "510/510 [==============================] - 0s 649us/step - loss: 0.3645 - acc: 0.8941 - val_loss: 0.7902 - val_acc: 0.6778\n",
      "Epoch 650/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3885 - acc: 0.8884Epoch 00650: val_loss did not improve\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.3895 - acc: 0.8922 - val_loss: 0.7860 - val_acc: 0.6889\n",
      "Epoch 651/3000\n",
      "510/510 [==============================] - 0s 654us/step - loss: 0.3652 - acc: 0.9078 - val_loss: 0.7865 - val_acc: 0.6778\n",
      "Epoch 652/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.3676 - acc: 0.9039 - val_loss: 0.7777 - val_acc: 0.7000\n",
      "Epoch 653/3000\n",
      "510/510 [==============================] - 0s 653us/step - loss: 0.3721 - acc: 0.9078 - val_loss: 0.7768 - val_acc: 0.7111\n",
      "Epoch 654/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.3981 - acc: 0.8745 - val_loss: 0.7701 - val_acc: 0.7111\n",
      "Epoch 655/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3779 - acc: 0.8906Epoch 00655: val_loss improved from 0.78380 to 0.76990, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.3712 - acc: 0.8941 - val_loss: 0.7699 - val_acc: 0.7222\n",
      "Epoch 656/3000\n",
      "510/510 [==============================] - 0s 669us/step - loss: 0.3742 - acc: 0.9020 - val_loss: 0.7805 - val_acc: 0.7222\n",
      "Epoch 657/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.3777 - acc: 0.8922 - val_loss: 0.7742 - val_acc: 0.7222\n",
      "Epoch 658/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.3879 - acc: 0.8686 - val_loss: 0.7731 - val_acc: 0.7222\n",
      "Epoch 659/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.3783 - acc: 0.8804 - val_loss: 0.7736 - val_acc: 0.7222\n",
      "Epoch 660/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3799 - acc: 0.8817Epoch 00660: val_loss did not improve\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.3824 - acc: 0.8784 - val_loss: 0.7778 - val_acc: 0.7111\n",
      "Epoch 661/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.3466 - acc: 0.9039 - val_loss: 0.7770 - val_acc: 0.7222\n",
      "Epoch 662/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.3823 - acc: 0.8941 - val_loss: 0.7829 - val_acc: 0.7111\n",
      "Epoch 663/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.3841 - acc: 0.8843 - val_loss: 0.7825 - val_acc: 0.7000\n",
      "Epoch 664/3000\n",
      "510/510 [==============================] - 0s 713us/step - loss: 0.3701 - acc: 0.9020 - val_loss: 0.7829 - val_acc: 0.7111\n",
      "Epoch 665/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3510 - acc: 0.8862Epoch 00665: val_loss did not improve\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.3559 - acc: 0.8882 - val_loss: 0.7821 - val_acc: 0.7222\n",
      "Epoch 666/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.3841 - acc: 0.8824 - val_loss: 0.7816 - val_acc: 0.7111\n",
      "Epoch 667/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.3619 - acc: 0.9020 - val_loss: 0.7808 - val_acc: 0.7222\n",
      "Epoch 668/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.3588 - acc: 0.8902 - val_loss: 0.7869 - val_acc: 0.7111\n",
      "Epoch 669/3000\n",
      "510/510 [==============================] - 0s 655us/step - loss: 0.3594 - acc: 0.8784 - val_loss: 0.7821 - val_acc: 0.7222\n",
      "Epoch 670/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3736 - acc: 0.8973Epoch 00670: val_loss did not improve\n",
      "510/510 [==============================] - 0s 673us/step - loss: 0.3690 - acc: 0.8961 - val_loss: 0.7764 - val_acc: 0.7111\n",
      "Epoch 671/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.3752 - acc: 0.8745 - val_loss: 0.7767 - val_acc: 0.7222\n",
      "Epoch 672/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.3712 - acc: 0.8863 - val_loss: 0.7752 - val_acc: 0.7222\n",
      "Epoch 673/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.3677 - acc: 0.9000 - val_loss: 0.7671 - val_acc: 0.7222\n",
      "Epoch 674/3000\n",
      "510/510 [==============================] - 0s 690us/step - loss: 0.3680 - acc: 0.8843 - val_loss: 0.7663 - val_acc: 0.7333\n",
      "Epoch 675/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3461 - acc: 0.9129Epoch 00675: val_loss improved from 0.76990 to 0.76898, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.3472 - acc: 0.9098 - val_loss: 0.7690 - val_acc: 0.7333\n",
      "Epoch 676/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.3561 - acc: 0.9000 - val_loss: 0.7705 - val_acc: 0.7222\n",
      "Epoch 677/3000\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.3892 - acc: 0.8706 - val_loss: 0.7707 - val_acc: 0.7111\n",
      "Epoch 678/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.3910 - acc: 0.8824 - val_loss: 0.7765 - val_acc: 0.7222\n",
      "Epoch 679/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.3733 - acc: 0.8882 - val_loss: 0.7767 - val_acc: 0.7222\n",
      "Epoch 680/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3703 - acc: 0.8906Epoch 00680: val_loss did not improve\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.3726 - acc: 0.8980 - val_loss: 0.7715 - val_acc: 0.7222\n",
      "Epoch 681/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.3667 - acc: 0.8961 - val_loss: 0.7724 - val_acc: 0.7222\n",
      "Epoch 682/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.3663 - acc: 0.8980 - val_loss: 0.7735 - val_acc: 0.7222\n",
      "Epoch 683/3000\n",
      "510/510 [==============================] - 0s 666us/step - loss: 0.3505 - acc: 0.9157 - val_loss: 0.7747 - val_acc: 0.7222\n",
      "Epoch 684/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.3649 - acc: 0.9020 - val_loss: 0.7784 - val_acc: 0.7222\n",
      "Epoch 685/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.3291 - acc: 0.9271Epoch 00685: val_loss did not improve\n",
      "510/510 [==============================] - 0s 656us/step - loss: 0.3329 - acc: 0.9255 - val_loss: 0.7783 - val_acc: 0.7222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 686/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.3690 - acc: 0.8922 - val_loss: 0.7716 - val_acc: 0.7333\n",
      "Epoch 687/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.3656 - acc: 0.9020 - val_loss: 0.7710 - val_acc: 0.7333\n",
      "Epoch 688/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.3607 - acc: 0.8941 - val_loss: 0.7674 - val_acc: 0.7333\n",
      "Epoch 689/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.3606 - acc: 0.9000 - val_loss: 0.7668 - val_acc: 0.7333\n",
      "Epoch 690/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3682 - acc: 0.8929Epoch 00690: val_loss improved from 0.76898 to 0.76121, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.3654 - acc: 0.8961 - val_loss: 0.7612 - val_acc: 0.7333\n",
      "Epoch 691/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.3744 - acc: 0.8882 - val_loss: 0.7727 - val_acc: 0.7222\n",
      "Epoch 692/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.3713 - acc: 0.8941 - val_loss: 0.7666 - val_acc: 0.7333\n",
      "Epoch 693/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.3533 - acc: 0.9098 - val_loss: 0.7671 - val_acc: 0.7222\n",
      "Epoch 694/3000\n",
      "510/510 [==============================] - 0s 649us/step - loss: 0.3545 - acc: 0.8980 - val_loss: 0.7658 - val_acc: 0.7222\n",
      "Epoch 695/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3852 - acc: 0.8772Epoch 00695: val_loss did not improve\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.3825 - acc: 0.8804 - val_loss: 0.7647 - val_acc: 0.7222\n",
      "Epoch 696/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.3460 - acc: 0.9000 - val_loss: 0.7657 - val_acc: 0.7222\n",
      "Epoch 697/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.3323 - acc: 0.9157 - val_loss: 0.7600 - val_acc: 0.7222\n",
      "Epoch 698/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.3509 - acc: 0.8980 - val_loss: 0.7605 - val_acc: 0.7222\n",
      "Epoch 699/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.3469 - acc: 0.9000 - val_loss: 0.7697 - val_acc: 0.7111\n",
      "Epoch 700/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.3602 - acc: 0.9036Epoch 00700: val_loss did not improve\n",
      "510/510 [==============================] - 0s 667us/step - loss: 0.3500 - acc: 0.9059 - val_loss: 0.7644 - val_acc: 0.7222\n",
      "Epoch 701/3000\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.3475 - acc: 0.8922 - val_loss: 0.7642 - val_acc: 0.7111\n",
      "Epoch 702/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.3765 - acc: 0.8784 - val_loss: 0.7553 - val_acc: 0.7222\n",
      "Epoch 703/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.3381 - acc: 0.9235 - val_loss: 0.7482 - val_acc: 0.7222\n",
      "Epoch 704/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.3694 - acc: 0.8843 - val_loss: 0.7472 - val_acc: 0.7222\n",
      "Epoch 705/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3549 - acc: 0.9152Epoch 00705: val_loss improved from 0.76121 to 0.75545, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.3552 - acc: 0.9137 - val_loss: 0.7555 - val_acc: 0.7222\n",
      "Epoch 706/3000\n",
      "510/510 [==============================] - 0s 662us/step - loss: 0.3409 - acc: 0.8980 - val_loss: 0.7507 - val_acc: 0.7222\n",
      "Epoch 707/3000\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.3396 - acc: 0.9137 - val_loss: 0.7461 - val_acc: 0.7222\n",
      "Epoch 708/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.3485 - acc: 0.8902 - val_loss: 0.7556 - val_acc: 0.7333\n",
      "Epoch 709/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.3648 - acc: 0.8922 - val_loss: 0.7528 - val_acc: 0.7444\n",
      "Epoch 710/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3650 - acc: 0.8906Epoch 00710: val_loss improved from 0.75545 to 0.75359, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.3632 - acc: 0.8902 - val_loss: 0.7536 - val_acc: 0.7222\n",
      "Epoch 711/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.3871 - acc: 0.8706 - val_loss: 0.7494 - val_acc: 0.7222\n",
      "Epoch 712/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.3580 - acc: 0.8941 - val_loss: 0.7590 - val_acc: 0.7222\n",
      "Epoch 713/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.3619 - acc: 0.9000 - val_loss: 0.7645 - val_acc: 0.7111\n",
      "Epoch 714/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.3621 - acc: 0.8824 - val_loss: 0.7671 - val_acc: 0.7111\n",
      "Epoch 715/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3685 - acc: 0.8951Epoch 00715: val_loss did not improve\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.3584 - acc: 0.9020 - val_loss: 0.7643 - val_acc: 0.7111\n",
      "Epoch 716/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.3538 - acc: 0.8961 - val_loss: 0.7616 - val_acc: 0.7111\n",
      "Epoch 717/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.3342 - acc: 0.9000 - val_loss: 0.7624 - val_acc: 0.7222\n",
      "Epoch 718/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.3534 - acc: 0.8843 - val_loss: 0.7628 - val_acc: 0.7222\n",
      "Epoch 719/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.3430 - acc: 0.9078 - val_loss: 0.7547 - val_acc: 0.7111\n",
      "Epoch 720/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3467 - acc: 0.9129Epoch 00720: val_loss did not improve\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.3497 - acc: 0.9078 - val_loss: 0.7633 - val_acc: 0.7222\n",
      "Epoch 721/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.3376 - acc: 0.9059 - val_loss: 0.7712 - val_acc: 0.7222\n",
      "Epoch 722/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.3514 - acc: 0.9059 - val_loss: 0.7634 - val_acc: 0.7333\n",
      "Epoch 723/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.3517 - acc: 0.8922 - val_loss: 0.7641 - val_acc: 0.7222\n",
      "Epoch 724/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.3623 - acc: 0.8863 - val_loss: 0.7597 - val_acc: 0.7333\n",
      "Epoch 725/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3306 - acc: 0.9129Epoch 00725: val_loss did not improve\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.3314 - acc: 0.9118 - val_loss: 0.7537 - val_acc: 0.7222\n",
      "Epoch 726/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.3469 - acc: 0.9000 - val_loss: 0.7537 - val_acc: 0.7333\n",
      "Epoch 727/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.3677 - acc: 0.8980 - val_loss: 0.7521 - val_acc: 0.7222\n",
      "Epoch 728/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.3308 - acc: 0.9059 - val_loss: 0.7507 - val_acc: 0.7111\n",
      "Epoch 729/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.3318 - acc: 0.9157 - val_loss: 0.7534 - val_acc: 0.7000\n",
      "Epoch 730/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3493 - acc: 0.8973Epoch 00730: val_loss improved from 0.75359 to 0.74871, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.3498 - acc: 0.9000 - val_loss: 0.7487 - val_acc: 0.7111\n",
      "Epoch 731/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.3289 - acc: 0.9255 - val_loss: 0.7485 - val_acc: 0.7111\n",
      "Epoch 732/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.3440 - acc: 0.9098 - val_loss: 0.7506 - val_acc: 0.7111\n",
      "Epoch 733/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.3436 - acc: 0.8980 - val_loss: 0.7535 - val_acc: 0.7111\n",
      "Epoch 734/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.3308 - acc: 0.9078 - val_loss: 0.7469 - val_acc: 0.7111\n",
      "Epoch 735/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3339 - acc: 0.9129Epoch 00735: val_loss did not improve\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.3342 - acc: 0.9098 - val_loss: 0.7517 - val_acc: 0.7111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 736/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.3107 - acc: 0.9216 - val_loss: 0.7466 - val_acc: 0.7000\n",
      "Epoch 737/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.3423 - acc: 0.8922 - val_loss: 0.7456 - val_acc: 0.7111\n",
      "Epoch 738/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.3696 - acc: 0.8765 - val_loss: 0.7432 - val_acc: 0.7222\n",
      "Epoch 739/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.3277 - acc: 0.9255 - val_loss: 0.7476 - val_acc: 0.7333\n",
      "Epoch 740/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3374 - acc: 0.9129Epoch 00740: val_loss improved from 0.74871 to 0.73891, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 637us/step - loss: 0.3275 - acc: 0.9176 - val_loss: 0.7389 - val_acc: 0.7222\n",
      "Epoch 741/3000\n",
      "510/510 [==============================] - 0s 555us/step - loss: 0.3589 - acc: 0.8863 - val_loss: 0.7354 - val_acc: 0.7333\n",
      "Epoch 742/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.3527 - acc: 0.8882 - val_loss: 0.7483 - val_acc: 0.7222\n",
      "Epoch 743/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.3431 - acc: 0.8922 - val_loss: 0.7423 - val_acc: 0.7222\n",
      "Epoch 744/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.3198 - acc: 0.9196 - val_loss: 0.7469 - val_acc: 0.7111\n",
      "Epoch 745/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3724 - acc: 0.8728Epoch 00745: val_loss improved from 0.73891 to 0.73744, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.3667 - acc: 0.8804 - val_loss: 0.7374 - val_acc: 0.7444\n",
      "Epoch 746/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.3349 - acc: 0.9078 - val_loss: 0.7384 - val_acc: 0.7333\n",
      "Epoch 747/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.3355 - acc: 0.9098 - val_loss: 0.7413 - val_acc: 0.7333\n",
      "Epoch 748/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.3365 - acc: 0.9255 - val_loss: 0.7424 - val_acc: 0.7333\n",
      "Epoch 749/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.3355 - acc: 0.9020 - val_loss: 0.7346 - val_acc: 0.7444\n",
      "Epoch 750/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3091 - acc: 0.9263Epoch 00750: val_loss improved from 0.73744 to 0.73446, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.3206 - acc: 0.9118 - val_loss: 0.7345 - val_acc: 0.7222\n",
      "Epoch 751/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.3327 - acc: 0.9137 - val_loss: 0.7377 - val_acc: 0.7333\n",
      "Epoch 752/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.3226 - acc: 0.9118 - val_loss: 0.7314 - val_acc: 0.7333\n",
      "Epoch 753/3000\n",
      "510/510 [==============================] - 0s 533us/step - loss: 0.3183 - acc: 0.9157 - val_loss: 0.7445 - val_acc: 0.7333\n",
      "Epoch 754/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.3224 - acc: 0.9275 - val_loss: 0.7453 - val_acc: 0.7333\n",
      "Epoch 755/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3383 - acc: 0.8973Epoch 00755: val_loss did not improve\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.3302 - acc: 0.9039 - val_loss: 0.7466 - val_acc: 0.7444\n",
      "Epoch 756/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.3693 - acc: 0.8725 - val_loss: 0.7460 - val_acc: 0.7333\n",
      "Epoch 757/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.3451 - acc: 0.9020 - val_loss: 0.7569 - val_acc: 0.7444\n",
      "Epoch 758/3000\n",
      "510/510 [==============================] - 0s 568us/step - loss: 0.3229 - acc: 0.9020 - val_loss: 0.7495 - val_acc: 0.7444\n",
      "Epoch 759/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.3411 - acc: 0.8902 - val_loss: 0.7465 - val_acc: 0.7444\n",
      "Epoch 760/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3512 - acc: 0.8973Epoch 00760: val_loss did not improve\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.3523 - acc: 0.8902 - val_loss: 0.7509 - val_acc: 0.7222\n",
      "Epoch 761/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.3372 - acc: 0.9039 - val_loss: 0.7441 - val_acc: 0.7333\n",
      "Epoch 762/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.3268 - acc: 0.9059 - val_loss: 0.7466 - val_acc: 0.7333\n",
      "Epoch 763/3000\n",
      "510/510 [==============================] - 0s 552us/step - loss: 0.3223 - acc: 0.9196 - val_loss: 0.7426 - val_acc: 0.7333\n",
      "Epoch 764/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.3281 - acc: 0.9059 - val_loss: 0.7392 - val_acc: 0.7333\n",
      "Epoch 765/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3188 - acc: 0.9241Epoch 00765: val_loss did not improve\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.3100 - acc: 0.9294 - val_loss: 0.7446 - val_acc: 0.7333\n",
      "Epoch 766/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.3260 - acc: 0.9098 - val_loss: 0.7381 - val_acc: 0.7333\n",
      "Epoch 767/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.3362 - acc: 0.8980 - val_loss: 0.7496 - val_acc: 0.7222\n",
      "Epoch 768/3000\n",
      "510/510 [==============================] - 0s 547us/step - loss: 0.3322 - acc: 0.9000 - val_loss: 0.7480 - val_acc: 0.7111\n",
      "Epoch 769/3000\n",
      "510/510 [==============================] - 0s 555us/step - loss: 0.3258 - acc: 0.9098 - val_loss: 0.7540 - val_acc: 0.7222\n",
      "Epoch 770/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3161 - acc: 0.8973Epoch 00770: val_loss did not improve\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.3162 - acc: 0.9000 - val_loss: 0.7523 - val_acc: 0.7333\n",
      "Epoch 771/3000\n",
      "510/510 [==============================] - 0s 654us/step - loss: 0.3181 - acc: 0.8961 - val_loss: 0.7537 - val_acc: 0.7333\n",
      "Epoch 772/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.3265 - acc: 0.9137 - val_loss: 0.7544 - val_acc: 0.7111\n",
      "Epoch 773/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.3550 - acc: 0.8941 - val_loss: 0.7501 - val_acc: 0.7222\n",
      "Epoch 774/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.3194 - acc: 0.9157 - val_loss: 0.7530 - val_acc: 0.7222\n",
      "Epoch 775/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3503 - acc: 0.8929Epoch 00775: val_loss did not improve\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.3480 - acc: 0.8922 - val_loss: 0.7529 - val_acc: 0.7333\n",
      "Epoch 776/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.3038 - acc: 0.9196 - val_loss: 0.7474 - val_acc: 0.7333\n",
      "Epoch 777/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.3230 - acc: 0.9216 - val_loss: 0.7459 - val_acc: 0.7222\n",
      "Epoch 778/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.3427 - acc: 0.9020 - val_loss: 0.7441 - val_acc: 0.7222\n",
      "Epoch 779/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.3427 - acc: 0.9059 - val_loss: 0.7398 - val_acc: 0.7222\n",
      "Epoch 780/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3101 - acc: 0.8996Epoch 00780: val_loss did not improve\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.3144 - acc: 0.9000 - val_loss: 0.7412 - val_acc: 0.7333\n",
      "Epoch 781/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.3200 - acc: 0.9078 - val_loss: 0.7373 - val_acc: 0.7222\n",
      "Epoch 782/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.3157 - acc: 0.9373 - val_loss: 0.7450 - val_acc: 0.7222\n",
      "Epoch 783/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.3121 - acc: 0.9059 - val_loss: 0.7441 - val_acc: 0.7222\n",
      "Epoch 784/3000\n",
      "510/510 [==============================] - 0s 550us/step - loss: 0.3158 - acc: 0.9176 - val_loss: 0.7413 - val_acc: 0.7222\n",
      "Epoch 785/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3239 - acc: 0.8996Epoch 00785: val_loss did not improve\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.3254 - acc: 0.9000 - val_loss: 0.7443 - val_acc: 0.7222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 786/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.3112 - acc: 0.9196 - val_loss: 0.7446 - val_acc: 0.7444\n",
      "Epoch 787/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.3187 - acc: 0.9039 - val_loss: 0.7510 - val_acc: 0.7333\n",
      "Epoch 788/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.3239 - acc: 0.8882 - val_loss: 0.7435 - val_acc: 0.7111\n",
      "Epoch 789/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.3124 - acc: 0.9196 - val_loss: 0.7387 - val_acc: 0.7222\n",
      "Epoch 790/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3198 - acc: 0.9062Epoch 00790: val_loss did not improve\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.3151 - acc: 0.9078 - val_loss: 0.7365 - val_acc: 0.7333\n",
      "Epoch 791/3000\n",
      "510/510 [==============================] - 0s 568us/step - loss: 0.3349 - acc: 0.8902 - val_loss: 0.7439 - val_acc: 0.7333\n",
      "Epoch 792/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.3036 - acc: 0.9235 - val_loss: 0.7376 - val_acc: 0.7444\n",
      "Epoch 793/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.2996 - acc: 0.9216 - val_loss: 0.7394 - val_acc: 0.7222\n",
      "Epoch 794/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.3117 - acc: 0.9275 - val_loss: 0.7431 - val_acc: 0.7111\n",
      "Epoch 795/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3198 - acc: 0.9129Epoch 00795: val_loss did not improve\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.3252 - acc: 0.9118 - val_loss: 0.7412 - val_acc: 0.7222\n",
      "Epoch 796/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.3081 - acc: 0.9235 - val_loss: 0.7396 - val_acc: 0.7111\n",
      "Epoch 797/3000\n",
      "510/510 [==============================] - 0s 561us/step - loss: 0.3145 - acc: 0.9059 - val_loss: 0.7386 - val_acc: 0.7111\n",
      "Epoch 798/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.3140 - acc: 0.9098 - val_loss: 0.7481 - val_acc: 0.7111\n",
      "Epoch 799/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.3088 - acc: 0.9078 - val_loss: 0.7448 - val_acc: 0.7111\n",
      "Epoch 800/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3755 - acc: 0.8817Epoch 00800: val_loss did not improve\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.3616 - acc: 0.8902 - val_loss: 0.7449 - val_acc: 0.7111\n",
      "Epoch 801/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.3024 - acc: 0.9275 - val_loss: 0.7394 - val_acc: 0.7111\n",
      "Epoch 802/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.2740 - acc: 0.9314 - val_loss: 0.7406 - val_acc: 0.7333\n",
      "Epoch 803/3000\n",
      "510/510 [==============================] - 0s 715us/step - loss: 0.3282 - acc: 0.9039 - val_loss: 0.7425 - val_acc: 0.7333\n",
      "Epoch 804/3000\n",
      "510/510 [==============================] - 0s 665us/step - loss: 0.3226 - acc: 0.9059 - val_loss: 0.7401 - val_acc: 0.7222\n",
      "Epoch 805/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.2936 - acc: 0.9219Epoch 00805: val_loss improved from 0.73446 to 0.72968, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 647us/step - loss: 0.2927 - acc: 0.9216 - val_loss: 0.7297 - val_acc: 0.7444\n",
      "Epoch 806/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.3164 - acc: 0.9137 - val_loss: 0.7358 - val_acc: 0.7333\n",
      "Epoch 807/3000\n",
      "510/510 [==============================] - 0s 702us/step - loss: 0.3224 - acc: 0.9157 - val_loss: 0.7337 - val_acc: 0.7556\n",
      "Epoch 808/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.3159 - acc: 0.9000 - val_loss: 0.7353 - val_acc: 0.7444\n",
      "Epoch 809/3000\n",
      "510/510 [==============================] - 0s 692us/step - loss: 0.3120 - acc: 0.9255 - val_loss: 0.7466 - val_acc: 0.7333\n",
      "Epoch 810/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2956 - acc: 0.9129Epoch 00810: val_loss did not improve\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2903 - acc: 0.9137 - val_loss: 0.7393 - val_acc: 0.7444\n",
      "Epoch 811/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.3124 - acc: 0.9294 - val_loss: 0.7408 - val_acc: 0.7333\n",
      "Epoch 812/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.3119 - acc: 0.9196 - val_loss: 0.7370 - val_acc: 0.7556\n",
      "Epoch 813/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.3170 - acc: 0.9020 - val_loss: 0.7353 - val_acc: 0.7444\n",
      "Epoch 814/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.2992 - acc: 0.9235 - val_loss: 0.7447 - val_acc: 0.7333\n",
      "Epoch 815/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3087 - acc: 0.9129Epoch 00815: val_loss did not improve\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.3081 - acc: 0.9118 - val_loss: 0.7425 - val_acc: 0.7333\n",
      "Epoch 816/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.2891 - acc: 0.9196 - val_loss: 0.7390 - val_acc: 0.7556\n",
      "Epoch 817/3000\n",
      "510/510 [==============================] - 0s 650us/step - loss: 0.3071 - acc: 0.9118 - val_loss: 0.7338 - val_acc: 0.7222\n",
      "Epoch 818/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.3226 - acc: 0.9000 - val_loss: 0.7398 - val_acc: 0.7333\n",
      "Epoch 819/3000\n",
      "510/510 [==============================] - 0s 645us/step - loss: 0.3097 - acc: 0.9294 - val_loss: 0.7403 - val_acc: 0.7333\n",
      "Epoch 820/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2845 - acc: 0.9152Epoch 00820: val_loss did not improve\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.2984 - acc: 0.9098 - val_loss: 0.7383 - val_acc: 0.7556\n",
      "Epoch 821/3000\n",
      "510/510 [==============================] - 0s 650us/step - loss: 0.3061 - acc: 0.9137 - val_loss: 0.7321 - val_acc: 0.7556\n",
      "Epoch 822/3000\n",
      "510/510 [==============================] - 0s 673us/step - loss: 0.3164 - acc: 0.9098 - val_loss: 0.7290 - val_acc: 0.7667\n",
      "Epoch 823/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.3346 - acc: 0.8902 - val_loss: 0.7343 - val_acc: 0.7667\n",
      "Epoch 824/3000\n",
      "510/510 [==============================] - 0s 698us/step - loss: 0.3436 - acc: 0.8941 - val_loss: 0.7348 - val_acc: 0.7667\n",
      "Epoch 825/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3081 - acc: 0.9241Epoch 00825: val_loss did not improve\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.3162 - acc: 0.9176 - val_loss: 0.7356 - val_acc: 0.7333\n",
      "Epoch 826/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.3118 - acc: 0.9078 - val_loss: 0.7319 - val_acc: 0.7556\n",
      "Epoch 827/3000\n",
      "510/510 [==============================] - 0s 673us/step - loss: 0.3164 - acc: 0.8961 - val_loss: 0.7361 - val_acc: 0.7556\n",
      "Epoch 828/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.3001 - acc: 0.9235 - val_loss: 0.7284 - val_acc: 0.7667\n",
      "Epoch 829/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.3087 - acc: 0.9216 - val_loss: 0.7287 - val_acc: 0.7667\n",
      "Epoch 830/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2904 - acc: 0.9353Epoch 00830: val_loss improved from 0.72968 to 0.72778, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 674us/step - loss: 0.2868 - acc: 0.9333 - val_loss: 0.7278 - val_acc: 0.7667\n",
      "Epoch 831/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.3007 - acc: 0.9255 - val_loss: 0.7376 - val_acc: 0.7333\n",
      "Epoch 832/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.3260 - acc: 0.8941 - val_loss: 0.7310 - val_acc: 0.7556\n",
      "Epoch 833/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.2971 - acc: 0.9039 - val_loss: 0.7255 - val_acc: 0.7444\n",
      "Epoch 834/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.3023 - acc: 0.9157 - val_loss: 0.7271 - val_acc: 0.7444\n",
      "Epoch 835/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2781 - acc: 0.9196Epoch 00835: val_loss did not improve\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.2865 - acc: 0.9235 - val_loss: 0.7291 - val_acc: 0.7556\n",
      "Epoch 836/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 616us/step - loss: 0.2875 - acc: 0.9255 - val_loss: 0.7383 - val_acc: 0.7333\n",
      "Epoch 837/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.3215 - acc: 0.9196 - val_loss: 0.7293 - val_acc: 0.7444\n",
      "Epoch 838/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.3069 - acc: 0.9137 - val_loss: 0.7387 - val_acc: 0.7333\n",
      "Epoch 839/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.3246 - acc: 0.8961 - val_loss: 0.7304 - val_acc: 0.7333\n",
      "Epoch 840/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3225 - acc: 0.8906Epoch 00840: val_loss improved from 0.72778 to 0.72566, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.3141 - acc: 0.8980 - val_loss: 0.7257 - val_acc: 0.7444\n",
      "Epoch 841/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.2860 - acc: 0.9255 - val_loss: 0.7251 - val_acc: 0.7222\n",
      "Epoch 842/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.3208 - acc: 0.8902 - val_loss: 0.7246 - val_acc: 0.7444\n",
      "Epoch 843/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.3223 - acc: 0.8980 - val_loss: 0.7200 - val_acc: 0.7333\n",
      "Epoch 844/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.3245 - acc: 0.8922 - val_loss: 0.7204 - val_acc: 0.7222\n",
      "Epoch 845/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3060 - acc: 0.9085Epoch 00845: val_loss did not improve\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.2999 - acc: 0.9118 - val_loss: 0.7282 - val_acc: 0.7333\n",
      "Epoch 846/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2908 - acc: 0.9118 - val_loss: 0.7256 - val_acc: 0.7444\n",
      "Epoch 847/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.2979 - acc: 0.9176 - val_loss: 0.7300 - val_acc: 0.7333\n",
      "Epoch 848/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.3147 - acc: 0.9078 - val_loss: 0.7304 - val_acc: 0.7222\n",
      "Epoch 849/3000\n",
      "510/510 [==============================] - 0s 659us/step - loss: 0.2989 - acc: 0.9137 - val_loss: 0.7351 - val_acc: 0.7444\n",
      "Epoch 850/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3335 - acc: 0.8996Epoch 00850: val_loss did not improve\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.3211 - acc: 0.9118 - val_loss: 0.7354 - val_acc: 0.7444\n",
      "Epoch 851/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.3007 - acc: 0.9137 - val_loss: 0.7390 - val_acc: 0.7333\n",
      "Epoch 852/3000\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.2901 - acc: 0.9176 - val_loss: 0.7410 - val_acc: 0.7333\n",
      "Epoch 853/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.3093 - acc: 0.9196 - val_loss: 0.7276 - val_acc: 0.7556\n",
      "Epoch 854/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.3067 - acc: 0.9216 - val_loss: 0.7306 - val_acc: 0.7444\n",
      "Epoch 855/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2945 - acc: 0.9152Epoch 00855: val_loss did not improve\n",
      "510/510 [==============================] - 0s 676us/step - loss: 0.2943 - acc: 0.9137 - val_loss: 0.7332 - val_acc: 0.7444\n",
      "Epoch 856/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.3107 - acc: 0.8941 - val_loss: 0.7339 - val_acc: 0.7333\n",
      "Epoch 857/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.2712 - acc: 0.9275 - val_loss: 0.7380 - val_acc: 0.7444\n",
      "Epoch 858/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.3069 - acc: 0.9137 - val_loss: 0.7377 - val_acc: 0.7444\n",
      "Epoch 859/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.3044 - acc: 0.9059 - val_loss: 0.7400 - val_acc: 0.7556\n",
      "Epoch 860/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2769 - acc: 0.9241Epoch 00860: val_loss did not improve\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.2701 - acc: 0.9255 - val_loss: 0.7343 - val_acc: 0.7667\n",
      "Epoch 861/3000\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.2792 - acc: 0.9176 - val_loss: 0.7403 - val_acc: 0.7556\n",
      "Epoch 862/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.3046 - acc: 0.9137 - val_loss: 0.7330 - val_acc: 0.7667\n",
      "Epoch 863/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.2886 - acc: 0.9275 - val_loss: 0.7302 - val_acc: 0.7556\n",
      "Epoch 864/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.2662 - acc: 0.9353 - val_loss: 0.7378 - val_acc: 0.7444\n",
      "Epoch 865/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3097 - acc: 0.9152Epoch 00865: val_loss did not improve\n",
      "510/510 [==============================] - 0s 708us/step - loss: 0.3039 - acc: 0.9196 - val_loss: 0.7308 - val_acc: 0.7333\n",
      "Epoch 866/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.3014 - acc: 0.9235 - val_loss: 0.7338 - val_acc: 0.7222\n",
      "Epoch 867/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.3092 - acc: 0.9059 - val_loss: 0.7269 - val_acc: 0.7222\n",
      "Epoch 868/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.3170 - acc: 0.8961 - val_loss: 0.7326 - val_acc: 0.7111\n",
      "Epoch 869/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.3083 - acc: 0.9118 - val_loss: 0.7256 - val_acc: 0.7333\n",
      "Epoch 870/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.2828 - acc: 0.9271Epoch 00870: val_loss improved from 0.72566 to 0.72373, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 653us/step - loss: 0.2748 - acc: 0.9333 - val_loss: 0.7237 - val_acc: 0.7333\n",
      "Epoch 871/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.2871 - acc: 0.9235 - val_loss: 0.7205 - val_acc: 0.7556\n",
      "Epoch 872/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.2593 - acc: 0.9412 - val_loss: 0.7219 - val_acc: 0.7556\n",
      "Epoch 873/3000\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.2876 - acc: 0.9235 - val_loss: 0.7224 - val_acc: 0.7333\n",
      "Epoch 874/3000\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.2762 - acc: 0.9235 - val_loss: 0.7180 - val_acc: 0.7556\n",
      "Epoch 875/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3070 - acc: 0.9107Epoch 00875: val_loss improved from 0.72373 to 0.72137, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.3002 - acc: 0.9137 - val_loss: 0.7214 - val_acc: 0.7667\n",
      "Epoch 876/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.2966 - acc: 0.9118 - val_loss: 0.7280 - val_acc: 0.7444\n",
      "Epoch 877/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.3056 - acc: 0.8980 - val_loss: 0.7290 - val_acc: 0.7667\n",
      "Epoch 878/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.2969 - acc: 0.9078 - val_loss: 0.7281 - val_acc: 0.7556\n",
      "Epoch 879/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.2901 - acc: 0.9157 - val_loss: 0.7307 - val_acc: 0.7556\n",
      "Epoch 880/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2902 - acc: 0.9286Epoch 00880: val_loss did not improve\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.2936 - acc: 0.9294 - val_loss: 0.7235 - val_acc: 0.7556\n",
      "Epoch 881/3000\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.2918 - acc: 0.9294 - val_loss: 0.7344 - val_acc: 0.7444\n",
      "Epoch 882/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.2861 - acc: 0.9098 - val_loss: 0.7326 - val_acc: 0.7444\n",
      "Epoch 883/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.2933 - acc: 0.9196 - val_loss: 0.7369 - val_acc: 0.7333\n",
      "Epoch 884/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.2655 - acc: 0.9333 - val_loss: 0.7291 - val_acc: 0.7333\n",
      "Epoch 885/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2627 - acc: 0.9375Epoch 00885: val_loss did not improve\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.2612 - acc: 0.9412 - val_loss: 0.7282 - val_acc: 0.7333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.3154 - acc: 0.9039 - val_loss: 0.7251 - val_acc: 0.7444\n",
      "Epoch 887/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.2788 - acc: 0.9118 - val_loss: 0.7164 - val_acc: 0.7333\n",
      "Epoch 888/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.2979 - acc: 0.9176 - val_loss: 0.7207 - val_acc: 0.7333\n",
      "Epoch 889/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.2777 - acc: 0.9333 - val_loss: 0.7256 - val_acc: 0.7222\n",
      "Epoch 890/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2766 - acc: 0.9196Epoch 00890: val_loss did not improve\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.2815 - acc: 0.9157 - val_loss: 0.7215 - val_acc: 0.7222\n",
      "Epoch 891/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.2678 - acc: 0.9373 - val_loss: 0.7168 - val_acc: 0.7333\n",
      "Epoch 892/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.2705 - acc: 0.9333 - val_loss: 0.7206 - val_acc: 0.7333\n",
      "Epoch 893/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.2711 - acc: 0.9333 - val_loss: 0.7175 - val_acc: 0.7333\n",
      "Epoch 894/3000\n",
      "510/510 [==============================] - 0s 662us/step - loss: 0.2949 - acc: 0.9176 - val_loss: 0.7171 - val_acc: 0.7222\n",
      "Epoch 895/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2616 - acc: 0.9219Epoch 00895: val_loss improved from 0.72137 to 0.71384, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 654us/step - loss: 0.2617 - acc: 0.9196 - val_loss: 0.7138 - val_acc: 0.7333\n",
      "Epoch 896/3000\n",
      "510/510 [==============================] - 0s 654us/step - loss: 0.2808 - acc: 0.9176 - val_loss: 0.7151 - val_acc: 0.7333\n",
      "Epoch 897/3000\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.2731 - acc: 0.9275 - val_loss: 0.7143 - val_acc: 0.7333\n",
      "Epoch 898/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.2824 - acc: 0.9216 - val_loss: 0.7184 - val_acc: 0.7444\n",
      "Epoch 899/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.2682 - acc: 0.9373 - val_loss: 0.7245 - val_acc: 0.7444\n",
      "Epoch 900/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2798 - acc: 0.9219Epoch 00900: val_loss did not improve\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.2747 - acc: 0.9235 - val_loss: 0.7238 - val_acc: 0.7222\n",
      "Epoch 901/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2611 - acc: 0.9294 - val_loss: 0.7197 - val_acc: 0.7333\n",
      "Epoch 902/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.3043 - acc: 0.9157 - val_loss: 0.7177 - val_acc: 0.7333\n",
      "Epoch 903/3000\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.2647 - acc: 0.9255 - val_loss: 0.7160 - val_acc: 0.7444\n",
      "Epoch 904/3000\n",
      "510/510 [==============================] - 0s 658us/step - loss: 0.2721 - acc: 0.9275 - val_loss: 0.7189 - val_acc: 0.7333\n",
      "Epoch 905/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2878 - acc: 0.9174Epoch 00905: val_loss did not improve\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.2881 - acc: 0.9157 - val_loss: 0.7234 - val_acc: 0.7333\n",
      "Epoch 906/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.3153 - acc: 0.9000 - val_loss: 0.7232 - val_acc: 0.7444\n",
      "Epoch 907/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.3049 - acc: 0.9020 - val_loss: 0.7233 - val_acc: 0.7556\n",
      "Epoch 908/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.2590 - acc: 0.9471 - val_loss: 0.7222 - val_acc: 0.7222\n",
      "Epoch 909/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.3008 - acc: 0.9118 - val_loss: 0.7210 - val_acc: 0.7222\n",
      "Epoch 910/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2669 - acc: 0.9442Epoch 00910: val_loss did not improve\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.2637 - acc: 0.9471 - val_loss: 0.7220 - val_acc: 0.7444\n",
      "Epoch 911/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2654 - acc: 0.9333 - val_loss: 0.7160 - val_acc: 0.7444\n",
      "Epoch 912/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.2670 - acc: 0.9353 - val_loss: 0.7194 - val_acc: 0.7444\n",
      "Epoch 913/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.2830 - acc: 0.9078 - val_loss: 0.7159 - val_acc: 0.7333\n",
      "Epoch 914/3000\n",
      "510/510 [==============================] - 0s 655us/step - loss: 0.2875 - acc: 0.9137 - val_loss: 0.7120 - val_acc: 0.7444\n",
      "Epoch 915/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2723 - acc: 0.9062Epoch 00915: val_loss did not improve\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2776 - acc: 0.9039 - val_loss: 0.7188 - val_acc: 0.7444\n",
      "Epoch 916/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2718 - acc: 0.9314 - val_loss: 0.7188 - val_acc: 0.7222\n",
      "Epoch 917/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.2699 - acc: 0.9196 - val_loss: 0.7156 - val_acc: 0.7222\n",
      "Epoch 918/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.2808 - acc: 0.9255 - val_loss: 0.7062 - val_acc: 0.7333\n",
      "Epoch 919/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.2820 - acc: 0.9275 - val_loss: 0.7108 - val_acc: 0.7111\n",
      "Epoch 920/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2546 - acc: 0.9375Epoch 00920: val_loss improved from 0.71384 to 0.70283, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.2574 - acc: 0.9314 - val_loss: 0.7028 - val_acc: 0.7333\n",
      "Epoch 921/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.2758 - acc: 0.9137 - val_loss: 0.7103 - val_acc: 0.7333\n",
      "Epoch 922/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.2803 - acc: 0.9176 - val_loss: 0.7060 - val_acc: 0.7333\n",
      "Epoch 923/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.2584 - acc: 0.9373 - val_loss: 0.7095 - val_acc: 0.7333\n",
      "Epoch 924/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.2597 - acc: 0.9471 - val_loss: 0.7079 - val_acc: 0.7556\n",
      "Epoch 925/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2598 - acc: 0.9308Epoch 00925: val_loss did not improve\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.2741 - acc: 0.9235 - val_loss: 0.7055 - val_acc: 0.7444\n",
      "Epoch 926/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.2788 - acc: 0.9157 - val_loss: 0.7089 - val_acc: 0.7333\n",
      "Epoch 927/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.2782 - acc: 0.9392 - val_loss: 0.7077 - val_acc: 0.7556\n",
      "Epoch 928/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.2794 - acc: 0.9294 - val_loss: 0.7106 - val_acc: 0.7333\n",
      "Epoch 929/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.2607 - acc: 0.9314 - val_loss: 0.7164 - val_acc: 0.7444\n",
      "Epoch 930/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2710 - acc: 0.9286Epoch 00930: val_loss did not improve\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.2712 - acc: 0.9294 - val_loss: 0.7117 - val_acc: 0.7444\n",
      "Epoch 931/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.2587 - acc: 0.9333 - val_loss: 0.7076 - val_acc: 0.7444\n",
      "Epoch 932/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.2676 - acc: 0.9294 - val_loss: 0.7177 - val_acc: 0.7444\n",
      "Epoch 933/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.2682 - acc: 0.9412 - val_loss: 0.7236 - val_acc: 0.7222\n",
      "Epoch 934/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.2576 - acc: 0.9235 - val_loss: 0.7225 - val_acc: 0.7222\n",
      "Epoch 935/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2818 - acc: 0.9241Epoch 00935: val_loss did not improve\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.2963 - acc: 0.9176 - val_loss: 0.7132 - val_acc: 0.7333\n",
      "Epoch 936/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 632us/step - loss: 0.2766 - acc: 0.9157 - val_loss: 0.7151 - val_acc: 0.7222\n",
      "Epoch 937/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.2705 - acc: 0.9078 - val_loss: 0.7081 - val_acc: 0.7222\n",
      "Epoch 938/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.2796 - acc: 0.9216 - val_loss: 0.7106 - val_acc: 0.7111\n",
      "Epoch 939/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2453 - acc: 0.9529 - val_loss: 0.7056 - val_acc: 0.7222\n",
      "Epoch 940/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2633 - acc: 0.9107Epoch 00940: val_loss did not improve\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.2691 - acc: 0.9118 - val_loss: 0.7037 - val_acc: 0.7333\n",
      "Epoch 941/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.2684 - acc: 0.9216 - val_loss: 0.7027 - val_acc: 0.7444\n",
      "Epoch 942/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.2840 - acc: 0.9137 - val_loss: 0.7010 - val_acc: 0.7222\n",
      "Epoch 943/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.2833 - acc: 0.9118 - val_loss: 0.7063 - val_acc: 0.7111\n",
      "Epoch 944/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2586 - acc: 0.9314 - val_loss: 0.7115 - val_acc: 0.7000\n",
      "Epoch 945/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2919 - acc: 0.9129Epoch 00945: val_loss did not improve\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.2834 - acc: 0.9176 - val_loss: 0.7099 - val_acc: 0.7111\n",
      "Epoch 946/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.2509 - acc: 0.9510 - val_loss: 0.7143 - val_acc: 0.7111\n",
      "Epoch 947/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.2848 - acc: 0.9078 - val_loss: 0.7176 - val_acc: 0.7222\n",
      "Epoch 948/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.3021 - acc: 0.9118 - val_loss: 0.7164 - val_acc: 0.7222\n",
      "Epoch 949/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.2804 - acc: 0.9275 - val_loss: 0.6997 - val_acc: 0.7333\n",
      "Epoch 950/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2815 - acc: 0.9152Epoch 00950: val_loss did not improve\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.2804 - acc: 0.9196 - val_loss: 0.7048 - val_acc: 0.7222\n",
      "Epoch 951/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.2646 - acc: 0.9294 - val_loss: 0.7037 - val_acc: 0.7111\n",
      "Epoch 952/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.2735 - acc: 0.9196 - val_loss: 0.7034 - val_acc: 0.7111\n",
      "Epoch 953/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.2957 - acc: 0.9118 - val_loss: 0.6996 - val_acc: 0.7222\n",
      "Epoch 954/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.2907 - acc: 0.9255 - val_loss: 0.7076 - val_acc: 0.7111\n",
      "Epoch 955/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2773 - acc: 0.9085Epoch 00955: val_loss did not improve\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.2704 - acc: 0.9118 - val_loss: 0.7056 - val_acc: 0.7222\n",
      "Epoch 956/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.2779 - acc: 0.9235 - val_loss: 0.6992 - val_acc: 0.7444\n",
      "Epoch 957/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.2507 - acc: 0.9451 - val_loss: 0.7060 - val_acc: 0.7333\n",
      "Epoch 958/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2750 - acc: 0.9314 - val_loss: 0.6935 - val_acc: 0.7333\n",
      "Epoch 959/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.2475 - acc: 0.9294 - val_loss: 0.6958 - val_acc: 0.7333\n",
      "Epoch 960/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2421 - acc: 0.9353Epoch 00960: val_loss improved from 0.70283 to 0.69880, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.2471 - acc: 0.9314 - val_loss: 0.6988 - val_acc: 0.7333\n",
      "Epoch 961/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.2759 - acc: 0.9137 - val_loss: 0.6970 - val_acc: 0.7222\n",
      "Epoch 962/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.2762 - acc: 0.9176 - val_loss: 0.6979 - val_acc: 0.7222\n",
      "Epoch 963/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.2697 - acc: 0.9235 - val_loss: 0.6967 - val_acc: 0.7222\n",
      "Epoch 964/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.2479 - acc: 0.9392 - val_loss: 0.6995 - val_acc: 0.7111\n",
      "Epoch 965/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2764 - acc: 0.9308Epoch 00965: val_loss improved from 0.69880 to 0.69818, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.2797 - acc: 0.9235 - val_loss: 0.6982 - val_acc: 0.7222\n",
      "Epoch 966/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.2627 - acc: 0.9176 - val_loss: 0.6967 - val_acc: 0.7222\n",
      "Epoch 967/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.2671 - acc: 0.9255 - val_loss: 0.7023 - val_acc: 0.7222\n",
      "Epoch 968/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.2366 - acc: 0.9431 - val_loss: 0.7084 - val_acc: 0.7222\n",
      "Epoch 969/3000\n",
      "510/510 [==============================] - 0s 551us/step - loss: 0.2642 - acc: 0.9235 - val_loss: 0.7000 - val_acc: 0.7111\n",
      "Epoch 970/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2507 - acc: 0.9397Epoch 00970: val_loss improved from 0.69818 to 0.69739, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2476 - acc: 0.9412 - val_loss: 0.6974 - val_acc: 0.7111\n",
      "Epoch 971/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.2812 - acc: 0.9196 - val_loss: 0.6996 - val_acc: 0.7333\n",
      "Epoch 972/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.2502 - acc: 0.9314 - val_loss: 0.6983 - val_acc: 0.7444\n",
      "Epoch 973/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.2585 - acc: 0.9353 - val_loss: 0.6890 - val_acc: 0.7556\n",
      "Epoch 974/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.2545 - acc: 0.9333 - val_loss: 0.6971 - val_acc: 0.7333\n",
      "Epoch 975/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2530 - acc: 0.9330Epoch 00975: val_loss improved from 0.69739 to 0.68802, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.2568 - acc: 0.9275 - val_loss: 0.6880 - val_acc: 0.7667\n",
      "Epoch 976/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.2508 - acc: 0.9275 - val_loss: 0.6868 - val_acc: 0.7667\n",
      "Epoch 977/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.2589 - acc: 0.9333 - val_loss: 0.6843 - val_acc: 0.7333\n",
      "Epoch 978/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.2741 - acc: 0.9255 - val_loss: 0.6902 - val_acc: 0.7333\n",
      "Epoch 979/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.2367 - acc: 0.9412 - val_loss: 0.6930 - val_acc: 0.7333\n",
      "Epoch 980/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2423 - acc: 0.9353Epoch 00980: val_loss did not improve\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.2449 - acc: 0.9333 - val_loss: 0.6967 - val_acc: 0.7333\n",
      "Epoch 981/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.2557 - acc: 0.9176 - val_loss: 0.7062 - val_acc: 0.7333\n",
      "Epoch 982/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.2562 - acc: 0.9294 - val_loss: 0.7046 - val_acc: 0.7333\n",
      "Epoch 983/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.2629 - acc: 0.9294 - val_loss: 0.6950 - val_acc: 0.7444\n",
      "Epoch 984/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.2444 - acc: 0.9529 - val_loss: 0.6946 - val_acc: 0.7444\n",
      "Epoch 985/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2706 - acc: 0.9152Epoch 00985: val_loss did not improve\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.2808 - acc: 0.9137 - val_loss: 0.6898 - val_acc: 0.7444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.2665 - acc: 0.9392 - val_loss: 0.7002 - val_acc: 0.7556\n",
      "Epoch 987/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.2403 - acc: 0.9412 - val_loss: 0.7019 - val_acc: 0.7444\n",
      "Epoch 988/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.2493 - acc: 0.9373 - val_loss: 0.6967 - val_acc: 0.7444\n",
      "Epoch 989/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.2400 - acc: 0.9431 - val_loss: 0.7050 - val_acc: 0.7333\n",
      "Epoch 990/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2481 - acc: 0.9375Epoch 00990: val_loss did not improve\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.2521 - acc: 0.9353 - val_loss: 0.7034 - val_acc: 0.7222\n",
      "Epoch 991/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.2598 - acc: 0.9196 - val_loss: 0.6908 - val_acc: 0.7333\n",
      "Epoch 992/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.2514 - acc: 0.9451 - val_loss: 0.6983 - val_acc: 0.7111\n",
      "Epoch 993/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.2515 - acc: 0.9235 - val_loss: 0.6914 - val_acc: 0.7222\n",
      "Epoch 994/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.2694 - acc: 0.9118 - val_loss: 0.7043 - val_acc: 0.7111\n",
      "Epoch 995/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2513 - acc: 0.9286Epoch 00995: val_loss did not improve\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.2615 - acc: 0.9216 - val_loss: 0.7059 - val_acc: 0.7000\n",
      "Epoch 996/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.2644 - acc: 0.9333 - val_loss: 0.7009 - val_acc: 0.7222\n",
      "Epoch 997/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.2675 - acc: 0.9176 - val_loss: 0.7046 - val_acc: 0.7222\n",
      "Epoch 998/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.2646 - acc: 0.9373 - val_loss: 0.6988 - val_acc: 0.7222\n",
      "Epoch 999/3000\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.2595 - acc: 0.9373 - val_loss: 0.6981 - val_acc: 0.7333\n",
      "Epoch 1000/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2710 - acc: 0.9196Epoch 01000: val_loss did not improve\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.2665 - acc: 0.9196 - val_loss: 0.6958 - val_acc: 0.7222\n",
      "Epoch 1001/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.2591 - acc: 0.9255 - val_loss: 0.6972 - val_acc: 0.7444\n",
      "Epoch 1002/3000\n",
      "510/510 [==============================] - 0s 677us/step - loss: 0.2485 - acc: 0.9333 - val_loss: 0.6947 - val_acc: 0.7556\n",
      "Epoch 1003/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.2627 - acc: 0.9255 - val_loss: 0.6955 - val_acc: 0.7556\n",
      "Epoch 1004/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.2428 - acc: 0.9471 - val_loss: 0.6924 - val_acc: 0.7222\n",
      "Epoch 1005/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2676 - acc: 0.9241Epoch 01005: val_loss did not improve\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.2668 - acc: 0.9275 - val_loss: 0.6991 - val_acc: 0.7444\n",
      "Epoch 1006/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.2471 - acc: 0.9314 - val_loss: 0.6981 - val_acc: 0.7333\n",
      "Epoch 1007/3000\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.2599 - acc: 0.9275 - val_loss: 0.6928 - val_acc: 0.7222\n",
      "Epoch 1008/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2661 - acc: 0.9275 - val_loss: 0.6875 - val_acc: 0.7333\n",
      "Epoch 1009/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.2405 - acc: 0.9451 - val_loss: 0.6876 - val_acc: 0.7333\n",
      "Epoch 1010/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2543 - acc: 0.9174Epoch 01010: val_loss improved from 0.68802 to 0.68774, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.2548 - acc: 0.9196 - val_loss: 0.6877 - val_acc: 0.7444\n",
      "Epoch 1011/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.2340 - acc: 0.9431 - val_loss: 0.6909 - val_acc: 0.7333\n",
      "Epoch 1012/3000\n",
      "510/510 [==============================] - 0s 662us/step - loss: 0.2561 - acc: 0.9373 - val_loss: 0.6838 - val_acc: 0.7556\n",
      "Epoch 1013/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.2425 - acc: 0.9353 - val_loss: 0.6809 - val_acc: 0.7556\n",
      "Epoch 1014/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.2626 - acc: 0.9294 - val_loss: 0.6871 - val_acc: 0.7444\n",
      "Epoch 1015/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2293 - acc: 0.9375Epoch 01015: val_loss did not improve\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.2244 - acc: 0.9412 - val_loss: 0.6880 - val_acc: 0.7444\n",
      "Epoch 1016/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.2541 - acc: 0.9294 - val_loss: 0.6838 - val_acc: 0.7444\n",
      "Epoch 1017/3000\n",
      "510/510 [==============================] - 0s 637us/step - loss: 0.2485 - acc: 0.9431 - val_loss: 0.6841 - val_acc: 0.7444\n",
      "Epoch 1018/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.2567 - acc: 0.9255 - val_loss: 0.6814 - val_acc: 0.7444\n",
      "Epoch 1019/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.2714 - acc: 0.9196 - val_loss: 0.6808 - val_acc: 0.7333\n",
      "Epoch 1020/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2699 - acc: 0.9241Epoch 01020: val_loss improved from 0.68774 to 0.68087, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.2671 - acc: 0.9255 - val_loss: 0.6809 - val_acc: 0.7333\n",
      "Epoch 1021/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.2497 - acc: 0.9333 - val_loss: 0.6704 - val_acc: 0.7444\n",
      "Epoch 1022/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.2468 - acc: 0.9216 - val_loss: 0.6763 - val_acc: 0.7333\n",
      "Epoch 1023/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2474 - acc: 0.9314 - val_loss: 0.6799 - val_acc: 0.7222\n",
      "Epoch 1024/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.2517 - acc: 0.9196 - val_loss: 0.6773 - val_acc: 0.7333\n",
      "Epoch 1025/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2701 - acc: 0.9174Epoch 01025: val_loss did not improve\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.2636 - acc: 0.9235 - val_loss: 0.6845 - val_acc: 0.7333\n",
      "Epoch 1026/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.2589 - acc: 0.9275 - val_loss: 0.6824 - val_acc: 0.7444\n",
      "Epoch 1027/3000\n",
      "510/510 [==============================] - 0s 666us/step - loss: 0.2456 - acc: 0.9353 - val_loss: 0.6866 - val_acc: 0.7444\n",
      "Epoch 1028/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.2707 - acc: 0.9118 - val_loss: 0.6806 - val_acc: 0.7444\n",
      "Epoch 1029/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.2551 - acc: 0.9333 - val_loss: 0.6774 - val_acc: 0.7556\n",
      "Epoch 1030/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2530 - acc: 0.9219Epoch 01030: val_loss improved from 0.68087 to 0.67955, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.2446 - acc: 0.9255 - val_loss: 0.6796 - val_acc: 0.7556\n",
      "Epoch 1031/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.2596 - acc: 0.9294 - val_loss: 0.6732 - val_acc: 0.7556\n",
      "Epoch 1032/3000\n",
      "510/510 [==============================] - 0s 645us/step - loss: 0.2284 - acc: 0.9412 - val_loss: 0.6770 - val_acc: 0.7556\n",
      "Epoch 1033/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.2336 - acc: 0.9412 - val_loss: 0.6812 - val_acc: 0.7444\n",
      "Epoch 1034/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.2355 - acc: 0.9392 - val_loss: 0.6890 - val_acc: 0.7444\n",
      "Epoch 1035/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2358 - acc: 0.9286Epoch 01035: val_loss did not improve\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.2283 - acc: 0.9314 - val_loss: 0.6892 - val_acc: 0.7444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1036/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2477 - acc: 0.9275 - val_loss: 0.6853 - val_acc: 0.7667\n",
      "Epoch 1037/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.2685 - acc: 0.9118 - val_loss: 0.6826 - val_acc: 0.7444\n",
      "Epoch 1038/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.2276 - acc: 0.9549 - val_loss: 0.6867 - val_acc: 0.7444\n",
      "Epoch 1039/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.2507 - acc: 0.9235 - val_loss: 0.6849 - val_acc: 0.7556\n",
      "Epoch 1040/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2347 - acc: 0.9464Epoch 01040: val_loss did not improve\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.2375 - acc: 0.9431 - val_loss: 0.6862 - val_acc: 0.7333\n",
      "Epoch 1041/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.2379 - acc: 0.9392 - val_loss: 0.6943 - val_acc: 0.7222\n",
      "Epoch 1042/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.2388 - acc: 0.9333 - val_loss: 0.6926 - val_acc: 0.7333\n",
      "Epoch 1043/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.2301 - acc: 0.9431 - val_loss: 0.6918 - val_acc: 0.7333\n",
      "Epoch 1044/3000\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.2484 - acc: 0.9431 - val_loss: 0.6983 - val_acc: 0.7333\n",
      "Epoch 1045/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2412 - acc: 0.9375Epoch 01045: val_loss did not improve\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.2380 - acc: 0.9373 - val_loss: 0.6952 - val_acc: 0.7333\n",
      "Epoch 1046/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.2527 - acc: 0.9333 - val_loss: 0.6900 - val_acc: 0.7333\n",
      "Epoch 1047/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.2231 - acc: 0.9373 - val_loss: 0.6902 - val_acc: 0.7111\n",
      "Epoch 1048/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.2483 - acc: 0.9255 - val_loss: 0.6869 - val_acc: 0.7222\n",
      "Epoch 1049/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2338 - acc: 0.9373 - val_loss: 0.6787 - val_acc: 0.7222\n",
      "Epoch 1050/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2425 - acc: 0.9442Epoch 01050: val_loss did not improve\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2467 - acc: 0.9412 - val_loss: 0.6842 - val_acc: 0.7444\n",
      "Epoch 1051/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.2534 - acc: 0.9275 - val_loss: 0.6788 - val_acc: 0.7222\n",
      "Epoch 1052/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.2524 - acc: 0.9255 - val_loss: 0.6849 - val_acc: 0.7333\n",
      "Epoch 1053/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.2603 - acc: 0.9216 - val_loss: 0.6878 - val_acc: 0.7333\n",
      "Epoch 1054/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.2371 - acc: 0.9314 - val_loss: 0.6849 - val_acc: 0.7222\n",
      "Epoch 1055/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2387 - acc: 0.9353Epoch 01055: val_loss did not improve\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.2393 - acc: 0.9314 - val_loss: 0.6860 - val_acc: 0.7222\n",
      "Epoch 1056/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.2249 - acc: 0.9588 - val_loss: 0.6858 - val_acc: 0.7333\n",
      "Epoch 1057/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.2150 - acc: 0.9529 - val_loss: 0.6840 - val_acc: 0.7333\n",
      "Epoch 1058/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.2304 - acc: 0.9490 - val_loss: 0.6890 - val_acc: 0.7333\n",
      "Epoch 1059/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.2481 - acc: 0.9255 - val_loss: 0.6899 - val_acc: 0.7222\n",
      "Epoch 1060/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.2348 - acc: 0.9349Epoch 01060: val_loss did not improve\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.2300 - acc: 0.9333 - val_loss: 0.6956 - val_acc: 0.7333\n",
      "Epoch 1061/3000\n",
      "510/510 [==============================] - 0s 647us/step - loss: 0.2521 - acc: 0.9255 - val_loss: 0.6930 - val_acc: 0.7444\n",
      "Epoch 1062/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.2366 - acc: 0.9392 - val_loss: 0.6929 - val_acc: 0.7444\n",
      "Epoch 1063/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.2311 - acc: 0.9549 - val_loss: 0.6844 - val_acc: 0.7444\n",
      "Epoch 1064/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.2216 - acc: 0.9412 - val_loss: 0.6881 - val_acc: 0.7444\n",
      "Epoch 1065/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2393 - acc: 0.9375Epoch 01065: val_loss did not improve\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2462 - acc: 0.9333 - val_loss: 0.6832 - val_acc: 0.7556\n",
      "Epoch 1066/3000\n",
      "510/510 [==============================] - 0s 718us/step - loss: 0.2167 - acc: 0.9373 - val_loss: 0.6803 - val_acc: 0.7556\n",
      "Epoch 1067/3000\n",
      "510/510 [==============================] - 0s 649us/step - loss: 0.2268 - acc: 0.9471 - val_loss: 0.6749 - val_acc: 0.7556\n",
      "Epoch 1068/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.2452 - acc: 0.9373 - val_loss: 0.6749 - val_acc: 0.7556\n",
      "Epoch 1069/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.2197 - acc: 0.9451 - val_loss: 0.6766 - val_acc: 0.7556\n",
      "Epoch 1070/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2445 - acc: 0.9375Epoch 01070: val_loss improved from 0.67955 to 0.67839, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 676us/step - loss: 0.2381 - acc: 0.9412 - val_loss: 0.6784 - val_acc: 0.7556\n",
      "Epoch 1071/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2281 - acc: 0.9451 - val_loss: 0.6752 - val_acc: 0.7556\n",
      "Epoch 1072/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.2628 - acc: 0.9294 - val_loss: 0.6833 - val_acc: 0.7444\n",
      "Epoch 1073/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.2639 - acc: 0.9196 - val_loss: 0.6833 - val_acc: 0.7222\n",
      "Epoch 1074/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.2401 - acc: 0.9176 - val_loss: 0.6784 - val_acc: 0.7333\n",
      "Epoch 1075/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2330 - acc: 0.9420Epoch 01075: val_loss did not improve\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.2328 - acc: 0.9412 - val_loss: 0.6859 - val_acc: 0.7222\n",
      "Epoch 1076/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.2163 - acc: 0.9510 - val_loss: 0.6814 - val_acc: 0.7333\n",
      "Epoch 1077/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.2539 - acc: 0.9255 - val_loss: 0.6729 - val_acc: 0.7333\n",
      "Epoch 1078/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.2469 - acc: 0.9392 - val_loss: 0.6705 - val_acc: 0.7444\n",
      "Epoch 1079/3000\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.2459 - acc: 0.9353 - val_loss: 0.6688 - val_acc: 0.7556\n",
      "Epoch 1080/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2108 - acc: 0.9375Epoch 01080: val_loss did not improve\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.2185 - acc: 0.9373 - val_loss: 0.6804 - val_acc: 0.7444\n",
      "Epoch 1081/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.2295 - acc: 0.9373 - val_loss: 0.6820 - val_acc: 0.7333\n",
      "Epoch 1082/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.2320 - acc: 0.9314 - val_loss: 0.6805 - val_acc: 0.7333\n",
      "Epoch 1083/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.2604 - acc: 0.9294 - val_loss: 0.6784 - val_acc: 0.7333\n",
      "Epoch 1084/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.2164 - acc: 0.9451 - val_loss: 0.6892 - val_acc: 0.7222\n",
      "Epoch 1085/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2431 - acc: 0.9219Epoch 01085: val_loss did not improve\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.2376 - acc: 0.9255 - val_loss: 0.6905 - val_acc: 0.7444\n",
      "Epoch 1086/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 583us/step - loss: 0.2172 - acc: 0.9471 - val_loss: 0.6850 - val_acc: 0.7222\n",
      "Epoch 1087/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.2336 - acc: 0.9412 - val_loss: 0.6852 - val_acc: 0.7333\n",
      "Epoch 1088/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.2158 - acc: 0.9431 - val_loss: 0.6955 - val_acc: 0.7222\n",
      "Epoch 1089/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2186 - acc: 0.9353 - val_loss: 0.6837 - val_acc: 0.7222\n",
      "Epoch 1090/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2082 - acc: 0.9397Epoch 01090: val_loss did not improve\n",
      "510/510 [==============================] - 0s 552us/step - loss: 0.2114 - acc: 0.9392 - val_loss: 0.6861 - val_acc: 0.7333\n",
      "Epoch 1091/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.2468 - acc: 0.9235 - val_loss: 0.6807 - val_acc: 0.7222\n",
      "Epoch 1092/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.2356 - acc: 0.9353 - val_loss: 0.6742 - val_acc: 0.7333\n",
      "Epoch 1093/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.2531 - acc: 0.9353 - val_loss: 0.6707 - val_acc: 0.7444\n",
      "Epoch 1094/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.2316 - acc: 0.9412 - val_loss: 0.6763 - val_acc: 0.7444\n",
      "Epoch 1095/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2306 - acc: 0.9375Epoch 01095: val_loss improved from 0.67839 to 0.67818, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.2257 - acc: 0.9373 - val_loss: 0.6782 - val_acc: 0.7333\n",
      "Epoch 1096/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.2287 - acc: 0.9353 - val_loss: 0.6757 - val_acc: 0.7222\n",
      "Epoch 1097/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.2112 - acc: 0.9412 - val_loss: 0.6759 - val_acc: 0.7222\n",
      "Epoch 1098/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.2198 - acc: 0.9451 - val_loss: 0.6790 - val_acc: 0.7333\n",
      "Epoch 1099/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2249 - acc: 0.9314 - val_loss: 0.6741 - val_acc: 0.7444\n",
      "Epoch 1100/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.2448 - acc: 0.9297Epoch 01100: val_loss improved from 0.67818 to 0.67596, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.2418 - acc: 0.9294 - val_loss: 0.6760 - val_acc: 0.7333\n",
      "Epoch 1101/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.2256 - acc: 0.9412 - val_loss: 0.6764 - val_acc: 0.7444\n",
      "Epoch 1102/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.2327 - acc: 0.9392 - val_loss: 0.6809 - val_acc: 0.7444\n",
      "Epoch 1103/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.2450 - acc: 0.9294 - val_loss: 0.6845 - val_acc: 0.7444\n",
      "Epoch 1104/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.2180 - acc: 0.9392 - val_loss: 0.6789 - val_acc: 0.7444\n",
      "Epoch 1105/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2461 - acc: 0.9375Epoch 01105: val_loss did not improve\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.2451 - acc: 0.9412 - val_loss: 0.6773 - val_acc: 0.7444\n",
      "Epoch 1106/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.2135 - acc: 0.9314 - val_loss: 0.6782 - val_acc: 0.7444\n",
      "Epoch 1107/3000\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.2389 - acc: 0.9333 - val_loss: 0.6754 - val_acc: 0.7333\n",
      "Epoch 1108/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.2240 - acc: 0.9431 - val_loss: 0.6732 - val_acc: 0.7444\n",
      "Epoch 1109/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.2389 - acc: 0.9373 - val_loss: 0.6677 - val_acc: 0.7444\n",
      "Epoch 1110/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2438 - acc: 0.9263Epoch 01110: val_loss improved from 0.67596 to 0.66446, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.2340 - acc: 0.9333 - val_loss: 0.6645 - val_acc: 0.7556\n",
      "Epoch 1111/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.2109 - acc: 0.9608 - val_loss: 0.6679 - val_acc: 0.7556\n",
      "Epoch 1112/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2145 - acc: 0.9451 - val_loss: 0.6780 - val_acc: 0.7444\n",
      "Epoch 1113/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.2217 - acc: 0.9373 - val_loss: 0.6826 - val_acc: 0.7333\n",
      "Epoch 1114/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.2156 - acc: 0.9471 - val_loss: 0.6814 - val_acc: 0.7333\n",
      "Epoch 1115/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2610 - acc: 0.9062Epoch 01115: val_loss did not improve\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2632 - acc: 0.9078 - val_loss: 0.6789 - val_acc: 0.7444\n",
      "Epoch 1116/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.2255 - acc: 0.9490 - val_loss: 0.6800 - val_acc: 0.7444\n",
      "Epoch 1117/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.2341 - acc: 0.9275 - val_loss: 0.6797 - val_acc: 0.7556\n",
      "Epoch 1118/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.2101 - acc: 0.9510 - val_loss: 0.6828 - val_acc: 0.7333\n",
      "Epoch 1119/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.2248 - acc: 0.9392 - val_loss: 0.6725 - val_acc: 0.7333\n",
      "Epoch 1120/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2335 - acc: 0.9353Epoch 01120: val_loss did not improve\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2326 - acc: 0.9353 - val_loss: 0.6751 - val_acc: 0.7333\n",
      "Epoch 1121/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.2123 - acc: 0.9471 - val_loss: 0.6772 - val_acc: 0.7333\n",
      "Epoch 1122/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.2250 - acc: 0.9392 - val_loss: 0.6835 - val_acc: 0.7222\n",
      "Epoch 1123/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.2334 - acc: 0.9333 - val_loss: 0.6682 - val_acc: 0.7556\n",
      "Epoch 1124/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.2293 - acc: 0.9294 - val_loss: 0.6708 - val_acc: 0.7444\n",
      "Epoch 1125/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2260 - acc: 0.9241Epoch 01125: val_loss did not improve\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.2168 - acc: 0.9333 - val_loss: 0.6685 - val_acc: 0.7556\n",
      "Epoch 1126/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.2410 - acc: 0.9373 - val_loss: 0.6720 - val_acc: 0.7444\n",
      "Epoch 1127/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.2259 - acc: 0.9255 - val_loss: 0.6751 - val_acc: 0.7222\n",
      "Epoch 1128/3000\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.2266 - acc: 0.9333 - val_loss: 0.6731 - val_acc: 0.7444\n",
      "Epoch 1129/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.2417 - acc: 0.9392 - val_loss: 0.6774 - val_acc: 0.7222\n",
      "Epoch 1130/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2557 - acc: 0.9219Epoch 01130: val_loss did not improve\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.2494 - acc: 0.9255 - val_loss: 0.6771 - val_acc: 0.7222\n",
      "Epoch 1131/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.2355 - acc: 0.9333 - val_loss: 0.6894 - val_acc: 0.7333\n",
      "Epoch 1132/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.2228 - acc: 0.9314 - val_loss: 0.6793 - val_acc: 0.7333\n",
      "Epoch 1133/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.2400 - acc: 0.9392 - val_loss: 0.6745 - val_acc: 0.7444\n",
      "Epoch 1134/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.2333 - acc: 0.9275 - val_loss: 0.6711 - val_acc: 0.7556\n",
      "Epoch 1135/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2540 - acc: 0.9241Epoch 01135: val_loss did not improve\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.2539 - acc: 0.9216 - val_loss: 0.6724 - val_acc: 0.7444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1136/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.2017 - acc: 0.9490 - val_loss: 0.6686 - val_acc: 0.7444\n",
      "Epoch 1137/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.2163 - acc: 0.9314 - val_loss: 0.6712 - val_acc: 0.7444\n",
      "Epoch 1138/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.2331 - acc: 0.9275 - val_loss: 0.6752 - val_acc: 0.7444\n",
      "Epoch 1139/3000\n",
      "510/510 [==============================] - 0s 568us/step - loss: 0.2116 - acc: 0.9412 - val_loss: 0.6739 - val_acc: 0.7222\n",
      "Epoch 1140/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2304 - acc: 0.9397Epoch 01140: val_loss did not improve\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.2337 - acc: 0.9353 - val_loss: 0.6645 - val_acc: 0.7333\n",
      "Epoch 1141/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.1975 - acc: 0.9431 - val_loss: 0.6728 - val_acc: 0.7444\n",
      "Epoch 1142/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.2305 - acc: 0.9275 - val_loss: 0.6718 - val_acc: 0.7556\n",
      "Epoch 1143/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.1985 - acc: 0.9529 - val_loss: 0.6715 - val_acc: 0.7333\n",
      "Epoch 1144/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.2180 - acc: 0.9373 - val_loss: 0.6794 - val_acc: 0.7444\n",
      "Epoch 1145/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2228 - acc: 0.9241Epoch 01145: val_loss did not improve\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.2224 - acc: 0.9255 - val_loss: 0.6759 - val_acc: 0.7333\n",
      "Epoch 1146/3000\n",
      "510/510 [==============================] - 0s 544us/step - loss: 0.2273 - acc: 0.9412 - val_loss: 0.6651 - val_acc: 0.7556\n",
      "Epoch 1147/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.2077 - acc: 0.9353 - val_loss: 0.6689 - val_acc: 0.7556\n",
      "Epoch 1148/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2149 - acc: 0.9529 - val_loss: 0.6641 - val_acc: 0.7667\n",
      "Epoch 1149/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.2108 - acc: 0.9451 - val_loss: 0.6632 - val_acc: 0.7556\n",
      "Epoch 1150/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2192 - acc: 0.9397Epoch 01150: val_loss did not improve\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.2282 - acc: 0.9373 - val_loss: 0.6709 - val_acc: 0.7556\n",
      "Epoch 1151/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.2338 - acc: 0.9373 - val_loss: 0.6728 - val_acc: 0.7444\n",
      "Epoch 1152/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.2316 - acc: 0.9373 - val_loss: 0.6673 - val_acc: 0.7222\n",
      "Epoch 1153/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.1939 - acc: 0.9529 - val_loss: 0.6694 - val_acc: 0.7333\n",
      "Epoch 1154/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.2247 - acc: 0.9373 - val_loss: 0.6754 - val_acc: 0.7222\n",
      "Epoch 1155/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2348 - acc: 0.9263Epoch 01155: val_loss did not improve\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.2320 - acc: 0.9275 - val_loss: 0.6769 - val_acc: 0.7222\n",
      "Epoch 1156/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.2358 - acc: 0.9373 - val_loss: 0.6753 - val_acc: 0.7444\n",
      "Epoch 1157/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.2330 - acc: 0.9392 - val_loss: 0.6828 - val_acc: 0.7444\n",
      "Epoch 1158/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.2145 - acc: 0.9451 - val_loss: 0.6802 - val_acc: 0.7667\n",
      "Epoch 1159/3000\n",
      "510/510 [==============================] - 0s 664us/step - loss: 0.2402 - acc: 0.9275 - val_loss: 0.6779 - val_acc: 0.7444\n",
      "Epoch 1160/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2102 - acc: 0.9375Epoch 01160: val_loss did not improve\n",
      "510/510 [==============================] - 0s 651us/step - loss: 0.2083 - acc: 0.9373 - val_loss: 0.6916 - val_acc: 0.7333\n",
      "Epoch 1161/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.2197 - acc: 0.9451 - val_loss: 0.6830 - val_acc: 0.7444\n",
      "Epoch 1162/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.2142 - acc: 0.9431 - val_loss: 0.6788 - val_acc: 0.7444\n",
      "Epoch 1163/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.2009 - acc: 0.9451 - val_loss: 0.6757 - val_acc: 0.7444\n",
      "Epoch 1164/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.2072 - acc: 0.9353 - val_loss: 0.6763 - val_acc: 0.7333\n",
      "Epoch 1165/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2346 - acc: 0.9196Epoch 01165: val_loss did not improve\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.2363 - acc: 0.9196 - val_loss: 0.6779 - val_acc: 0.7556\n",
      "Epoch 1166/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.1992 - acc: 0.9588 - val_loss: 0.6735 - val_acc: 0.7667\n",
      "Epoch 1167/3000\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.2270 - acc: 0.9333 - val_loss: 0.6739 - val_acc: 0.7667\n",
      "Epoch 1168/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.2066 - acc: 0.9608 - val_loss: 0.6750 - val_acc: 0.7444\n",
      "Epoch 1169/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.2066 - acc: 0.9529 - val_loss: 0.6820 - val_acc: 0.7333\n",
      "Epoch 1170/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2570 - acc: 0.9464Epoch 01170: val_loss did not improve\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.2482 - acc: 0.9471 - val_loss: 0.6835 - val_acc: 0.7222\n",
      "Epoch 1171/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.2100 - acc: 0.9412 - val_loss: 0.6804 - val_acc: 0.7333\n",
      "Epoch 1172/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.2187 - acc: 0.9314 - val_loss: 0.6713 - val_acc: 0.7444\n",
      "Epoch 1173/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.2176 - acc: 0.9510 - val_loss: 0.6660 - val_acc: 0.7444\n",
      "Epoch 1174/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.2074 - acc: 0.9490 - val_loss: 0.6758 - val_acc: 0.7556\n",
      "Epoch 1175/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2432 - acc: 0.9263Epoch 01175: val_loss did not improve\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.2334 - acc: 0.9314 - val_loss: 0.6692 - val_acc: 0.7444\n",
      "Epoch 1176/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.2439 - acc: 0.9216 - val_loss: 0.6741 - val_acc: 0.7556\n",
      "Epoch 1177/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.2093 - acc: 0.9490 - val_loss: 0.6703 - val_acc: 0.7556\n",
      "Epoch 1178/3000\n",
      "510/510 [==============================] - 0s 685us/step - loss: 0.2238 - acc: 0.9451 - val_loss: 0.6781 - val_acc: 0.7556\n",
      "Epoch 1179/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.2195 - acc: 0.9490 - val_loss: 0.6717 - val_acc: 0.7667\n",
      "Epoch 1180/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2127 - acc: 0.9420Epoch 01180: val_loss did not improve\n",
      "510/510 [==============================] - 0s 637us/step - loss: 0.2192 - acc: 0.9373 - val_loss: 0.6721 - val_acc: 0.7556\n",
      "Epoch 1181/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.2248 - acc: 0.9392 - val_loss: 0.6791 - val_acc: 0.7444\n",
      "Epoch 1182/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.1974 - acc: 0.9529 - val_loss: 0.6864 - val_acc: 0.7444\n",
      "Epoch 1183/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.2362 - acc: 0.9333 - val_loss: 0.6801 - val_acc: 0.7556\n",
      "Epoch 1184/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.2205 - acc: 0.9314 - val_loss: 0.6828 - val_acc: 0.7444\n",
      "Epoch 1185/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2236 - acc: 0.9397Epoch 01185: val_loss did not improve\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.2267 - acc: 0.9333 - val_loss: 0.6930 - val_acc: 0.7222\n",
      "Epoch 1186/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 615us/step - loss: 0.2239 - acc: 0.9333 - val_loss: 0.6918 - val_acc: 0.7333\n",
      "Epoch 1187/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.2114 - acc: 0.9373 - val_loss: 0.6963 - val_acc: 0.7333\n",
      "Epoch 1188/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.1820 - acc: 0.9549 - val_loss: 0.6986 - val_acc: 0.7333\n",
      "Epoch 1189/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.2092 - acc: 0.9471 - val_loss: 0.6972 - val_acc: 0.7222\n",
      "Epoch 1190/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1989 - acc: 0.9375Epoch 01190: val_loss did not improve\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.2066 - acc: 0.9333 - val_loss: 0.6857 - val_acc: 0.7333\n",
      "Epoch 1191/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2022 - acc: 0.9451 - val_loss: 0.6923 - val_acc: 0.7222\n",
      "Epoch 1192/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.2289 - acc: 0.9392 - val_loss: 0.6911 - val_acc: 0.7222\n",
      "Epoch 1193/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.2067 - acc: 0.9471 - val_loss: 0.6751 - val_acc: 0.7444\n",
      "Epoch 1194/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.1989 - acc: 0.9431 - val_loss: 0.6801 - val_acc: 0.7333\n",
      "Epoch 1195/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1970 - acc: 0.9576Epoch 01195: val_loss did not improve\n",
      "510/510 [==============================] - 0s 666us/step - loss: 0.1989 - acc: 0.9529 - val_loss: 0.6796 - val_acc: 0.7222\n",
      "Epoch 1196/3000\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.1822 - acc: 0.957 - 0s 600us/step - loss: 0.1948 - acc: 0.9510 - val_loss: 0.6772 - val_acc: 0.7333\n",
      "Epoch 1197/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.2339 - acc: 0.9333 - val_loss: 0.6791 - val_acc: 0.7444\n",
      "Epoch 1198/3000\n",
      "510/510 [==============================] - 0s 678us/step - loss: 0.1951 - acc: 0.9549 - val_loss: 0.6744 - val_acc: 0.7444\n",
      "Epoch 1199/3000\n",
      "510/510 [==============================] - 0s 650us/step - loss: 0.2119 - acc: 0.9490 - val_loss: 0.6782 - val_acc: 0.7444\n",
      "Epoch 1200/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1956 - acc: 0.9554Epoch 01200: val_loss did not improve\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.2003 - acc: 0.9510 - val_loss: 0.6748 - val_acc: 0.7556\n",
      "Epoch 1201/3000\n",
      "510/510 [==============================] - 0s 672us/step - loss: 0.2119 - acc: 0.9333 - val_loss: 0.6683 - val_acc: 0.7556\n",
      "Epoch 1202/3000\n",
      "510/510 [==============================] - 0s 649us/step - loss: 0.2012 - acc: 0.9549 - val_loss: 0.6705 - val_acc: 0.7556\n",
      "Epoch 1203/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.1966 - acc: 0.9431 - val_loss: 0.6772 - val_acc: 0.7556\n",
      "Epoch 1204/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.2094 - acc: 0.9412 - val_loss: 0.6752 - val_acc: 0.7556\n",
      "Epoch 1205/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2228 - acc: 0.9353Epoch 01205: val_loss did not improve\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.2152 - acc: 0.9412 - val_loss: 0.6674 - val_acc: 0.7778\n",
      "Epoch 1206/3000\n",
      "510/510 [==============================] - 0s 652us/step - loss: 0.2103 - acc: 0.9353 - val_loss: 0.6695 - val_acc: 0.7667\n",
      "Epoch 1207/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2135 - acc: 0.9451 - val_loss: 0.6700 - val_acc: 0.7556\n",
      "Epoch 1208/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.2003 - acc: 0.9412 - val_loss: 0.6709 - val_acc: 0.7556\n",
      "Epoch 1209/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.2218 - acc: 0.9255 - val_loss: 0.6775 - val_acc: 0.7444\n",
      "Epoch 1210/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2119 - acc: 0.9420Epoch 01210: val_loss did not improve\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.2168 - acc: 0.9431 - val_loss: 0.6778 - val_acc: 0.7444\n",
      "Epoch 1211/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.1963 - acc: 0.9627 - val_loss: 0.6687 - val_acc: 0.7556\n",
      "Epoch 1212/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.2124 - acc: 0.9549 - val_loss: 0.6659 - val_acc: 0.7556\n",
      "Epoch 1213/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.2016 - acc: 0.9451 - val_loss: 0.6647 - val_acc: 0.7444\n",
      "Epoch 1214/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2005 - acc: 0.9333 - val_loss: 0.6651 - val_acc: 0.7333\n",
      "Epoch 1215/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.1888 - acc: 0.9635Epoch 01215: val_loss improved from 0.66446 to 0.66223, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 677us/step - loss: 0.2067 - acc: 0.9490 - val_loss: 0.6622 - val_acc: 0.7667\n",
      "Epoch 1216/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.2115 - acc: 0.9471 - val_loss: 0.6642 - val_acc: 0.7667\n",
      "Epoch 1217/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.2217 - acc: 0.9412 - val_loss: 0.6714 - val_acc: 0.7556\n",
      "Epoch 1218/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.2224 - acc: 0.9529 - val_loss: 0.6671 - val_acc: 0.7556\n",
      "Epoch 1219/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.1886 - acc: 0.9510 - val_loss: 0.6745 - val_acc: 0.7667\n",
      "Epoch 1220/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2373 - acc: 0.9330Epoch 01220: val_loss did not improve\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.2305 - acc: 0.9373 - val_loss: 0.6832 - val_acc: 0.7667\n",
      "Epoch 1221/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.2154 - acc: 0.9451 - val_loss: 0.6786 - val_acc: 0.7667\n",
      "Epoch 1222/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.1914 - acc: 0.9569 - val_loss: 0.6734 - val_acc: 0.7667\n",
      "Epoch 1223/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2021 - acc: 0.9471 - val_loss: 0.6753 - val_acc: 0.7667\n",
      "Epoch 1224/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.2190 - acc: 0.9373 - val_loss: 0.6748 - val_acc: 0.7556\n",
      "Epoch 1225/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2387 - acc: 0.9375Epoch 01225: val_loss did not improve\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.2335 - acc: 0.9353 - val_loss: 0.6738 - val_acc: 0.7556\n",
      "Epoch 1226/3000\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.1991 - acc: 0.9510 - val_loss: 0.6696 - val_acc: 0.7556\n",
      "Epoch 1227/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.2141 - acc: 0.9431 - val_loss: 0.6669 - val_acc: 0.7444\n",
      "Epoch 1228/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.2035 - acc: 0.9353 - val_loss: 0.6681 - val_acc: 0.7444\n",
      "Epoch 1229/3000\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.1768 - acc: 0.9569 - val_loss: 0.6712 - val_acc: 0.7556\n",
      "Epoch 1230/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1949 - acc: 0.9531Epoch 01230: val_loss did not improve\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.1961 - acc: 0.9510 - val_loss: 0.6674 - val_acc: 0.7667\n",
      "Epoch 1231/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.2100 - acc: 0.9490 - val_loss: 0.6673 - val_acc: 0.7556\n",
      "Epoch 1232/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.1763 - acc: 0.9667 - val_loss: 0.6771 - val_acc: 0.7556\n",
      "Epoch 1233/3000\n",
      "510/510 [==============================] - 0s 645us/step - loss: 0.2337 - acc: 0.9235 - val_loss: 0.6764 - val_acc: 0.7556\n",
      "Epoch 1234/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.1820 - acc: 0.9627 - val_loss: 0.6738 - val_acc: 0.7556\n",
      "Epoch 1235/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1903 - acc: 0.9531Epoch 01235: val_loss did not improve\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.1945 - acc: 0.9490 - val_loss: 0.6795 - val_acc: 0.7556\n",
      "Epoch 1236/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 593us/step - loss: 0.1945 - acc: 0.9529 - val_loss: 0.6821 - val_acc: 0.7444\n",
      "Epoch 1237/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.2056 - acc: 0.9490 - val_loss: 0.6855 - val_acc: 0.7333\n",
      "Epoch 1238/3000\n",
      "510/510 [==============================] - 0s 547us/step - loss: 0.1864 - acc: 0.9608 - val_loss: 0.6818 - val_acc: 0.7222\n",
      "Epoch 1239/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.2370 - acc: 0.9275 - val_loss: 0.6784 - val_acc: 0.7222\n",
      "Epoch 1240/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1881 - acc: 0.9487Epoch 01240: val_loss did not improve\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.1808 - acc: 0.9529 - val_loss: 0.6869 - val_acc: 0.7333\n",
      "Epoch 1241/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.1950 - acc: 0.9392 - val_loss: 0.6789 - val_acc: 0.7222\n",
      "Epoch 1242/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.1999 - acc: 0.9529 - val_loss: 0.6817 - val_acc: 0.7333\n",
      "Epoch 1243/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.1893 - acc: 0.9529 - val_loss: 0.6777 - val_acc: 0.7333\n",
      "Epoch 1244/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2069 - acc: 0.9431 - val_loss: 0.6742 - val_acc: 0.7333\n",
      "Epoch 1245/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2084 - acc: 0.9353Epoch 01245: val_loss did not improve\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.2149 - acc: 0.9373 - val_loss: 0.6709 - val_acc: 0.7444\n",
      "Epoch 1246/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.2044 - acc: 0.9333 - val_loss: 0.6879 - val_acc: 0.7444\n",
      "Epoch 1247/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.2092 - acc: 0.9373 - val_loss: 0.6799 - val_acc: 0.7556\n",
      "Epoch 1248/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.2049 - acc: 0.9392 - val_loss: 0.6714 - val_acc: 0.7444\n",
      "Epoch 1249/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.1953 - acc: 0.9490 - val_loss: 0.6711 - val_acc: 0.7667\n",
      "Epoch 1250/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1903 - acc: 0.9353Epoch 01250: val_loss did not improve\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.1909 - acc: 0.9373 - val_loss: 0.6697 - val_acc: 0.7556\n",
      "Epoch 1251/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.2003 - acc: 0.9510 - val_loss: 0.6777 - val_acc: 0.7333\n",
      "Epoch 1252/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.1980 - acc: 0.9510 - val_loss: 0.6829 - val_acc: 0.7333\n",
      "Epoch 1253/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2214 - acc: 0.9333 - val_loss: 0.6822 - val_acc: 0.7444\n",
      "Epoch 1254/3000\n",
      "510/510 [==============================] - 0s 547us/step - loss: 0.2053 - acc: 0.9490 - val_loss: 0.6837 - val_acc: 0.7222\n",
      "Epoch 1255/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1935 - acc: 0.9353Epoch 01255: val_loss did not improve\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.1909 - acc: 0.9373 - val_loss: 0.6817 - val_acc: 0.7333\n",
      "Epoch 1256/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.1912 - acc: 0.9451 - val_loss: 0.6812 - val_acc: 0.7333\n",
      "Epoch 1257/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.1868 - acc: 0.9588 - val_loss: 0.6837 - val_acc: 0.7333\n",
      "Epoch 1258/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.1861 - acc: 0.9471 - val_loss: 0.6754 - val_acc: 0.7333\n",
      "Epoch 1259/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.1955 - acc: 0.9471 - val_loss: 0.6815 - val_acc: 0.7222\n",
      "Epoch 1260/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2075 - acc: 0.9375Epoch 01260: val_loss did not improve\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.2092 - acc: 0.9392 - val_loss: 0.6743 - val_acc: 0.7444\n",
      "Epoch 1261/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.1825 - acc: 0.9549 - val_loss: 0.6793 - val_acc: 0.7333\n",
      "Epoch 1262/3000\n",
      "510/510 [==============================] - 0s 541us/step - loss: 0.2063 - acc: 0.9353 - val_loss: 0.6729 - val_acc: 0.7333\n",
      "Epoch 1263/3000\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.2040 - acc: 0.9431 - val_loss: 0.6760 - val_acc: 0.7333\n",
      "Epoch 1264/3000\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.1938 - acc: 0.9529 - val_loss: 0.6723 - val_acc: 0.7333\n",
      "Epoch 1265/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2085 - acc: 0.9308Epoch 01265: val_loss did not improve\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.2088 - acc: 0.9314 - val_loss: 0.6646 - val_acc: 0.7333\n",
      "Epoch 1266/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.2061 - acc: 0.9392 - val_loss: 0.6590 - val_acc: 0.7444\n",
      "Epoch 1267/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.1927 - acc: 0.9529 - val_loss: 0.6581 - val_acc: 0.7556\n",
      "Epoch 1268/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.1902 - acc: 0.9510 - val_loss: 0.6633 - val_acc: 0.7333\n",
      "Epoch 1269/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.2002 - acc: 0.9451 - val_loss: 0.6647 - val_acc: 0.7333\n",
      "Epoch 1270/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2045 - acc: 0.9487Epoch 01270: val_loss did not improve\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.2044 - acc: 0.9490 - val_loss: 0.6624 - val_acc: 0.7444\n",
      "Epoch 1271/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.2010 - acc: 0.9333 - val_loss: 0.6604 - val_acc: 0.7444\n",
      "Epoch 1272/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.1931 - acc: 0.9412 - val_loss: 0.6606 - val_acc: 0.7222\n",
      "Epoch 1273/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.2033 - acc: 0.9392 - val_loss: 0.6630 - val_acc: 0.7444\n",
      "Epoch 1274/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.2076 - acc: 0.9392 - val_loss: 0.6485 - val_acc: 0.7556\n",
      "Epoch 1275/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2019 - acc: 0.9420Epoch 01275: val_loss improved from 0.66223 to 0.65053, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.2001 - acc: 0.9431 - val_loss: 0.6505 - val_acc: 0.7556\n",
      "Epoch 1276/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.1961 - acc: 0.9451 - val_loss: 0.6511 - val_acc: 0.7444\n",
      "Epoch 1277/3000\n",
      "510/510 [==============================] - 0s 551us/step - loss: 0.1977 - acc: 0.9451 - val_loss: 0.6555 - val_acc: 0.7333\n",
      "Epoch 1278/3000\n",
      "510/510 [==============================] - 0s 568us/step - loss: 0.1873 - acc: 0.9529 - val_loss: 0.6604 - val_acc: 0.7333\n",
      "Epoch 1279/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.2117 - acc: 0.9353 - val_loss: 0.6575 - val_acc: 0.7333\n",
      "Epoch 1280/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1848 - acc: 0.9576Epoch 01280: val_loss did not improve\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.1866 - acc: 0.9569 - val_loss: 0.6594 - val_acc: 0.7222\n",
      "Epoch 1281/3000\n",
      "510/510 [==============================] - 0s 545us/step - loss: 0.1864 - acc: 0.9510 - val_loss: 0.6570 - val_acc: 0.7333\n",
      "Epoch 1282/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.1829 - acc: 0.9588 - val_loss: 0.6518 - val_acc: 0.7444\n",
      "Epoch 1283/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.1767 - acc: 0.9588 - val_loss: 0.6598 - val_acc: 0.7444\n",
      "Epoch 1284/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.1924 - acc: 0.9451 - val_loss: 0.6569 - val_acc: 0.7333\n",
      "Epoch 1285/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1838 - acc: 0.9464Epoch 01285: val_loss did not improve\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.1934 - acc: 0.9412 - val_loss: 0.6566 - val_acc: 0.7556\n",
      "Epoch 1286/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 558us/step - loss: 0.2080 - acc: 0.9373 - val_loss: 0.6479 - val_acc: 0.7444\n",
      "Epoch 1287/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.1847 - acc: 0.9471 - val_loss: 0.6540 - val_acc: 0.7444\n",
      "Epoch 1288/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.1930 - acc: 0.9471 - val_loss: 0.6489 - val_acc: 0.7444\n",
      "Epoch 1289/3000\n",
      "510/510 [==============================] - 0s 554us/step - loss: 0.1855 - acc: 0.9490 - val_loss: 0.6480 - val_acc: 0.7333\n",
      "Epoch 1290/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2046 - acc: 0.9509Epoch 01290: val_loss improved from 0.65053 to 0.64680, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.2085 - acc: 0.9510 - val_loss: 0.6468 - val_acc: 0.7222\n",
      "Epoch 1291/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.2020 - acc: 0.9353 - val_loss: 0.6539 - val_acc: 0.7222\n",
      "Epoch 1292/3000\n",
      "510/510 [==============================] - 0s 554us/step - loss: 0.1934 - acc: 0.9490 - val_loss: 0.6567 - val_acc: 0.7222\n",
      "Epoch 1293/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.2051 - acc: 0.9392 - val_loss: 0.6549 - val_acc: 0.7222\n",
      "Epoch 1294/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.1908 - acc: 0.9549 - val_loss: 0.6494 - val_acc: 0.7333\n",
      "Epoch 1295/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1649 - acc: 0.9598Epoch 01295: val_loss did not improve\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.1685 - acc: 0.9588 - val_loss: 0.6539 - val_acc: 0.7333\n",
      "Epoch 1296/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.1908 - acc: 0.9529 - val_loss: 0.6519 - val_acc: 0.7333\n",
      "Epoch 1297/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.1780 - acc: 0.9529 - val_loss: 0.6452 - val_acc: 0.7556\n",
      "Epoch 1298/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.1873 - acc: 0.9490 - val_loss: 0.6583 - val_acc: 0.7444\n",
      "Epoch 1299/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.1876 - acc: 0.9412 - val_loss: 0.6597 - val_acc: 0.7444\n",
      "Epoch 1300/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2015 - acc: 0.9397Epoch 01300: val_loss did not improve\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.1995 - acc: 0.9431 - val_loss: 0.6506 - val_acc: 0.7444\n",
      "Epoch 1301/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.1938 - acc: 0.9412 - val_loss: 0.6559 - val_acc: 0.7333\n",
      "Epoch 1302/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.1919 - acc: 0.9490 - val_loss: 0.6538 - val_acc: 0.7556\n",
      "Epoch 1303/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.2005 - acc: 0.9431 - val_loss: 0.6466 - val_acc: 0.7444\n",
      "Epoch 1304/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.2089 - acc: 0.9471 - val_loss: 0.6443 - val_acc: 0.7333\n",
      "Epoch 1305/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1841 - acc: 0.9375Epoch 01305: val_loss did not improve\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.1823 - acc: 0.9412 - val_loss: 0.6503 - val_acc: 0.7444\n",
      "Epoch 1306/3000\n",
      "510/510 [==============================] - 0s 541us/step - loss: 0.1901 - acc: 0.9392 - val_loss: 0.6421 - val_acc: 0.7444\n",
      "Epoch 1307/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.2025 - acc: 0.9412 - val_loss: 0.6503 - val_acc: 0.7667\n",
      "Epoch 1308/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.2057 - acc: 0.9471 - val_loss: 0.6447 - val_acc: 0.7556\n",
      "Epoch 1309/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.1742 - acc: 0.9647 - val_loss: 0.6336 - val_acc: 0.7778\n",
      "Epoch 1310/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2326 - acc: 0.9353Epoch 01310: val_loss improved from 0.64680 to 0.63014, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.2179 - acc: 0.9412 - val_loss: 0.6301 - val_acc: 0.7889\n",
      "Epoch 1311/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.1924 - acc: 0.9412 - val_loss: 0.6374 - val_acc: 0.7778\n",
      "Epoch 1312/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.2190 - acc: 0.9412 - val_loss: 0.6304 - val_acc: 0.7667\n",
      "Epoch 1313/3000\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.1904 - acc: 0.9451 - val_loss: 0.6336 - val_acc: 0.7778\n",
      "Epoch 1314/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.1894 - acc: 0.9569 - val_loss: 0.6298 - val_acc: 0.7778\n",
      "Epoch 1315/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2085 - acc: 0.9353Epoch 01315: val_loss did not improve\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.2104 - acc: 0.9373 - val_loss: 0.6376 - val_acc: 0.7667\n",
      "Epoch 1316/3000\n",
      "510/510 [==============================] - 0s 526us/step - loss: 0.2016 - acc: 0.9569 - val_loss: 0.6399 - val_acc: 0.7556\n",
      "Epoch 1317/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.2013 - acc: 0.9353 - val_loss: 0.6426 - val_acc: 0.7556\n",
      "Epoch 1318/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.1891 - acc: 0.9451 - val_loss: 0.6385 - val_acc: 0.7556\n",
      "Epoch 1319/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.1958 - acc: 0.9451 - val_loss: 0.6408 - val_acc: 0.7556\n",
      "Epoch 1320/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1867 - acc: 0.9621Epoch 01320: val_loss did not improve\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.1942 - acc: 0.9588 - val_loss: 0.6366 - val_acc: 0.7667\n",
      "Epoch 1321/3000\n",
      "510/510 [==============================] - 0s 543us/step - loss: 0.1948 - acc: 0.9471 - val_loss: 0.6443 - val_acc: 0.7667\n",
      "Epoch 1322/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 0.1861 - acc: 0.9510 - val_loss: 0.6478 - val_acc: 0.7667\n",
      "Epoch 1323/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.1905 - acc: 0.9451 - val_loss: 0.6480 - val_acc: 0.7556\n",
      "Epoch 1324/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.1953 - acc: 0.9412 - val_loss: 0.6461 - val_acc: 0.7667\n",
      "Epoch 1325/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1775 - acc: 0.9554Epoch 01325: val_loss did not improve\n",
      "510/510 [==============================] - 0s 535us/step - loss: 0.1837 - acc: 0.9529 - val_loss: 0.6388 - val_acc: 0.7556\n",
      "Epoch 1326/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.2208 - acc: 0.9235 - val_loss: 0.6474 - val_acc: 0.7444\n",
      "Epoch 1327/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.2096 - acc: 0.9490 - val_loss: 0.6446 - val_acc: 0.7556\n",
      "Epoch 1328/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.2076 - acc: 0.9510 - val_loss: 0.6408 - val_acc: 0.7556\n",
      "Epoch 1329/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.1762 - acc: 0.9451 - val_loss: 0.6396 - val_acc: 0.7556\n",
      "Epoch 1330/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1880 - acc: 0.9420Epoch 01330: val_loss did not improve\n",
      "510/510 [==============================] - 0s 552us/step - loss: 0.1855 - acc: 0.9471 - val_loss: 0.6449 - val_acc: 0.7667\n",
      "Epoch 1331/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.1921 - acc: 0.9490 - val_loss: 0.6454 - val_acc: 0.7444\n",
      "Epoch 1332/3000\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.1952 - acc: 0.9451 - val_loss: 0.6477 - val_acc: 0.7556\n",
      "Epoch 1333/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.1795 - acc: 0.9490 - val_loss: 0.6487 - val_acc: 0.7778\n",
      "Epoch 1334/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.2005 - acc: 0.9412 - val_loss: 0.6487 - val_acc: 0.7889\n",
      "Epoch 1335/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1622 - acc: 0.9688Epoch 01335: val_loss did not improve\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.1727 - acc: 0.9608 - val_loss: 0.6462 - val_acc: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1336/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.1782 - acc: 0.9510 - val_loss: 0.6453 - val_acc: 0.7667\n",
      "Epoch 1337/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.1842 - acc: 0.9451 - val_loss: 0.6443 - val_acc: 0.7667\n",
      "Epoch 1338/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.1831 - acc: 0.9529 - val_loss: 0.6536 - val_acc: 0.7556\n",
      "Epoch 1339/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.1955 - acc: 0.9353 - val_loss: 0.6490 - val_acc: 0.7556\n",
      "Epoch 1340/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1900 - acc: 0.9554Epoch 01340: val_loss did not improve\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.1878 - acc: 0.9569 - val_loss: 0.6405 - val_acc: 0.7556\n",
      "Epoch 1341/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.2201 - acc: 0.9314 - val_loss: 0.6422 - val_acc: 0.7556\n",
      "Epoch 1342/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.1832 - acc: 0.9529 - val_loss: 0.6351 - val_acc: 0.7667\n",
      "Epoch 1343/3000\n",
      "510/510 [==============================] - 0s 568us/step - loss: 0.2001 - acc: 0.9471 - val_loss: 0.6396 - val_acc: 0.7667\n",
      "Epoch 1344/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.1931 - acc: 0.9451 - val_loss: 0.6445 - val_acc: 0.7556\n",
      "Epoch 1345/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1824 - acc: 0.9531Epoch 01345: val_loss did not improve\n",
      "510/510 [==============================] - 0s 555us/step - loss: 0.1822 - acc: 0.9471 - val_loss: 0.6509 - val_acc: 0.7556\n",
      "Epoch 1346/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.2042 - acc: 0.9255 - val_loss: 0.6465 - val_acc: 0.7556\n",
      "Epoch 1347/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.2080 - acc: 0.9373 - val_loss: 0.6427 - val_acc: 0.7667\n",
      "Epoch 1348/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.1699 - acc: 0.9608 - val_loss: 0.6364 - val_acc: 0.7667\n",
      "Epoch 1349/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.2016 - acc: 0.9471 - val_loss: 0.6380 - val_acc: 0.7444\n",
      "Epoch 1350/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1912 - acc: 0.9464Epoch 01350: val_loss did not improve\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.1961 - acc: 0.9431 - val_loss: 0.6423 - val_acc: 0.7778\n",
      "Epoch 1351/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.1826 - acc: 0.9529 - val_loss: 0.6452 - val_acc: 0.7556\n",
      "Epoch 1352/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.1766 - acc: 0.9451 - val_loss: 0.6490 - val_acc: 0.7667\n",
      "Epoch 1353/3000\n",
      "510/510 [==============================] - 0s 556us/step - loss: 0.1929 - acc: 0.9471 - val_loss: 0.6572 - val_acc: 0.7444\n",
      "Epoch 1354/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.2017 - acc: 0.9490 - val_loss: 0.6466 - val_acc: 0.7556\n",
      "Epoch 1355/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1841 - acc: 0.9509Epoch 01355: val_loss did not improve\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.1758 - acc: 0.9529 - val_loss: 0.6520 - val_acc: 0.7667\n",
      "Epoch 1356/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.1777 - acc: 0.9549 - val_loss: 0.6432 - val_acc: 0.7667\n",
      "Epoch 1357/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.1796 - acc: 0.9490 - val_loss: 0.6448 - val_acc: 0.7556\n",
      "Epoch 1358/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.1966 - acc: 0.9412 - val_loss: 0.6409 - val_acc: 0.7778\n",
      "Epoch 1359/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.1842 - acc: 0.9529 - val_loss: 0.6327 - val_acc: 0.7889\n",
      "Epoch 1360/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1696 - acc: 0.9665Epoch 01360: val_loss did not improve\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.1793 - acc: 0.9608 - val_loss: 0.6444 - val_acc: 0.7444\n",
      "Epoch 1361/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.1866 - acc: 0.9608 - val_loss: 0.6439 - val_acc: 0.7444\n",
      "Epoch 1362/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.1965 - acc: 0.9333 - val_loss: 0.6455 - val_acc: 0.7556\n",
      "Epoch 1363/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.1957 - acc: 0.9431 - val_loss: 0.6360 - val_acc: 0.7778\n",
      "Epoch 1364/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.2023 - acc: 0.9412 - val_loss: 0.6257 - val_acc: 0.7778\n",
      "Epoch 1365/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1796 - acc: 0.9643Epoch 01365: val_loss did not improve\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.1813 - acc: 0.9627 - val_loss: 0.6371 - val_acc: 0.7667\n",
      "Epoch 1366/3000\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.1728 - acc: 0.9608 - val_loss: 0.6315 - val_acc: 0.7667\n",
      "Epoch 1367/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.1899 - acc: 0.9294 - val_loss: 0.6213 - val_acc: 0.7778\n",
      "Epoch 1368/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.1746 - acc: 0.9569 - val_loss: 0.6280 - val_acc: 0.7667\n",
      "Epoch 1369/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.1773 - acc: 0.9549 - val_loss: 0.6331 - val_acc: 0.7556\n",
      "Epoch 1370/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1886 - acc: 0.9487Epoch 01370: val_loss did not improve\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.1864 - acc: 0.9510 - val_loss: 0.6356 - val_acc: 0.7556\n",
      "Epoch 1371/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.1912 - acc: 0.9510 - val_loss: 0.6276 - val_acc: 0.7444\n",
      "Epoch 1372/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.1858 - acc: 0.9451 - val_loss: 0.6291 - val_acc: 0.7444\n",
      "Epoch 1373/3000\n",
      "510/510 [==============================] - 0s 547us/step - loss: 0.1814 - acc: 0.9490 - val_loss: 0.6322 - val_acc: 0.7556\n",
      "Epoch 1374/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.1714 - acc: 0.9549 - val_loss: 0.6490 - val_acc: 0.7444\n",
      "Epoch 1375/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1965 - acc: 0.9420Epoch 01375: val_loss did not improve\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.1953 - acc: 0.9431 - val_loss: 0.6492 - val_acc: 0.7444\n",
      "Epoch 1376/3000\n",
      "510/510 [==============================] - 0s 546us/step - loss: 0.1768 - acc: 0.9510 - val_loss: 0.6412 - val_acc: 0.7556\n",
      "Epoch 1377/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.1848 - acc: 0.9392 - val_loss: 0.6429 - val_acc: 0.7667\n",
      "Epoch 1378/3000\n",
      "510/510 [==============================] - 0s 552us/step - loss: 0.1928 - acc: 0.9451 - val_loss: 0.6294 - val_acc: 0.7778\n",
      "Epoch 1379/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.1737 - acc: 0.9569 - val_loss: 0.6330 - val_acc: 0.7778\n",
      "Epoch 1380/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2033 - acc: 0.9286Epoch 01380: val_loss improved from 0.63014 to 0.61699, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.1989 - acc: 0.9314 - val_loss: 0.6170 - val_acc: 0.7778\n",
      "Epoch 1381/3000\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.1695 - acc: 0.9549 - val_loss: 0.6244 - val_acc: 0.7556\n",
      "Epoch 1382/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.1905 - acc: 0.9451 - val_loss: 0.6244 - val_acc: 0.7556\n",
      "Epoch 1383/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.2008 - acc: 0.9451 - val_loss: 0.6132 - val_acc: 0.7667\n",
      "Epoch 1384/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.1842 - acc: 0.9529 - val_loss: 0.6217 - val_acc: 0.7667\n",
      "Epoch 1385/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1862 - acc: 0.9554Epoch 01385: val_loss did not improve\n",
      "510/510 [==============================] - 0s 558us/step - loss: 0.1810 - acc: 0.9529 - val_loss: 0.6223 - val_acc: 0.7444\n",
      "Epoch 1386/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 573us/step - loss: 0.1573 - acc: 0.9745 - val_loss: 0.6282 - val_acc: 0.7667\n",
      "Epoch 1387/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.1783 - acc: 0.9549 - val_loss: 0.6306 - val_acc: 0.7667\n",
      "Epoch 1388/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.1766 - acc: 0.9529 - val_loss: 0.6258 - val_acc: 0.7778\n",
      "Epoch 1389/3000\n",
      "510/510 [==============================] - 0s 561us/step - loss: 0.1788 - acc: 0.9510 - val_loss: 0.6263 - val_acc: 0.7778\n",
      "Epoch 1390/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1632 - acc: 0.9598Epoch 01390: val_loss did not improve\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.1743 - acc: 0.9549 - val_loss: 0.6243 - val_acc: 0.7889\n",
      "Epoch 1391/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.1861 - acc: 0.9510 - val_loss: 0.6339 - val_acc: 0.7778\n",
      "Epoch 1392/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.1726 - acc: 0.9510 - val_loss: 0.6293 - val_acc: 0.7667\n",
      "Epoch 1393/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.1746 - acc: 0.9667 - val_loss: 0.6261 - val_acc: 0.7778\n",
      "Epoch 1394/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.1862 - acc: 0.9569 - val_loss: 0.6244 - val_acc: 0.7778\n",
      "Epoch 1395/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1780 - acc: 0.9464Epoch 01395: val_loss improved from 0.61699 to 0.61098, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.1776 - acc: 0.9490 - val_loss: 0.6110 - val_acc: 0.7778\n",
      "Epoch 1396/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.1699 - acc: 0.9647 - val_loss: 0.6182 - val_acc: 0.7778\n",
      "Epoch 1397/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.1651 - acc: 0.9608 - val_loss: 0.6213 - val_acc: 0.7667\n",
      "Epoch 1398/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.1719 - acc: 0.9549 - val_loss: 0.6240 - val_acc: 0.7667\n",
      "Epoch 1399/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.1883 - acc: 0.9412 - val_loss: 0.6211 - val_acc: 0.7556\n",
      "Epoch 1400/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1670 - acc: 0.9531Epoch 01400: val_loss did not improve\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.1642 - acc: 0.9549 - val_loss: 0.6379 - val_acc: 0.7333\n",
      "Epoch 1401/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.1822 - acc: 0.9431 - val_loss: 0.6365 - val_acc: 0.7333\n",
      "Epoch 1402/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.1760 - acc: 0.9569 - val_loss: 0.6338 - val_acc: 0.7556\n",
      "Epoch 1403/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.1697 - acc: 0.9608 - val_loss: 0.6372 - val_acc: 0.7556\n",
      "Epoch 1404/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.1859 - acc: 0.9392 - val_loss: 0.6434 - val_acc: 0.7556\n",
      "Epoch 1405/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1711 - acc: 0.9531Epoch 01405: val_loss did not improve\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.1757 - acc: 0.9529 - val_loss: 0.6436 - val_acc: 0.7556\n",
      "Epoch 1406/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.1758 - acc: 0.9490 - val_loss: 0.6408 - val_acc: 0.7556\n",
      "Epoch 1407/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.1821 - acc: 0.9490 - val_loss: 0.6455 - val_acc: 0.7444\n",
      "Epoch 1408/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.1908 - acc: 0.9490 - val_loss: 0.6320 - val_acc: 0.7556\n",
      "Epoch 1409/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.1986 - acc: 0.9529 - val_loss: 0.6300 - val_acc: 0.7556\n",
      "Epoch 1410/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1830 - acc: 0.9420Epoch 01410: val_loss did not improve\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.1883 - acc: 0.9431 - val_loss: 0.6265 - val_acc: 0.7667\n",
      "Epoch 1411/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.1692 - acc: 0.9647 - val_loss: 0.6170 - val_acc: 0.7667\n",
      "Epoch 1412/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.1643 - acc: 0.9569 - val_loss: 0.6287 - val_acc: 0.7444\n",
      "Epoch 1413/3000\n",
      "510/510 [==============================] - 0s 654us/step - loss: 0.1727 - acc: 0.9588 - val_loss: 0.6150 - val_acc: 0.7556\n",
      "Epoch 1414/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.1754 - acc: 0.9608 - val_loss: 0.6118 - val_acc: 0.7556\n",
      "Epoch 1415/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1838 - acc: 0.9509Epoch 01415: val_loss did not improve\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.1841 - acc: 0.9471 - val_loss: 0.6126 - val_acc: 0.7556\n",
      "Epoch 1416/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.1703 - acc: 0.9549 - val_loss: 0.6150 - val_acc: 0.7556\n",
      "Epoch 1417/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.1689 - acc: 0.9608 - val_loss: 0.6134 - val_acc: 0.7556\n",
      "Epoch 1418/3000\n",
      "510/510 [==============================] - 0s 637us/step - loss: 0.1867 - acc: 0.9490 - val_loss: 0.6158 - val_acc: 0.7444\n",
      "Epoch 1419/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.1781 - acc: 0.9549 - val_loss: 0.6127 - val_acc: 0.7556\n",
      "Epoch 1420/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.1615 - acc: 0.9557Epoch 01420: val_loss improved from 0.61098 to 0.61050, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 693us/step - loss: 0.1706 - acc: 0.9510 - val_loss: 0.6105 - val_acc: 0.7778\n",
      "Epoch 1421/3000\n",
      "510/510 [==============================] - 0s 694us/step - loss: 0.1863 - acc: 0.9529 - val_loss: 0.6131 - val_acc: 0.7667\n",
      "Epoch 1422/3000\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.1694 - acc: 0.9549 - val_loss: 0.6117 - val_acc: 0.7667\n",
      "Epoch 1423/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.1675 - acc: 0.9529 - val_loss: 0.6108 - val_acc: 0.7444\n",
      "Epoch 1424/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.1585 - acc: 0.9529 - val_loss: 0.6088 - val_acc: 0.7667\n",
      "Epoch 1425/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1747 - acc: 0.9576Epoch 01425: val_loss improved from 0.61050 to 0.60586, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.1835 - acc: 0.9549 - val_loss: 0.6059 - val_acc: 0.7667\n",
      "Epoch 1426/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.1711 - acc: 0.9451 - val_loss: 0.6132 - val_acc: 0.7444\n",
      "Epoch 1427/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.1907 - acc: 0.9471 - val_loss: 0.6225 - val_acc: 0.7778\n",
      "Epoch 1428/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.1857 - acc: 0.9471 - val_loss: 0.6202 - val_acc: 0.7667\n",
      "Epoch 1429/3000\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.1832 - acc: 0.9490 - val_loss: 0.6194 - val_acc: 0.7778\n",
      "Epoch 1430/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1724 - acc: 0.9509Epoch 01430: val_loss did not improve\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.1686 - acc: 0.9569 - val_loss: 0.6198 - val_acc: 0.7667\n",
      "Epoch 1431/3000\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.1668 - acc: 0.9627 - val_loss: 0.6082 - val_acc: 0.7778\n",
      "Epoch 1432/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.1713 - acc: 0.9549 - val_loss: 0.6085 - val_acc: 0.7778\n",
      "Epoch 1433/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.1691 - acc: 0.9490 - val_loss: 0.6219 - val_acc: 0.7667\n",
      "Epoch 1434/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.1894 - acc: 0.9373 - val_loss: 0.6271 - val_acc: 0.7556\n",
      "Epoch 1435/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1664 - acc: 0.9509Epoch 01435: val_loss did not improve\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.1626 - acc: 0.9549 - val_loss: 0.6256 - val_acc: 0.7556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1436/3000\n",
      "510/510 [==============================] - 0s 659us/step - loss: 0.1620 - acc: 0.9569 - val_loss: 0.6224 - val_acc: 0.7778\n",
      "Epoch 1437/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.1685 - acc: 0.9647 - val_loss: 0.6210 - val_acc: 0.7667\n",
      "Epoch 1438/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.1625 - acc: 0.9549 - val_loss: 0.6305 - val_acc: 0.7667\n",
      "Epoch 1439/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.2029 - acc: 0.9333 - val_loss: 0.6320 - val_acc: 0.7667\n",
      "Epoch 1440/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1656 - acc: 0.9576Epoch 01440: val_loss did not improve\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.1648 - acc: 0.9588 - val_loss: 0.6255 - val_acc: 0.7556\n",
      "Epoch 1441/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.1726 - acc: 0.9451 - val_loss: 0.6237 - val_acc: 0.7556\n",
      "Epoch 1442/3000\n",
      "510/510 [==============================] - 0s 662us/step - loss: 0.1678 - acc: 0.9529 - val_loss: 0.6077 - val_acc: 0.7778\n",
      "Epoch 1443/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.1791 - acc: 0.9412 - val_loss: 0.6061 - val_acc: 0.8111\n",
      "Epoch 1444/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.1758 - acc: 0.9549 - val_loss: 0.6154 - val_acc: 0.7889\n",
      "Epoch 1445/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1681 - acc: 0.9554Epoch 01445: val_loss did not improve\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.1659 - acc: 0.9549 - val_loss: 0.6235 - val_acc: 0.7667\n",
      "Epoch 1446/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.1633 - acc: 0.9588 - val_loss: 0.6213 - val_acc: 0.8111\n",
      "Epoch 1447/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.1891 - acc: 0.9431 - val_loss: 0.6085 - val_acc: 0.7889\n",
      "Epoch 1448/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.1711 - acc: 0.9471 - val_loss: 0.6108 - val_acc: 0.7889\n",
      "Epoch 1449/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.1838 - acc: 0.9588 - val_loss: 0.6103 - val_acc: 0.7889\n",
      "Epoch 1450/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.1633 - acc: 0.9635Epoch 01450: val_loss did not improve\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.1630 - acc: 0.9608 - val_loss: 0.6164 - val_acc: 0.8000\n",
      "Epoch 1451/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.1539 - acc: 0.9608 - val_loss: 0.6160 - val_acc: 0.8000\n",
      "Epoch 1452/3000\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.1716 - acc: 0.9510 - val_loss: 0.6150 - val_acc: 0.8000\n",
      "Epoch 1453/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.1918 - acc: 0.9471 - val_loss: 0.6199 - val_acc: 0.7889\n",
      "Epoch 1454/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.1674 - acc: 0.9549 - val_loss: 0.6188 - val_acc: 0.7778\n",
      "Epoch 1455/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1807 - acc: 0.9487Epoch 01455: val_loss did not improve\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.1810 - acc: 0.9490 - val_loss: 0.6138 - val_acc: 0.7889\n",
      "Epoch 1456/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.1657 - acc: 0.9608 - val_loss: 0.6176 - val_acc: 0.7778\n",
      "Epoch 1457/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.1676 - acc: 0.9549 - val_loss: 0.6135 - val_acc: 0.7778\n",
      "Epoch 1458/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.1599 - acc: 0.9627 - val_loss: 0.6195 - val_acc: 0.7889\n",
      "Epoch 1459/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.1848 - acc: 0.9471 - val_loss: 0.6165 - val_acc: 0.7889\n",
      "Epoch 1460/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1680 - acc: 0.9464Epoch 01460: val_loss did not improve\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.1732 - acc: 0.9431 - val_loss: 0.6246 - val_acc: 0.7778\n",
      "Epoch 1461/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.1709 - acc: 0.9569 - val_loss: 0.6233 - val_acc: 0.7778\n",
      "Epoch 1462/3000\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.1807 - acc: 0.9412 - val_loss: 0.6163 - val_acc: 0.7778\n",
      "Epoch 1463/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.1490 - acc: 0.9627 - val_loss: 0.6110 - val_acc: 0.7889\n",
      "Epoch 1464/3000\n",
      "510/510 [==============================] - 0s 650us/step - loss: 0.1827 - acc: 0.9451 - val_loss: 0.6064 - val_acc: 0.8000\n",
      "Epoch 1465/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1915 - acc: 0.9442Epoch 01465: val_loss improved from 0.60586 to 0.60421, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.1814 - acc: 0.9490 - val_loss: 0.6042 - val_acc: 0.7889\n",
      "Epoch 1466/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.1700 - acc: 0.9529 - val_loss: 0.6200 - val_acc: 0.7778\n",
      "Epoch 1467/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.1644 - acc: 0.9549 - val_loss: 0.6254 - val_acc: 0.7667\n",
      "Epoch 1468/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.1797 - acc: 0.9471 - val_loss: 0.6161 - val_acc: 0.7778\n",
      "Epoch 1469/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.1761 - acc: 0.9569 - val_loss: 0.6217 - val_acc: 0.7778\n",
      "Epoch 1470/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1625 - acc: 0.9576Epoch 01470: val_loss did not improve\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.1569 - acc: 0.9608 - val_loss: 0.6184 - val_acc: 0.7778\n",
      "Epoch 1471/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.1581 - acc: 0.9627 - val_loss: 0.6233 - val_acc: 0.7889\n",
      "Epoch 1472/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.1886 - acc: 0.9471 - val_loss: 0.6282 - val_acc: 0.7889\n",
      "Epoch 1473/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.1810 - acc: 0.9490 - val_loss: 0.6259 - val_acc: 0.7778\n",
      "Epoch 1474/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.1664 - acc: 0.9569 - val_loss: 0.6247 - val_acc: 0.7778\n",
      "Epoch 1475/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.2005 - acc: 0.9308Epoch 01475: val_loss did not improve\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.1934 - acc: 0.9333 - val_loss: 0.6258 - val_acc: 0.7778\n",
      "Epoch 1476/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.1681 - acc: 0.9569 - val_loss: 0.6258 - val_acc: 0.7778\n",
      "Epoch 1477/3000\n",
      "510/510 [==============================] - 0s 645us/step - loss: 0.1685 - acc: 0.9490 - val_loss: 0.6144 - val_acc: 0.7778\n",
      "Epoch 1478/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.1624 - acc: 0.9549 - val_loss: 0.6203 - val_acc: 0.7889\n",
      "Epoch 1479/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.1463 - acc: 0.9686 - val_loss: 0.6259 - val_acc: 0.7889\n",
      "Epoch 1480/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1678 - acc: 0.9598Epoch 01480: val_loss did not improve\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.1648 - acc: 0.9608 - val_loss: 0.6166 - val_acc: 0.7889\n",
      "Epoch 1481/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.1759 - acc: 0.9529 - val_loss: 0.6164 - val_acc: 0.7889\n",
      "Epoch 1482/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.1749 - acc: 0.9373 - val_loss: 0.6157 - val_acc: 0.7778\n",
      "Epoch 1483/3000\n",
      "510/510 [==============================] - 0s 663us/step - loss: 0.1626 - acc: 0.9588 - val_loss: 0.6157 - val_acc: 0.7778\n",
      "Epoch 1484/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.1737 - acc: 0.9627 - val_loss: 0.6082 - val_acc: 0.7889\n",
      "Epoch 1485/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1656 - acc: 0.9554Epoch 01485: val_loss did not improve\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.1638 - acc: 0.9569 - val_loss: 0.6073 - val_acc: 0.8000\n",
      "Epoch 1486/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 607us/step - loss: 0.1752 - acc: 0.9431 - val_loss: 0.6138 - val_acc: 0.8000\n",
      "Epoch 1487/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.1825 - acc: 0.9471 - val_loss: 0.6124 - val_acc: 0.7889\n",
      "Epoch 1488/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.1602 - acc: 0.9608 - val_loss: 0.6066 - val_acc: 0.7889\n",
      "Epoch 1489/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.1666 - acc: 0.9569 - val_loss: 0.6037 - val_acc: 0.7667\n",
      "Epoch 1490/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1695 - acc: 0.9509Epoch 01490: val_loss did not improve\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.1671 - acc: 0.9549 - val_loss: 0.6176 - val_acc: 0.7778\n",
      "Epoch 1491/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.1860 - acc: 0.9490 - val_loss: 0.6193 - val_acc: 0.7556\n",
      "Epoch 1492/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.1582 - acc: 0.9588 - val_loss: 0.6188 - val_acc: 0.7889\n",
      "Epoch 1493/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.1725 - acc: 0.9529 - val_loss: 0.5992 - val_acc: 0.7889\n",
      "Epoch 1494/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.1722 - acc: 0.9451 - val_loss: 0.6017 - val_acc: 0.7778\n",
      "Epoch 1495/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1522 - acc: 0.9554Epoch 01495: val_loss did not improve\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.1518 - acc: 0.9549 - val_loss: 0.6104 - val_acc: 0.7667\n",
      "Epoch 1496/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.1722 - acc: 0.9510 - val_loss: 0.6068 - val_acc: 0.7778\n",
      "Epoch 1497/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.1538 - acc: 0.9706 - val_loss: 0.6060 - val_acc: 0.7778\n",
      "Epoch 1498/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.1711 - acc: 0.9569 - val_loss: 0.6111 - val_acc: 0.7667\n",
      "Epoch 1499/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.1444 - acc: 0.9588 - val_loss: 0.6124 - val_acc: 0.7667\n",
      "Epoch 1500/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1656 - acc: 0.9554Epoch 01500: val_loss improved from 0.60421 to 0.60163, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 718us/step - loss: 0.1695 - acc: 0.9549 - val_loss: 0.6016 - val_acc: 0.7778\n",
      "Epoch 1501/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.1765 - acc: 0.9451 - val_loss: 0.6000 - val_acc: 0.7778\n",
      "Epoch 1502/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.1627 - acc: 0.9588 - val_loss: 0.6057 - val_acc: 0.7667\n",
      "Epoch 1503/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.1856 - acc: 0.9373 - val_loss: 0.6151 - val_acc: 0.7778\n",
      "Epoch 1504/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.1630 - acc: 0.9549 - val_loss: 0.6058 - val_acc: 0.7667\n",
      "Epoch 1505/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1873 - acc: 0.9397Epoch 01505: val_loss improved from 0.60163 to 0.59910, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.1831 - acc: 0.9451 - val_loss: 0.5991 - val_acc: 0.7667\n",
      "Epoch 1506/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.1732 - acc: 0.9549 - val_loss: 0.6094 - val_acc: 0.7778\n",
      "Epoch 1507/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.1690 - acc: 0.9451 - val_loss: 0.6172 - val_acc: 0.7778\n",
      "Epoch 1508/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.1552 - acc: 0.9588 - val_loss: 0.6189 - val_acc: 0.7889\n",
      "Epoch 1509/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.1649 - acc: 0.9471 - val_loss: 0.6054 - val_acc: 0.7889\n",
      "Epoch 1510/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1719 - acc: 0.9554Epoch 01510: val_loss did not improve\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.1611 - acc: 0.9588 - val_loss: 0.6033 - val_acc: 0.7778\n",
      "Epoch 1511/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.1437 - acc: 0.9706 - val_loss: 0.6003 - val_acc: 0.7667\n",
      "Epoch 1512/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.1808 - acc: 0.9412 - val_loss: 0.5953 - val_acc: 0.7667\n",
      "Epoch 1513/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.1593 - acc: 0.9627 - val_loss: 0.5955 - val_acc: 0.8000\n",
      "Epoch 1514/3000\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.1585 - acc: 0.9588 - val_loss: 0.5974 - val_acc: 0.7889\n",
      "Epoch 1515/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1763 - acc: 0.9531Epoch 01515: val_loss improved from 0.59910 to 0.59821, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.1691 - acc: 0.9549 - val_loss: 0.5982 - val_acc: 0.7889\n",
      "Epoch 1516/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.1606 - acc: 0.9647 - val_loss: 0.6012 - val_acc: 0.7889\n",
      "Epoch 1517/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.1732 - acc: 0.9529 - val_loss: 0.6018 - val_acc: 0.8000\n",
      "Epoch 1518/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.1669 - acc: 0.9608 - val_loss: 0.6008 - val_acc: 0.7889\n",
      "Epoch 1519/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.1638 - acc: 0.9569 - val_loss: 0.6042 - val_acc: 0.7778\n",
      "Epoch 1520/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1776 - acc: 0.9375Epoch 01520: val_loss improved from 0.59821 to 0.59483, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.1737 - acc: 0.9412 - val_loss: 0.5948 - val_acc: 0.7556\n",
      "Epoch 1521/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.1519 - acc: 0.9608 - val_loss: 0.6003 - val_acc: 0.7667\n",
      "Epoch 1522/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.1660 - acc: 0.9510 - val_loss: 0.6053 - val_acc: 0.7778\n",
      "Epoch 1523/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.1555 - acc: 0.9608 - val_loss: 0.6138 - val_acc: 0.7556\n",
      "Epoch 1524/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.1601 - acc: 0.9588 - val_loss: 0.6228 - val_acc: 0.7556\n",
      "Epoch 1525/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1646 - acc: 0.9554Epoch 01525: val_loss did not improve\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.1637 - acc: 0.9569 - val_loss: 0.6154 - val_acc: 0.7667\n",
      "Epoch 1526/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.1694 - acc: 0.9490 - val_loss: 0.6299 - val_acc: 0.7556\n",
      "Epoch 1527/3000\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.1525 - acc: 0.9627 - val_loss: 0.6260 - val_acc: 0.7556\n",
      "Epoch 1528/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.1668 - acc: 0.9373 - val_loss: 0.6230 - val_acc: 0.7778\n",
      "Epoch 1529/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.1571 - acc: 0.9627 - val_loss: 0.6286 - val_acc: 0.7667\n",
      "Epoch 1530/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1466 - acc: 0.9621Epoch 01530: val_loss did not improve\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.1540 - acc: 0.9588 - val_loss: 0.6196 - val_acc: 0.7667\n",
      "Epoch 1531/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.1613 - acc: 0.9608 - val_loss: 0.6209 - val_acc: 0.7778\n",
      "Epoch 1532/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.1373 - acc: 0.9588 - val_loss: 0.6185 - val_acc: 0.7889\n",
      "Epoch 1533/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.1682 - acc: 0.9569 - val_loss: 0.6189 - val_acc: 0.7778\n",
      "Epoch 1534/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.1471 - acc: 0.9608 - val_loss: 0.6218 - val_acc: 0.7667\n",
      "Epoch 1535/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1675 - acc: 0.9665Epoch 01535: val_loss did not improve\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.1680 - acc: 0.9647 - val_loss: 0.6157 - val_acc: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1536/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.1602 - acc: 0.9510 - val_loss: 0.6172 - val_acc: 0.7778\n",
      "Epoch 1537/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.1472 - acc: 0.9647 - val_loss: 0.6163 - val_acc: 0.8000\n",
      "Epoch 1538/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.1365 - acc: 0.9725 - val_loss: 0.6321 - val_acc: 0.7667\n",
      "Epoch 1539/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.1424 - acc: 0.9627 - val_loss: 0.6278 - val_acc: 0.7667\n",
      "Epoch 1540/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1556 - acc: 0.9643Epoch 01540: val_loss did not improve\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.1678 - acc: 0.9569 - val_loss: 0.6264 - val_acc: 0.7778\n",
      "Epoch 1541/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.1685 - acc: 0.9549 - val_loss: 0.6136 - val_acc: 0.7889\n",
      "Epoch 1542/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.1587 - acc: 0.9588 - val_loss: 0.6102 - val_acc: 0.7889\n",
      "Epoch 1543/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.1476 - acc: 0.9608 - val_loss: 0.6167 - val_acc: 0.7778\n",
      "Epoch 1544/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.1607 - acc: 0.9569 - val_loss: 0.6256 - val_acc: 0.7667\n",
      "Epoch 1545/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1543 - acc: 0.9531Epoch 01545: val_loss did not improve\n",
      "510/510 [==============================] - 0s 656us/step - loss: 0.1533 - acc: 0.9529 - val_loss: 0.6147 - val_acc: 0.7778\n",
      "Epoch 1546/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.1565 - acc: 0.9667 - val_loss: 0.6130 - val_acc: 0.7778\n",
      "Epoch 1547/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.1469 - acc: 0.9725 - val_loss: 0.6178 - val_acc: 0.7889\n",
      "Epoch 1548/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.1695 - acc: 0.9510 - val_loss: 0.6197 - val_acc: 0.7778\n",
      "Epoch 1549/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.1572 - acc: 0.9588 - val_loss: 0.6313 - val_acc: 0.7889\n",
      "Epoch 1550/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1618 - acc: 0.9598Epoch 01550: val_loss did not improve\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.1568 - acc: 0.9608 - val_loss: 0.6218 - val_acc: 0.7667\n",
      "Epoch 1551/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.1568 - acc: 0.9569 - val_loss: 0.6221 - val_acc: 0.7778\n",
      "Epoch 1552/3000\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.1567 - acc: 0.9569 - val_loss: 0.6266 - val_acc: 0.7778\n",
      "Epoch 1553/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.1605 - acc: 0.9549 - val_loss: 0.6319 - val_acc: 0.7667\n",
      "Epoch 1554/3000\n",
      "510/510 [==============================] - 0s 555us/step - loss: 0.1536 - acc: 0.9549 - val_loss: 0.6310 - val_acc: 0.7667\n",
      "Epoch 1555/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1622 - acc: 0.9464Epoch 01555: val_loss did not improve\n",
      "510/510 [==============================] - 0s 649us/step - loss: 0.1695 - acc: 0.9431 - val_loss: 0.6369 - val_acc: 0.7778\n",
      "Epoch 1556/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.1834 - acc: 0.9333 - val_loss: 0.6332 - val_acc: 0.7889\n",
      "Epoch 1557/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.1779 - acc: 0.9392 - val_loss: 0.6302 - val_acc: 0.7667\n",
      "Epoch 1558/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.1521 - acc: 0.9627 - val_loss: 0.6329 - val_acc: 0.7778\n",
      "Epoch 1559/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.1644 - acc: 0.9529 - val_loss: 0.6288 - val_acc: 0.7778\n",
      "Epoch 1560/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1558 - acc: 0.9554Epoch 01560: val_loss did not improve\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.1643 - acc: 0.9569 - val_loss: 0.6231 - val_acc: 0.7778\n",
      "Epoch 1561/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.1598 - acc: 0.9627 - val_loss: 0.6224 - val_acc: 0.7889\n",
      "Epoch 1562/3000\n",
      "510/510 [==============================] - 0s 659us/step - loss: 0.1659 - acc: 0.9490 - val_loss: 0.6140 - val_acc: 0.8000\n",
      "Epoch 1563/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.1520 - acc: 0.9667 - val_loss: 0.6134 - val_acc: 0.8000\n",
      "Epoch 1564/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.1724 - acc: 0.9431 - val_loss: 0.6114 - val_acc: 0.7889\n",
      "Epoch 1565/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1559 - acc: 0.9621Epoch 01565: val_loss did not improve\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.1630 - acc: 0.9549 - val_loss: 0.6183 - val_acc: 0.7889\n",
      "Epoch 1566/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.1598 - acc: 0.9549 - val_loss: 0.6311 - val_acc: 0.7778\n",
      "Epoch 1567/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.1598 - acc: 0.9549 - val_loss: 0.6312 - val_acc: 0.7556\n",
      "Epoch 1568/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.1782 - acc: 0.9510 - val_loss: 0.6233 - val_acc: 0.7667\n",
      "Epoch 1569/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.1580 - acc: 0.9627 - val_loss: 0.6037 - val_acc: 0.7889\n",
      "Epoch 1570/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1660 - acc: 0.9576Epoch 01570: val_loss did not improve\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.1653 - acc: 0.9549 - val_loss: 0.5964 - val_acc: 0.7889\n",
      "Epoch 1571/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.1562 - acc: 0.9549 - val_loss: 0.5976 - val_acc: 0.7889\n",
      "Epoch 1572/3000\n",
      "510/510 [==============================] - 0s 625us/step - loss: 0.1525 - acc: 0.9647 - val_loss: 0.6093 - val_acc: 0.7889\n",
      "Epoch 1573/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.1501 - acc: 0.9588 - val_loss: 0.6112 - val_acc: 0.7889\n",
      "Epoch 1574/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.1678 - acc: 0.9490 - val_loss: 0.6156 - val_acc: 0.7778\n",
      "Epoch 1575/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1377 - acc: 0.9688Epoch 01575: val_loss did not improve\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.1423 - acc: 0.9686 - val_loss: 0.6144 - val_acc: 0.7778\n",
      "Epoch 1576/3000\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.1646 - acc: 0.9490 - val_loss: 0.6134 - val_acc: 0.7889\n",
      "Epoch 1577/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.1666 - acc: 0.9471 - val_loss: 0.6150 - val_acc: 0.7667\n",
      "Epoch 1578/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.1402 - acc: 0.9706 - val_loss: 0.6125 - val_acc: 0.7778\n",
      "Epoch 1579/3000\n",
      "510/510 [==============================] - 0s 650us/step - loss: 0.1645 - acc: 0.9608 - val_loss: 0.6131 - val_acc: 0.7889\n",
      "Epoch 1580/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1260 - acc: 0.9710Epoch 01580: val_loss did not improve\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.1367 - acc: 0.9667 - val_loss: 0.6289 - val_acc: 0.7889\n",
      "Epoch 1581/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.1508 - acc: 0.9608 - val_loss: 0.6171 - val_acc: 0.7889\n",
      "Epoch 1582/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.1734 - acc: 0.9627 - val_loss: 0.6259 - val_acc: 0.7889\n",
      "Epoch 1583/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.1478 - acc: 0.9569 - val_loss: 0.6272 - val_acc: 0.7778\n",
      "Epoch 1584/3000\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.1606 - acc: 0.9510 - val_loss: 0.6286 - val_acc: 0.7778\n",
      "Epoch 1585/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1383 - acc: 0.9754Epoch 01585: val_loss did not improve\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.1462 - acc: 0.9725 - val_loss: 0.6334 - val_acc: 0.7667\n",
      "Epoch 1586/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 662us/step - loss: 0.1386 - acc: 0.9588 - val_loss: 0.6347 - val_acc: 0.7667\n",
      "Epoch 1587/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.1579 - acc: 0.9706 - val_loss: 0.6229 - val_acc: 0.7667\n",
      "Epoch 1588/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.1405 - acc: 0.9647 - val_loss: 0.6194 - val_acc: 0.7556\n",
      "Epoch 1589/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.1589 - acc: 0.9667 - val_loss: 0.6086 - val_acc: 0.7889\n",
      "Epoch 1590/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1638 - acc: 0.9554Epoch 01590: val_loss did not improve\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.1623 - acc: 0.9529 - val_loss: 0.6046 - val_acc: 0.7889\n",
      "Epoch 1591/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.1555 - acc: 0.9588 - val_loss: 0.6116 - val_acc: 0.7889\n",
      "Epoch 1592/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.1489 - acc: 0.9569 - val_loss: 0.6147 - val_acc: 0.7889\n",
      "Epoch 1593/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.1530 - acc: 0.9588 - val_loss: 0.6109 - val_acc: 0.7778\n",
      "Epoch 1594/3000\n",
      "510/510 [==============================] - 0s 649us/step - loss: 0.1564 - acc: 0.9569 - val_loss: 0.6165 - val_acc: 0.7889\n",
      "Epoch 1595/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.1584 - acc: 0.9479Epoch 01595: val_loss did not improve\n",
      "510/510 [==============================] - 0s 652us/step - loss: 0.1524 - acc: 0.9608 - val_loss: 0.6134 - val_acc: 0.7889\n",
      "Epoch 1596/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.1525 - acc: 0.9529 - val_loss: 0.6210 - val_acc: 0.7889\n",
      "Epoch 1597/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.1637 - acc: 0.9569 - val_loss: 0.6326 - val_acc: 0.7889\n",
      "Epoch 1598/3000\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.1344 - acc: 0.9765 - val_loss: 0.6346 - val_acc: 0.7667\n",
      "Epoch 1599/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.1599 - acc: 0.9490 - val_loss: 0.6332 - val_acc: 0.7778\n",
      "Epoch 1600/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1559 - acc: 0.9643Epoch 01600: val_loss did not improve\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.1623 - acc: 0.9627 - val_loss: 0.6302 - val_acc: 0.7667\n",
      "Epoch 1601/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.1550 - acc: 0.9569 - val_loss: 0.6222 - val_acc: 0.7778\n",
      "Epoch 1602/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.1522 - acc: 0.9569 - val_loss: 0.6156 - val_acc: 0.7889\n",
      "Epoch 1603/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.1584 - acc: 0.9569 - val_loss: 0.6240 - val_acc: 0.7889\n",
      "Epoch 1604/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 0.1554 - acc: 0.9569 - val_loss: 0.6202 - val_acc: 0.7889\n",
      "Epoch 1605/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1456 - acc: 0.9688Epoch 01605: val_loss did not improve\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.1519 - acc: 0.9608 - val_loss: 0.6159 - val_acc: 0.7889\n",
      "Epoch 1606/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.1561 - acc: 0.9431 - val_loss: 0.6103 - val_acc: 0.7889\n",
      "Epoch 1607/3000\n",
      "510/510 [==============================] - 0s 672us/step - loss: 0.1613 - acc: 0.9529 - val_loss: 0.6148 - val_acc: 0.7778\n",
      "Epoch 1608/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.1678 - acc: 0.9471 - val_loss: 0.6163 - val_acc: 0.7889\n",
      "Epoch 1609/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.1595 - acc: 0.9588 - val_loss: 0.6072 - val_acc: 0.8000\n",
      "Epoch 1610/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1540 - acc: 0.9576Epoch 01610: val_loss did not improve\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.1574 - acc: 0.9569 - val_loss: 0.6196 - val_acc: 0.7778\n",
      "Epoch 1611/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.1711 - acc: 0.9510 - val_loss: 0.6121 - val_acc: 0.7889\n",
      "Epoch 1612/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.1508 - acc: 0.9608 - val_loss: 0.6118 - val_acc: 0.7889\n",
      "Epoch 1613/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.1497 - acc: 0.9608 - val_loss: 0.6112 - val_acc: 0.7778\n",
      "Epoch 1614/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.1314 - acc: 0.9667 - val_loss: 0.6088 - val_acc: 0.8000\n",
      "Epoch 1615/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1445 - acc: 0.9665Epoch 01615: val_loss did not improve\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.1428 - acc: 0.9667 - val_loss: 0.6141 - val_acc: 0.7889\n",
      "Epoch 1616/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.1559 - acc: 0.9569 - val_loss: 0.6224 - val_acc: 0.7889\n",
      "Epoch 1617/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.1565 - acc: 0.9588 - val_loss: 0.6229 - val_acc: 0.7889\n",
      "Epoch 1618/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.1583 - acc: 0.9569 - val_loss: 0.6259 - val_acc: 0.7889\n",
      "Epoch 1619/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.1454 - acc: 0.9549 - val_loss: 0.6223 - val_acc: 0.7889\n",
      "Epoch 1620/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.1570 - acc: 0.9643Epoch 01620: val_loss did not improve\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.1533 - acc: 0.9647 - val_loss: 0.6217 - val_acc: 0.7778\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(patience=100, mode='auto', monitor='val_loss')\n",
    "checkPoint = ModelCheckpoint(filepath=\"top_weight.h5\", verbose=1, save_best_only=True, save_weights_only=True, period=5)\n",
    "Logs = CSVLogger('logs.csv', separator=',', append=True)\n",
    "history = model.fit(X_train, y_train, epochs=3000, batch_size=64, shuffle=True, validation_split=0.15, callbacks=[checkPoint, Logs, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFCRJA4OCJGlgUJAkDf4FKDnRQ03Atx8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc538fa7050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKSillHL7fzozgwjPx8LEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc537553310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting 了 QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 408us/step\n",
      "loss: 0.62\n",
      "acc: 82.31%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print 'loss: %.2f'%(loss)\n",
    "print 'acc: %.2f%%'%(acc*100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  4  5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  9  1  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  9  2  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  2 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  6  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  8  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2 13  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1 11  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  2 11]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93        15\n",
      "          1       0.80      0.40      0.53        10\n",
      "          2       0.62      1.00      0.76         8\n",
      "          3       1.00      0.82      0.90        11\n",
      "          4       0.75      0.82      0.78        11\n",
      "          5       0.83      0.83      0.83        12\n",
      "          6       0.86      0.75      0.80         8\n",
      "          7       0.67      0.80      0.73        10\n",
      "          8       0.87      0.87      0.87        15\n",
      "          9       0.80      1.00      0.89         4\n",
      "         10       0.85      0.85      0.85        13\n",
      "         11       0.92      0.85      0.88        13\n",
      "\n",
      "avg / total       0.84      0.82      0.82       130\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD8CAYAAAD5TVjyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc538295990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "real = np.argmax(y_test, axis=1)\n",
    "confuse = np.asarray(confusion_matrix(real,pred))\n",
    "print confuse\n",
    "print classification_report(real,pred)\n",
    "confuse = confuse / float(confuse.sum())\n",
    "plt.imshow(confuse, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
