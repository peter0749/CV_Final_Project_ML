{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDNN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loadmat('./dataset.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__globals__', 'test_data', 'test_label', '__version__', 'train_label', 'train_data']\n"
     ]
    }
   ],
   "source": [
    "print list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 林敬翔 之銘言：\n",
    "跟助教要的 dataset\n",
    "\n",
    "第一維是數量 第二維是時間序 第三維是資料量\n",
    "\n",
    "Label 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ....\n",
    "\n",
    "1-10左手腕 11-20右手腕 21-30左手臂 31-40右手臂\n",
    "\n",
    "220代表frame數\n",
    "\n",
    "我是取一秒25frame\n",
    "\n",
    "動作數量是以動作為單位沒錯 1*220*40表是某一個動作的完整資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下資料維度\n",
    "\n",
    "data: (?, 220, 40) = (幾筆資料, 時間步, 資料維度)\n",
    "\n",
    "label: (?, 1) = (幾筆資料, 動作label)\n",
    "\n",
    "動作 label: 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: __header__\n",
      "not an array\n",
      "key: __globals__\n",
      "not an array\n",
      "key: test_data\n",
      "(130, 220, 40)\n",
      "key: test_label\n",
      "(130, 1)\n",
      "key: __version__\n",
      "not an array\n",
      "key: train_label\n",
      "(600, 1)\n",
      "key: train_data\n",
      "(600, 220, 40)\n"
     ]
    }
   ],
   "source": [
    "for key, value in data.iteritems():\n",
    "    print 'key: %s'%key\n",
    "    if hasattr(value , 'shape'):\n",
    "        print value.shape\n",
    "    else:\n",
    "        print 'not an array'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列資料 + 預測 -> RNN? / HMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Keras CuDNNLSTM\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "import keras\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Flatten, Conv1D, MaxPool1D\n",
    "from keras.layers import LSTM, CuDNNLSTM, RepeatVector, TimeDistributed, Bidirectional\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.utils import to_categorical # one-hot encoding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, actions=40, cell_size=128, lstm_layer_n = 3, conv_filters=64, \n",
    "                conv_kernel=7, pooling_step=2, learning_rate=0.01, dropout_r=0.14, \n",
    "                optimizer=keras.optimizers.RMSprop, clipnorm=1., \n",
    "                tLSTM=LSTM, use_temporal_subsampling=False):\n",
    "    if lstm_layer_n<1: raise ValueError('lstm_layer_n must >= 1')\n",
    "    optimizer = optimizer(lr=learning_rate, clipnorm=clipnorm)\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0, input_shape=input_shape))\n",
    "    if use_temporal_subsampling:\n",
    "        model.add(Conv1D(filters=conv_filters, kernel_size=conv_kernel, padding='same', activation='relu'))\n",
    "        model.add(MaxPool1D(pool_size=pooling_step))\n",
    "        model.add(Dropout(dropout_r))\n",
    "    for _ in xrange(1, lstm_layer_n):\n",
    "        model.add(#Bidirectional(\n",
    "                                tLSTM(cell_size, return_sequences=True,\n",
    "                                      unit_forget_bias=True, recurrent_regularizer=regularizers.l2(0.001))#,\n",
    "                                #merge_mode='sum'\n",
    "                               )#)\n",
    "        model.add(Dropout(dropout_r))\n",
    "    model.add(#Bidirectional(\n",
    "                            tLSTM(cell_size, return_sequences=False, unit_forget_bias=True, \n",
    "                                  recurrent_regularizer=regularizers.l2(0.001))#,\n",
    "                            #merge_mode='sum'\n",
    "                           )#)\n",
    "    model.add(Dropout(dropout_r))\n",
    "    model.add(Dense(actions, activation='softmax'))\n",
    "    model.compile(\n",
    "            loss = 'categorical_crossentropy',\n",
    "            optimizer=optimizer, metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info for 0 pca: 1.00\n",
      "info for 1 pca: 1.00\n",
      "info for 2 pca: 1.00\n",
      "info for 3 pca: 1.00\n",
      "info for 4 pca: 1.00\n",
      "info for 5 pca: 1.00\n",
      "info for 6 pca: 1.00\n",
      "info for 7 pca: 1.00\n",
      "info for 8 pca: 1.00\n",
      "info for 9 pca: 1.00\n",
      "info for 10 pca: 1.00\n",
      "info for 11 pca: 1.00\n",
      "info for 12 pca: 1.00\n",
      "info for 13 pca: 1.00\n",
      "info for 14 pca: 1.00\n",
      "info for 15 pca: 1.00\n",
      "info for 16 pca: 1.00\n",
      "info for 17 pca: 1.00\n",
      "info for 18 pca: 1.00\n",
      "info for 19 pca: 1.00\n",
      "info for 20 pca: 1.00\n",
      "info for 21 pca: 1.00\n",
      "info for 22 pca: 1.00\n",
      "info for 23 pca: 1.00\n",
      "info for 24 pca: 1.00\n",
      "info for 25 pca: 1.00\n",
      "info for 26 pca: 1.00\n",
      "info for 27 pca: 1.00\n",
      "info for 28 pca: 1.00\n",
      "info for 29 pca: 1.00\n",
      "info for 30 pca: 1.00\n",
      "info for 31 pca: 1.00\n",
      "info for 32 pca: 1.00\n",
      "info for 33 pca: 1.00\n",
      "info for 34 pca: 1.00\n",
      "info for 35 pca: 1.00\n",
      "info for 36 pca: 1.00\n",
      "info for 37 pca: 1.00\n",
      "info for 38 pca: 1.00\n",
      "info for 39 pca: 1.00\n",
      "info for 40 pca: 1.00\n",
      "info for 41 pca: 1.00\n",
      "info for 42 pca: 1.00\n",
      "info for 43 pca: 1.00\n",
      "info for 44 pca: 1.00\n",
      "info for 45 pca: 1.00\n",
      "info for 46 pca: 1.00\n",
      "info for 47 pca: 1.00\n",
      "info for 48 pca: 1.00\n",
      "info for 49 pca: 1.00\n",
      "info for 50 pca: 1.00\n",
      "info for 51 pca: 1.00\n",
      "info for 52 pca: 1.00\n",
      "info for 53 pca: 1.00\n",
      "info for 54 pca: 1.00\n",
      "info for 55 pca: 1.00\n",
      "info for 56 pca: 1.00\n",
      "info for 57 pca: 1.00\n",
      "info for 58 pca: 1.00\n",
      "info for 59 pca: 1.00\n",
      "info for 60 pca: 1.00\n",
      "info for 61 pca: 1.00\n",
      "info for 62 pca: 1.00\n",
      "info for 63 pca: 1.00\n",
      "info for 64 pca: 1.00\n",
      "info for 65 pca: 1.00\n",
      "info for 66 pca: 1.00\n",
      "info for 67 pca: 1.00\n",
      "info for 68 pca: 1.00\n",
      "info for 69 pca: 1.00\n",
      "info for 70 pca: 1.00\n",
      "info for 71 pca: 1.00\n",
      "info for 72 pca: 1.00\n",
      "info for 73 pca: 1.00\n",
      "info for 74 pca: 1.00\n",
      "info for 75 pca: 1.00\n",
      "info for 76 pca: 1.00\n",
      "info for 77 pca: 1.00\n",
      "info for 78 pca: 1.00\n",
      "info for 79 pca: 1.00\n",
      "info for 80 pca: 1.00\n",
      "info for 81 pca: 1.00\n",
      "info for 82 pca: 1.00\n",
      "info for 83 pca: 1.00\n",
      "info for 84 pca: 1.00\n",
      "info for 85 pca: 1.00\n",
      "info for 86 pca: 1.00\n",
      "info for 87 pca: 1.00\n",
      "info for 88 pca: 1.00\n",
      "info for 89 pca: 1.00\n",
      "info for 90 pca: 1.00\n",
      "info for 91 pca: 1.00\n",
      "info for 92 pca: 1.00\n",
      "info for 93 pca: 1.00\n",
      "info for 94 pca: 1.00\n",
      "info for 95 pca: 1.00\n",
      "info for 96 pca: 1.00\n",
      "info for 97 pca: 1.00\n",
      "info for 98 pca: 1.00\n",
      "info for 99 pca: 1.00\n",
      "info for 100 pca: 1.00\n",
      "info for 101 pca: 1.00\n",
      "info for 102 pca: 1.00\n",
      "info for 103 pca: 1.00\n",
      "info for 104 pca: 1.00\n",
      "info for 105 pca: 1.00\n",
      "info for 106 pca: 1.00\n",
      "info for 107 pca: 1.00\n",
      "info for 108 pca: 1.00\n",
      "info for 109 pca: 1.00\n",
      "info for 110 pca: 1.00\n",
      "info for 111 pca: 1.00\n",
      "info for 112 pca: 1.00\n",
      "info for 113 pca: 1.00\n",
      "info for 114 pca: 1.00\n",
      "info for 115 pca: 1.00\n",
      "info for 116 pca: 1.00\n",
      "info for 117 pca: 1.00\n",
      "info for 118 pca: 1.00\n",
      "info for 119 pca: 1.00\n",
      "info for 120 pca: 1.00\n",
      "info for 121 pca: 1.00\n",
      "info for 122 pca: 1.00\n",
      "info for 123 pca: 1.00\n",
      "info for 124 pca: 1.00\n",
      "info for 125 pca: 1.00\n",
      "info for 126 pca: 1.00\n",
      "info for 127 pca: 1.00\n",
      "info for 128 pca: 1.00\n",
      "info for 129 pca: 1.00\n",
      "info for 130 pca: 1.00\n",
      "info for 131 pca: 1.00\n",
      "info for 132 pca: 1.00\n",
      "info for 133 pca: 1.00\n",
      "info for 134 pca: 1.00\n",
      "info for 135 pca: 1.00\n",
      "info for 136 pca: 1.00\n",
      "info for 137 pca: 1.00\n",
      "info for 138 pca: 1.00\n",
      "info for 139 pca: 1.00\n",
      "info for 140 pca: 1.00\n",
      "info for 141 pca: 1.00\n",
      "info for 142 pca: 1.00\n",
      "info for 143 pca: 1.00\n",
      "info for 144 pca: 1.00\n",
      "info for 145 pca: 1.00\n",
      "info for 146 pca: 1.00\n",
      "info for 147 pca: 1.00\n",
      "info for 148 pca: 1.00\n",
      "info for 149 pca: 1.00\n",
      "info for 150 pca: 1.00\n",
      "info for 151 pca: 1.00\n",
      "info for 152 pca: 1.00\n",
      "info for 153 pca: 1.00\n",
      "info for 154 pca: 1.00\n",
      "info for 155 pca: 1.00\n",
      "info for 156 pca: 1.00\n",
      "info for 157 pca: 1.00\n",
      "info for 158 pca: 1.00\n",
      "info for 159 pca: 1.00\n",
      "info for 160 pca: 1.00\n",
      "info for 161 pca: 1.00\n",
      "info for 162 pca: 1.00\n",
      "info for 163 pca: 1.00\n",
      "info for 164 pca: 1.00\n",
      "info for 165 pca: 1.00\n",
      "info for 166 pca: 1.00\n",
      "info for 167 pca: 1.00\n",
      "info for 168 pca: 1.00\n",
      "info for 169 pca: 1.00\n",
      "info for 170 pca: 1.00\n",
      "info for 171 pca: 1.00\n",
      "info for 172 pca: 1.00\n",
      "info for 173 pca: 1.00\n",
      "info for 174 pca: 1.00\n",
      "info for 175 pca: 1.00\n",
      "info for 176 pca: 1.00\n",
      "info for 177 pca: 1.00\n",
      "info for 178 pca: 1.00\n",
      "info for 179 pca: 1.00\n",
      "info for 180 pca: 1.00\n",
      "info for 181 pca: 1.00\n",
      "info for 182 pca: 1.00\n",
      "info for 183 pca: 1.00\n",
      "info for 184 pca: 1.00\n",
      "info for 185 pca: 1.00\n",
      "info for 186 pca: 1.00\n",
      "info for 187 pca: 1.00\n",
      "info for 188 pca: 1.00\n",
      "info for 189 pca: 1.00\n",
      "info for 190 pca: 1.00\n",
      "info for 191 pca: 1.00\n",
      "info for 192 pca: 1.00\n",
      "info for 193 pca: 1.00\n",
      "info for 194 pca: 1.00\n",
      "info for 195 pca: 1.00\n",
      "info for 196 pca: 1.00\n",
      "info for 197 pca: 1.00\n",
      "info for 198 pca: 1.00\n",
      "info for 199 pca: 1.00\n",
      "info for 200 pca: 1.00\n",
      "info for 201 pca: 1.00\n",
      "info for 202 pca: 1.00\n",
      "info for 203 pca: 1.00\n",
      "info for 204 pca: 1.00\n",
      "info for 205 pca: 1.00\n",
      "info for 206 pca: 1.00\n",
      "info for 207 pca: 1.00\n",
      "info for 208 pca: 1.00\n",
      "info for 209 pca: 1.00\n",
      "info for 210 pca: 1.00\n",
      "info for 211 pca: 1.00\n",
      "info for 212 pca: 1.00\n",
      "info for 213 pca: 1.00\n",
      "info for 214 pca: 1.00\n",
      "info for 215 pca: 1.00\n",
      "info for 216 pca: 1.00\n",
      "info for 217 pca: 1.00\n",
      "info for 218 pca: 1.00\n",
      "info for 219 pca: 1.00\n",
      "(600, 220, 12)\n",
      "(130, 220, 12)\n",
      "(600, 12)\n",
      "(130, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = data['train_data'], data['test_data'], data['train_label'], data['test_label']\n",
    "label_enc = OHE(sparse=False) # One-hot encoder, which has attribute [transform, inverse_transform]\n",
    "y_test = label_enc.fit_transform(y_test)\n",
    "y_train = label_enc.transform(y_train)\n",
    "\n",
    "pcas = []\n",
    "n_comp = 12\n",
    "X_train_new = np.zeros((X_train.shape[0], X_train.shape[1], n_comp))\n",
    "X_test_new = np.zeros((X_test.shape[0], X_test.shape[1], n_comp))\n",
    "for i in xrange(X_train.shape[-2]):\n",
    "    pca = PCA(n_components=n_comp).fit(X_train[:,i,:])\n",
    "    pcas.append( (pca, np.sum(pca.explained_variance_ratio_)) )\n",
    "    X_train_new[:,i,:] = pcas[-1][0].transform(X_train[:,i,:])\n",
    "    X_test_new[:,i,:] = pcas[-1][0].transform(X_test[:,i,:])\n",
    "\n",
    "X_train = X_train_new\n",
    "X_test = X_test_new\n",
    "del X_train_new\n",
    "del X_test_new\n",
    "\n",
    "for i, pca in enumerate(pcas):\n",
    "    print 'info for %d pca: %.2f'%(i,pca[1])\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_1 (Dropout)          (None, 220, 12)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 100)               45600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 46,812\n",
      "Trainable params: 46,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(input_shape=X_test.shape[1:] ,actions=y_test.shape[-1], cell_size=100, lstm_layer_n=1,\n",
    "                    learning_rate=0.0001, dropout_r=0.5,\n",
    "                    tLSTM=CuDNNLSTM if USE_CUDNN else LSTM, use_temporal_subsampling=False\n",
    "                   )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 510 samples, validate on 90 samples\n",
      "Epoch 1/3000\n",
      "510/510 [==============================] - 4s 7ms/step - loss: 2.9748 - acc: 0.0863 - val_loss: 2.6915 - val_acc: 0.1333\n",
      "Epoch 2/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 2.8675 - acc: 0.1255 - val_loss: 2.6510 - val_acc: 0.1333\n",
      "Epoch 3/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 2.8540 - acc: 0.1333 - val_loss: 2.6206 - val_acc: 0.1444\n",
      "Epoch 4/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.7833 - acc: 0.1314 - val_loss: 2.5862 - val_acc: 0.1667\n",
      "Epoch 5/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.7417 - acc: 0.1138Epoch 00005: val_loss improved from inf to 2.54791, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 2.7362 - acc: 0.1078 - val_loss: 2.5479 - val_acc: 0.2000\n",
      "Epoch 6/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 2.7100 - acc: 0.1255 - val_loss: 2.5176 - val_acc: 0.2111\n",
      "Epoch 7/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.6611 - acc: 0.1392 - val_loss: 2.4848 - val_acc: 0.2222\n",
      "Epoch 8/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 2.6023 - acc: 0.1569 - val_loss: 2.4611 - val_acc: 0.2333\n",
      "Epoch 9/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 2.5522 - acc: 0.1667 - val_loss: 2.4276 - val_acc: 0.2556\n",
      "Epoch 10/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.5352 - acc: 0.1920Epoch 00010: val_loss improved from 2.54791 to 2.39968, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 2.5360 - acc: 0.1922 - val_loss: 2.3997 - val_acc: 0.2556\n",
      "Epoch 11/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.4772 - acc: 0.1843 - val_loss: 2.3731 - val_acc: 0.2444\n",
      "Epoch 12/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.4809 - acc: 0.1804 - val_loss: 2.3496 - val_acc: 0.2556\n",
      "Epoch 13/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 2.4109 - acc: 0.2176 - val_loss: 2.3273 - val_acc: 0.2556\n",
      "Epoch 14/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.3608 - acc: 0.2235 - val_loss: 2.3020 - val_acc: 0.2667\n",
      "Epoch 15/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.4399 - acc: 0.1942Epoch 00015: val_loss improved from 2.39968 to 2.27392, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 2.4306 - acc: 0.1941 - val_loss: 2.2739 - val_acc: 0.2778\n",
      "Epoch 16/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 2.3451 - acc: 0.2451 - val_loss: 2.2438 - val_acc: 0.2778\n",
      "Epoch 17/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 2.2580 - acc: 0.2627 - val_loss: 2.2155 - val_acc: 0.2778\n",
      "Epoch 18/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.3093 - acc: 0.2490 - val_loss: 2.1879 - val_acc: 0.2778\n",
      "Epoch 19/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.2524 - acc: 0.2627 - val_loss: 2.1673 - val_acc: 0.2889\n",
      "Epoch 20/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.2219 - acc: 0.2812Epoch 00020: val_loss improved from 2.27392 to 2.14286, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 2.2263 - acc: 0.2745 - val_loss: 2.1429 - val_acc: 0.2889\n",
      "Epoch 21/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 2.1561 - acc: 0.2922 - val_loss: 2.1168 - val_acc: 0.3111\n",
      "Epoch 22/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.1710 - acc: 0.2529 - val_loss: 2.0967 - val_acc: 0.3111\n",
      "Epoch 23/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 2.1281 - acc: 0.2843 - val_loss: 2.0764 - val_acc: 0.3111\n",
      "Epoch 24/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 2.1302 - acc: 0.2922 - val_loss: 2.0550 - val_acc: 0.3222\n",
      "Epoch 25/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.0963 - acc: 0.3080Epoch 00025: val_loss improved from 2.14286 to 2.03475, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.0890 - acc: 0.3098 - val_loss: 2.0348 - val_acc: 0.3444\n",
      "Epoch 26/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.0376 - acc: 0.3216 - val_loss: 2.0159 - val_acc: 0.3444\n",
      "Epoch 27/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.9924 - acc: 0.3294 - val_loss: 1.9990 - val_acc: 0.3556\n",
      "Epoch 28/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 1.9830 - acc: 0.3333 - val_loss: 1.9780 - val_acc: 0.3556\n",
      "Epoch 29/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.9886 - acc: 0.3490 - val_loss: 1.9638 - val_acc: 0.3778\n",
      "Epoch 30/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.9704 - acc: 0.3683Epoch 00030: val_loss improved from 2.03475 to 1.94674, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.9805 - acc: 0.3647 - val_loss: 1.9467 - val_acc: 0.3778\n",
      "Epoch 31/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.9551 - acc: 0.3412 - val_loss: 1.9236 - val_acc: 0.3889\n",
      "Epoch 32/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.8903 - acc: 0.3647 - val_loss: 1.9095 - val_acc: 0.3889\n",
      "Epoch 33/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.8928 - acc: 0.3725 - val_loss: 1.8975 - val_acc: 0.4000\n",
      "Epoch 34/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.8610 - acc: 0.3784 - val_loss: 1.8858 - val_acc: 0.4000\n",
      "Epoch 35/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.8255 - acc: 0.3996Epoch 00035: val_loss improved from 1.94674 to 1.87109, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.8351 - acc: 0.4020 - val_loss: 1.8711 - val_acc: 0.4000\n",
      "Epoch 36/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.8741 - acc: 0.3725 - val_loss: 1.8543 - val_acc: 0.4000\n",
      "Epoch 37/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.8449 - acc: 0.3980 - val_loss: 1.8438 - val_acc: 0.3889\n",
      "Epoch 38/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.8021 - acc: 0.4098 - val_loss: 1.8310 - val_acc: 0.3778\n",
      "Epoch 39/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.7818 - acc: 0.4137 - val_loss: 1.8191 - val_acc: 0.3778\n",
      "Epoch 40/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.8390 - acc: 0.3906Epoch 00040: val_loss improved from 1.87109 to 1.80431, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.8204 - acc: 0.3961 - val_loss: 1.8043 - val_acc: 0.3778\n",
      "Epoch 41/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.7403 - acc: 0.4196 - val_loss: 1.7934 - val_acc: 0.3778\n",
      "Epoch 42/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.7527 - acc: 0.4412 - val_loss: 1.7809 - val_acc: 0.3778\n",
      "Epoch 43/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.7429 - acc: 0.4039 - val_loss: 1.7680 - val_acc: 0.3778\n",
      "Epoch 44/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.6865 - acc: 0.3941 - val_loss: 1.7544 - val_acc: 0.3667\n",
      "Epoch 45/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.7382 - acc: 0.4397Epoch 00045: val_loss improved from 1.80431 to 1.73910, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.7223 - acc: 0.4412 - val_loss: 1.7391 - val_acc: 0.3778\n",
      "Epoch 46/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.6729 - acc: 0.4686 - val_loss: 1.7313 - val_acc: 0.3889\n",
      "Epoch 47/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.7120 - acc: 0.4353 - val_loss: 1.7241 - val_acc: 0.3778\n",
      "Epoch 48/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.6881 - acc: 0.4255 - val_loss: 1.7070 - val_acc: 0.3889\n",
      "Epoch 49/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.6464 - acc: 0.4647 - val_loss: 1.6978 - val_acc: 0.3889\n",
      "Epoch 50/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/510 [=========================>....] - ETA: 0s - loss: 1.6420 - acc: 0.4464Epoch 00050: val_loss improved from 1.73910 to 1.68950, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.6350 - acc: 0.4549 - val_loss: 1.6895 - val_acc: 0.3889\n",
      "Epoch 51/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.6785 - acc: 0.4216 - val_loss: 1.6786 - val_acc: 0.3889\n",
      "Epoch 52/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 1.6371 - acc: 0.4784 - val_loss: 1.6692 - val_acc: 0.4000\n",
      "Epoch 53/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.6608 - acc: 0.4216 - val_loss: 1.6555 - val_acc: 0.4000\n",
      "Epoch 54/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.5792 - acc: 0.4804 - val_loss: 1.6480 - val_acc: 0.4111\n",
      "Epoch 55/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.6493 - acc: 0.4196Epoch 00055: val_loss improved from 1.68950 to 1.63720, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.6423 - acc: 0.4235 - val_loss: 1.6372 - val_acc: 0.4111\n",
      "Epoch 56/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.5888 - acc: 0.4980 - val_loss: 1.6277 - val_acc: 0.4111\n",
      "Epoch 57/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.5908 - acc: 0.4392 - val_loss: 1.6196 - val_acc: 0.4222\n",
      "Epoch 58/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.5557 - acc: 0.5000 - val_loss: 1.6158 - val_acc: 0.4111\n",
      "Epoch 59/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.5363 - acc: 0.4784 - val_loss: 1.6074 - val_acc: 0.4000\n",
      "Epoch 60/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.5712 - acc: 0.4643Epoch 00060: val_loss improved from 1.63720 to 1.59889, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.5613 - acc: 0.4784 - val_loss: 1.5989 - val_acc: 0.4111\n",
      "Epoch 61/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.5476 - acc: 0.4922 - val_loss: 1.5903 - val_acc: 0.4111\n",
      "Epoch 62/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.5305 - acc: 0.5098 - val_loss: 1.5857 - val_acc: 0.4111\n",
      "Epoch 63/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.4865 - acc: 0.5196 - val_loss: 1.5770 - val_acc: 0.4222\n",
      "Epoch 64/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.4941 - acc: 0.5000 - val_loss: 1.5691 - val_acc: 0.4222\n",
      "Epoch 65/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.5077 - acc: 0.4955Epoch 00065: val_loss improved from 1.59889 to 1.55998, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 1.4951 - acc: 0.5078 - val_loss: 1.5600 - val_acc: 0.4111\n",
      "Epoch 66/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.4494 - acc: 0.5216 - val_loss: 1.5553 - val_acc: 0.4111\n",
      "Epoch 67/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.4195 - acc: 0.5275 - val_loss: 1.5484 - val_acc: 0.4111\n",
      "Epoch 68/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.4776 - acc: 0.4980 - val_loss: 1.5348 - val_acc: 0.4111\n",
      "Epoch 69/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.4530 - acc: 0.5039 - val_loss: 1.5308 - val_acc: 0.4111\n",
      "Epoch 70/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.4448 - acc: 0.5357Epoch 00070: val_loss improved from 1.55998 to 1.52032, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.4414 - acc: 0.5314 - val_loss: 1.5203 - val_acc: 0.4111\n",
      "Epoch 71/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.4064 - acc: 0.5196 - val_loss: 1.5116 - val_acc: 0.4000\n",
      "Epoch 72/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 1.4507 - acc: 0.5118 - val_loss: 1.5078 - val_acc: 0.4111\n",
      "Epoch 73/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.4356 - acc: 0.5373 - val_loss: 1.5007 - val_acc: 0.4000\n",
      "Epoch 74/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.4666 - acc: 0.4980 - val_loss: 1.4917 - val_acc: 0.4000\n",
      "Epoch 75/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.3777 - acc: 0.5603Epoch 00075: val_loss improved from 1.52032 to 1.48607, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.3637 - acc: 0.5667 - val_loss: 1.4861 - val_acc: 0.4111\n",
      "Epoch 76/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.4369 - acc: 0.5176 - val_loss: 1.4769 - val_acc: 0.4111\n",
      "Epoch 77/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.3696 - acc: 0.5529 - val_loss: 1.4712 - val_acc: 0.4556\n",
      "Epoch 78/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.3563 - acc: 0.5824 - val_loss: 1.4669 - val_acc: 0.4556\n",
      "Epoch 79/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.3695 - acc: 0.5137 - val_loss: 1.4606 - val_acc: 0.4556\n",
      "Epoch 80/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.3751 - acc: 0.5491Epoch 00080: val_loss improved from 1.48607 to 1.45155, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.3643 - acc: 0.5529 - val_loss: 1.4515 - val_acc: 0.4556\n",
      "Epoch 81/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.3470 - acc: 0.5510 - val_loss: 1.4428 - val_acc: 0.4667\n",
      "Epoch 82/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 1.3904 - acc: 0.5373 - val_loss: 1.4348 - val_acc: 0.4667\n",
      "Epoch 83/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.3214 - acc: 0.5608 - val_loss: 1.4312 - val_acc: 0.4556\n",
      "Epoch 84/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.3214 - acc: 0.5667 - val_loss: 1.4273 - val_acc: 0.4667\n",
      "Epoch 85/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.3291 - acc: 0.5446Epoch 00085: val_loss improved from 1.45155 to 1.41998, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.3312 - acc: 0.5412 - val_loss: 1.4200 - val_acc: 0.4667\n",
      "Epoch 86/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.3383 - acc: 0.5333 - val_loss: 1.4147 - val_acc: 0.4667\n",
      "Epoch 87/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 1.3274 - acc: 0.5529 - val_loss: 1.4128 - val_acc: 0.4667\n",
      "Epoch 88/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.3220 - acc: 0.5824 - val_loss: 1.4064 - val_acc: 0.5000\n",
      "Epoch 89/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.3473 - acc: 0.5412 - val_loss: 1.4011 - val_acc: 0.4889\n",
      "Epoch 90/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2945 - acc: 0.5558Epoch 00090: val_loss improved from 1.41998 to 1.39595, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2936 - acc: 0.5529 - val_loss: 1.3959 - val_acc: 0.5000\n",
      "Epoch 91/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2706 - acc: 0.5863 - val_loss: 1.3898 - val_acc: 0.5111\n",
      "Epoch 92/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.2994 - acc: 0.5392 - val_loss: 1.3843 - val_acc: 0.5000\n",
      "Epoch 93/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.3192 - acc: 0.5314 - val_loss: 1.3840 - val_acc: 0.5111\n",
      "Epoch 94/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2786 - acc: 0.5980 - val_loss: 1.3818 - val_acc: 0.5111\n",
      "Epoch 95/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2548 - acc: 0.5603Epoch 00095: val_loss improved from 1.39595 to 1.37129, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.2763 - acc: 0.5471 - val_loss: 1.3713 - val_acc: 0.5111\n",
      "Epoch 96/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2958 - acc: 0.5725 - val_loss: 1.3631 - val_acc: 0.5222\n",
      "Epoch 97/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.2367 - acc: 0.5686 - val_loss: 1.3608 - val_acc: 0.5111\n",
      "Epoch 98/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2924 - acc: 0.5765 - val_loss: 1.3561 - val_acc: 0.5222\n",
      "Epoch 99/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.2494 - acc: 0.5745 - val_loss: 1.3511 - val_acc: 0.5222\n",
      "Epoch 100/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2402 - acc: 0.5536Epoch 00100: val_loss improved from 1.37129 to 1.34811, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2442 - acc: 0.5608 - val_loss: 1.3481 - val_acc: 0.5222\n",
      "Epoch 101/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2180 - acc: 0.5941 - val_loss: 1.3453 - val_acc: 0.4889\n",
      "Epoch 102/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.3004 - acc: 0.5373 - val_loss: 1.3399 - val_acc: 0.4889\n",
      "Epoch 103/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.2172 - acc: 0.5843 - val_loss: 1.3351 - val_acc: 0.5000\n",
      "Epoch 104/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2236 - acc: 0.6039 - val_loss: 1.3313 - val_acc: 0.5000\n",
      "Epoch 105/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2188 - acc: 0.5625Epoch 00105: val_loss improved from 1.34811 to 1.32613, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.2233 - acc: 0.5647 - val_loss: 1.3261 - val_acc: 0.5000\n",
      "Epoch 106/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1878 - acc: 0.6216 - val_loss: 1.3240 - val_acc: 0.5000\n",
      "Epoch 107/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.2428 - acc: 0.5941 - val_loss: 1.3185 - val_acc: 0.5000\n",
      "Epoch 108/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2069 - acc: 0.5784 - val_loss: 1.3125 - val_acc: 0.5333\n",
      "Epoch 109/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 1.2062 - acc: 0.5843 - val_loss: 1.3099 - val_acc: 0.5222\n",
      "Epoch 110/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.1732 - acc: 0.6049Epoch 00110: val_loss improved from 1.32613 to 1.31159, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.1868 - acc: 0.5863 - val_loss: 1.3116 - val_acc: 0.5222\n",
      "Epoch 111/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1678 - acc: 0.5902 - val_loss: 1.3078 - val_acc: 0.5111\n",
      "Epoch 112/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.2252 - acc: 0.5804 - val_loss: 1.3045 - val_acc: 0.5222\n",
      "Epoch 113/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.2153 - acc: 0.5549 - val_loss: 1.2997 - val_acc: 0.5222\n",
      "Epoch 114/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1620 - acc: 0.6098 - val_loss: 1.2935 - val_acc: 0.5333\n",
      "Epoch 115/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.1988 - acc: 0.5737Epoch 00115: val_loss improved from 1.31159 to 1.29136, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.1938 - acc: 0.5745 - val_loss: 1.2914 - val_acc: 0.5333\n",
      "Epoch 116/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1654 - acc: 0.6157 - val_loss: 1.2869 - val_acc: 0.5444\n",
      "Epoch 117/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1518 - acc: 0.5980 - val_loss: 1.2838 - val_acc: 0.5333\n",
      "Epoch 118/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 1.1776 - acc: 0.5863 - val_loss: 1.2824 - val_acc: 0.5333\n",
      "Epoch 119/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1110 - acc: 0.6157 - val_loss: 1.2790 - val_acc: 0.5444\n",
      "Epoch 120/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.1235 - acc: 0.6295Epoch 00120: val_loss improved from 1.29136 to 1.27511, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1250 - acc: 0.6275 - val_loss: 1.2751 - val_acc: 0.5444\n",
      "Epoch 121/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.1488 - acc: 0.6157 - val_loss: 1.2728 - val_acc: 0.5444\n",
      "Epoch 122/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1165 - acc: 0.6157 - val_loss: 1.2701 - val_acc: 0.5556\n",
      "Epoch 123/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.0916 - acc: 0.6569 - val_loss: 1.2625 - val_acc: 0.5667\n",
      "Epoch 124/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1201 - acc: 0.6020 - val_loss: 1.2611 - val_acc: 0.5667\n",
      "Epoch 125/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.1385 - acc: 0.6071Epoch 00125: val_loss improved from 1.27511 to 1.26033, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.1522 - acc: 0.6020 - val_loss: 1.2603 - val_acc: 0.5778\n",
      "Epoch 126/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1637 - acc: 0.5843 - val_loss: 1.2565 - val_acc: 0.5556\n",
      "Epoch 127/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1371 - acc: 0.6196 - val_loss: 1.2505 - val_acc: 0.5667\n",
      "Epoch 128/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 1.1354 - acc: 0.6000 - val_loss: 1.2501 - val_acc: 0.5667\n",
      "Epoch 129/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1206 - acc: 0.6039 - val_loss: 1.2409 - val_acc: 0.5667\n",
      "Epoch 130/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0683 - acc: 0.6607Epoch 00130: val_loss improved from 1.26033 to 1.23978, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 1.0819 - acc: 0.6451 - val_loss: 1.2398 - val_acc: 0.5667\n",
      "Epoch 131/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0851 - acc: 0.6490 - val_loss: 1.2371 - val_acc: 0.5667\n",
      "Epoch 132/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.1011 - acc: 0.6216 - val_loss: 1.2302 - val_acc: 0.5667\n",
      "Epoch 133/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.1161 - acc: 0.6157 - val_loss: 1.2262 - val_acc: 0.5778\n",
      "Epoch 134/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0924 - acc: 0.6275 - val_loss: 1.2221 - val_acc: 0.5889\n",
      "Epoch 135/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0915 - acc: 0.6451Epoch 00135: val_loss improved from 1.23978 to 1.20856, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.0961 - acc: 0.6431 - val_loss: 1.2086 - val_acc: 0.5889\n",
      "Epoch 136/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0397 - acc: 0.6824 - val_loss: 1.2069 - val_acc: 0.5889\n",
      "Epoch 137/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0817 - acc: 0.6549 - val_loss: 1.2052 - val_acc: 0.5889\n",
      "Epoch 138/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.0555 - acc: 0.6510 - val_loss: 1.2027 - val_acc: 0.5778\n",
      "Epoch 139/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.1254 - acc: 0.6020 - val_loss: 1.1992 - val_acc: 0.5889\n",
      "Epoch 140/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0880 - acc: 0.6116Epoch 00140: val_loss improved from 1.20856 to 1.19917, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0883 - acc: 0.6176 - val_loss: 1.1992 - val_acc: 0.5778\n",
      "Epoch 141/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.0719 - acc: 0.6373 - val_loss: 1.1941 - val_acc: 0.5778\n",
      "Epoch 142/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0506 - acc: 0.6608 - val_loss: 1.1915 - val_acc: 0.5778\n",
      "Epoch 143/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.0643 - acc: 0.6294 - val_loss: 1.1873 - val_acc: 0.5778\n",
      "Epoch 144/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0717 - acc: 0.6157 - val_loss: 1.1853 - val_acc: 0.5778\n",
      "Epoch 145/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0552 - acc: 0.6339Epoch 00145: val_loss improved from 1.19917 to 1.17972, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0718 - acc: 0.6275 - val_loss: 1.1797 - val_acc: 0.5778\n",
      "Epoch 146/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 1s 3ms/step - loss: 1.0599 - acc: 0.6275 - val_loss: 1.1766 - val_acc: 0.5889\n",
      "Epoch 147/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0449 - acc: 0.6412 - val_loss: 1.1744 - val_acc: 0.5889\n",
      "Epoch 148/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.0074 - acc: 0.6686 - val_loss: 1.1696 - val_acc: 0.5889\n",
      "Epoch 149/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0482 - acc: 0.6353 - val_loss: 1.1699 - val_acc: 0.5778\n",
      "Epoch 150/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0338 - acc: 0.6451Epoch 00150: val_loss improved from 1.17972 to 1.16618, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0364 - acc: 0.6392 - val_loss: 1.1662 - val_acc: 0.5778\n",
      "Epoch 151/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 1.0302 - acc: 0.6392 - val_loss: 1.1616 - val_acc: 0.5778\n",
      "Epoch 152/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0522 - acc: 0.6353 - val_loss: 1.1638 - val_acc: 0.5889\n",
      "Epoch 153/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0205 - acc: 0.6745 - val_loss: 1.1601 - val_acc: 0.5778\n",
      "Epoch 154/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.0210 - acc: 0.6784 - val_loss: 1.1661 - val_acc: 0.5667\n",
      "Epoch 155/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0012 - acc: 0.6607Epoch 00155: val_loss improved from 1.16618 to 1.15756, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 1.0112 - acc: 0.6529 - val_loss: 1.1576 - val_acc: 0.5667\n",
      "Epoch 156/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.0364 - acc: 0.6725 - val_loss: 1.1549 - val_acc: 0.5778\n",
      "Epoch 157/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0128 - acc: 0.6471 - val_loss: 1.1481 - val_acc: 0.5889\n",
      "Epoch 158/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0065 - acc: 0.6686 - val_loss: 1.1487 - val_acc: 0.5889\n",
      "Epoch 159/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.0042 - acc: 0.6373 - val_loss: 1.1485 - val_acc: 0.5778\n",
      "Epoch 160/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0043 - acc: 0.6808Epoch 00160: val_loss improved from 1.15756 to 1.14247, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0142 - acc: 0.6706 - val_loss: 1.1425 - val_acc: 0.5889\n",
      "Epoch 161/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 1.0262 - acc: 0.6176 - val_loss: 1.1412 - val_acc: 0.5778\n",
      "Epoch 162/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.9855 - acc: 0.6725 - val_loss: 1.1353 - val_acc: 0.5889\n",
      "Epoch 163/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.9693 - acc: 0.6882 - val_loss: 1.1357 - val_acc: 0.5778\n",
      "Epoch 164/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.9739 - acc: 0.6882 - val_loss: 1.1322 - val_acc: 0.5889\n",
      "Epoch 165/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9948 - acc: 0.6384Epoch 00165: val_loss improved from 1.14247 to 1.13460, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.9919 - acc: 0.6431 - val_loss: 1.1346 - val_acc: 0.5889\n",
      "Epoch 166/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.9730 - acc: 0.6725 - val_loss: 1.1293 - val_acc: 0.5778\n",
      "Epoch 167/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9928 - acc: 0.6765 - val_loss: 1.1239 - val_acc: 0.5778\n",
      "Epoch 168/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9526 - acc: 0.6745 - val_loss: 1.1234 - val_acc: 0.5778\n",
      "Epoch 169/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 1.0119 - acc: 0.6745 - val_loss: 1.1245 - val_acc: 0.5667\n",
      "Epoch 170/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9710 - acc: 0.6741Epoch 00170: val_loss improved from 1.13460 to 1.12197, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9882 - acc: 0.6588 - val_loss: 1.1220 - val_acc: 0.5778\n",
      "Epoch 171/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.9980 - acc: 0.6471 - val_loss: 1.1207 - val_acc: 0.6111\n",
      "Epoch 172/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9286 - acc: 0.6941 - val_loss: 1.1174 - val_acc: 0.6111\n",
      "Epoch 173/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.9711 - acc: 0.6824 - val_loss: 1.1163 - val_acc: 0.6111\n",
      "Epoch 174/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.9532 - acc: 0.6843 - val_loss: 1.1155 - val_acc: 0.6111\n",
      "Epoch 175/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9207 - acc: 0.6830Epoch 00175: val_loss improved from 1.12197 to 1.10945, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9409 - acc: 0.6745 - val_loss: 1.1094 - val_acc: 0.6111\n",
      "Epoch 176/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.9244 - acc: 0.6804 - val_loss: 1.1042 - val_acc: 0.6111\n",
      "Epoch 177/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9737 - acc: 0.6745 - val_loss: 1.0981 - val_acc: 0.6222\n",
      "Epoch 178/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.9479 - acc: 0.6725 - val_loss: 1.1011 - val_acc: 0.6222\n",
      "Epoch 179/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9588 - acc: 0.6627 - val_loss: 1.1016 - val_acc: 0.6111\n",
      "Epoch 180/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9593 - acc: 0.6741Epoch 00180: val_loss improved from 1.10945 to 1.09970, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9668 - acc: 0.6745 - val_loss: 1.0997 - val_acc: 0.6222\n",
      "Epoch 181/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.9491 - acc: 0.6863 - val_loss: 1.0958 - val_acc: 0.6333\n",
      "Epoch 182/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9254 - acc: 0.6863 - val_loss: 1.0934 - val_acc: 0.6222\n",
      "Epoch 183/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9325 - acc: 0.6863 - val_loss: 1.0896 - val_acc: 0.6222\n",
      "Epoch 184/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.8565 - acc: 0.7333 - val_loss: 1.0886 - val_acc: 0.6222\n",
      "Epoch 185/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9281 - acc: 0.6741Epoch 00185: val_loss improved from 1.09970 to 1.08858, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9277 - acc: 0.6784 - val_loss: 1.0886 - val_acc: 0.6111\n",
      "Epoch 186/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9101 - acc: 0.7157 - val_loss: 1.0814 - val_acc: 0.6222\n",
      "Epoch 187/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.9294 - acc: 0.6745 - val_loss: 1.0784 - val_acc: 0.6111\n",
      "Epoch 188/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9223 - acc: 0.6922 - val_loss: 1.0799 - val_acc: 0.6111\n",
      "Epoch 189/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.9053 - acc: 0.6882 - val_loss: 1.0799 - val_acc: 0.6000\n",
      "Epoch 190/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9291 - acc: 0.6808Epoch 00190: val_loss improved from 1.08858 to 1.07545, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9289 - acc: 0.6804 - val_loss: 1.0754 - val_acc: 0.6000\n",
      "Epoch 191/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9332 - acc: 0.6667 - val_loss: 1.0712 - val_acc: 0.6111\n",
      "Epoch 192/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.9187 - acc: 0.7098 - val_loss: 1.0665 - val_acc: 0.6333\n",
      "Epoch 193/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8832 - acc: 0.7118 - val_loss: 1.0639 - val_acc: 0.6111\n",
      "Epoch 194/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.9043 - acc: 0.6902 - val_loss: 1.0642 - val_acc: 0.6111\n",
      "Epoch 195/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9412 - acc: 0.6607Epoch 00195: val_loss improved from 1.07545 to 1.06052, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9379 - acc: 0.6627 - val_loss: 1.0605 - val_acc: 0.6333\n",
      "Epoch 196/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.9108 - acc: 0.7020 - val_loss: 1.0582 - val_acc: 0.6333\n",
      "Epoch 197/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.9133 - acc: 0.6804 - val_loss: 1.0610 - val_acc: 0.6333\n",
      "Epoch 198/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8977 - acc: 0.7020 - val_loss: 1.0566 - val_acc: 0.6222\n",
      "Epoch 199/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8759 - acc: 0.7020 - val_loss: 1.0562 - val_acc: 0.6333\n",
      "Epoch 200/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8805 - acc: 0.7054Epoch 00200: val_loss improved from 1.06052 to 1.05012, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.8774 - acc: 0.7078 - val_loss: 1.0501 - val_acc: 0.6333\n",
      "Epoch 201/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8839 - acc: 0.6824 - val_loss: 1.0488 - val_acc: 0.6333\n",
      "Epoch 202/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.8998 - acc: 0.7039 - val_loss: 1.0469 - val_acc: 0.6222\n",
      "Epoch 203/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8796 - acc: 0.7118 - val_loss: 1.0465 - val_acc: 0.6333\n",
      "Epoch 204/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8855 - acc: 0.6922 - val_loss: 1.0508 - val_acc: 0.6111\n",
      "Epoch 205/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8537 - acc: 0.7165Epoch 00205: val_loss improved from 1.05012 to 1.04796, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.8707 - acc: 0.7118 - val_loss: 1.0480 - val_acc: 0.6333\n",
      "Epoch 206/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8831 - acc: 0.6863 - val_loss: 1.0477 - val_acc: 0.6222\n",
      "Epoch 207/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8851 - acc: 0.7000 - val_loss: 1.0459 - val_acc: 0.6222\n",
      "Epoch 208/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8822 - acc: 0.7039 - val_loss: 1.0424 - val_acc: 0.6222\n",
      "Epoch 209/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8429 - acc: 0.7137 - val_loss: 1.0445 - val_acc: 0.6111\n",
      "Epoch 210/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8718 - acc: 0.7054Epoch 00210: val_loss improved from 1.04796 to 1.03966, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8651 - acc: 0.7039 - val_loss: 1.0397 - val_acc: 0.6222\n",
      "Epoch 211/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8924 - acc: 0.6686 - val_loss: 1.0378 - val_acc: 0.6333\n",
      "Epoch 212/3000\n",
      "510/510 [==============================] - 3s 6ms/step - loss: 0.8516 - acc: 0.7098 - val_loss: 1.0362 - val_acc: 0.6333\n",
      "Epoch 213/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.8408 - acc: 0.7059 - val_loss: 1.0335 - val_acc: 0.6444\n",
      "Epoch 214/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8098 - acc: 0.7451 - val_loss: 1.0371 - val_acc: 0.6333\n",
      "Epoch 215/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8580 - acc: 0.7188Epoch 00215: val_loss improved from 1.03966 to 1.02957, saving model to top_weight.h5\n",
      "510/510 [==============================] - 3s 7ms/step - loss: 0.8747 - acc: 0.7039 - val_loss: 1.0296 - val_acc: 0.6333\n",
      "Epoch 216/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8392 - acc: 0.7196 - val_loss: 1.0302 - val_acc: 0.6333\n",
      "Epoch 217/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8877 - acc: 0.6902 - val_loss: 1.0300 - val_acc: 0.6333\n",
      "Epoch 218/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8503 - acc: 0.7020 - val_loss: 1.0274 - val_acc: 0.6333\n",
      "Epoch 219/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8083 - acc: 0.7588 - val_loss: 1.0301 - val_acc: 0.6222\n",
      "Epoch 220/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8396 - acc: 0.7098Epoch 00220: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8329 - acc: 0.7196 - val_loss: 1.0336 - val_acc: 0.6222\n",
      "Epoch 221/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8458 - acc: 0.7078 - val_loss: 1.0247 - val_acc: 0.6111\n",
      "Epoch 222/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8159 - acc: 0.7333 - val_loss: 1.0238 - val_acc: 0.6222\n",
      "Epoch 223/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8763 - acc: 0.6843 - val_loss: 1.0223 - val_acc: 0.6111\n",
      "Epoch 224/3000\n",
      "510/510 [==============================] - 3s 6ms/step - loss: 0.8582 - acc: 0.7118 - val_loss: 1.0220 - val_acc: 0.6111\n",
      "Epoch 225/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8062 - acc: 0.7299Epoch 00225: val_loss improved from 1.02957 to 1.01534, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8114 - acc: 0.7294 - val_loss: 1.0153 - val_acc: 0.6222\n",
      "Epoch 226/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8471 - acc: 0.6804 - val_loss: 1.0129 - val_acc: 0.6333\n",
      "Epoch 227/3000\n",
      "510/510 [==============================] - 3s 6ms/step - loss: 0.8570 - acc: 0.7078 - val_loss: 1.0108 - val_acc: 0.6333\n",
      "Epoch 228/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8174 - acc: 0.7255 - val_loss: 1.0094 - val_acc: 0.6222\n",
      "Epoch 229/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8285 - acc: 0.7196 - val_loss: 1.0036 - val_acc: 0.6333\n",
      "Epoch 230/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8213 - acc: 0.7344Epoch 00230: val_loss improved from 1.01534 to 1.00266, saving model to top_weight.h5\n",
      "510/510 [==============================] - 3s 6ms/step - loss: 0.8375 - acc: 0.7118 - val_loss: 1.0027 - val_acc: 0.6333\n",
      "Epoch 231/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8353 - acc: 0.7020 - val_loss: 0.9991 - val_acc: 0.6444\n",
      "Epoch 232/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8488 - acc: 0.7020 - val_loss: 1.0003 - val_acc: 0.6333\n",
      "Epoch 233/3000\n",
      "510/510 [==============================] - 3s 6ms/step - loss: 0.8243 - acc: 0.7275 - val_loss: 1.0017 - val_acc: 0.6444\n",
      "Epoch 234/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8218 - acc: 0.7275 - val_loss: 1.0046 - val_acc: 0.6222\n",
      "Epoch 235/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8079 - acc: 0.7232Epoch 00235: val_loss improved from 1.00266 to 0.99663, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8101 - acc: 0.7176 - val_loss: 0.9966 - val_acc: 0.6333\n",
      "Epoch 236/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8145 - acc: 0.7137 - val_loss: 0.9993 - val_acc: 0.6222\n",
      "Epoch 237/3000\n",
      "510/510 [==============================] - 3s 6ms/step - loss: 0.8119 - acc: 0.7275 - val_loss: 0.9986 - val_acc: 0.6222\n",
      "Epoch 238/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.7761 - acc: 0.7529 - val_loss: 0.9938 - val_acc: 0.6333\n",
      "Epoch 239/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8125 - acc: 0.7431 - val_loss: 0.9907 - val_acc: 0.6333\n",
      "Epoch 240/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8072 - acc: 0.7165Epoch 00240: val_loss improved from 0.99663 to 0.99050, saving model to top_weight.h5\n",
      "510/510 [==============================] - 3s 6ms/step - loss: 0.8162 - acc: 0.7059 - val_loss: 0.9905 - val_acc: 0.6444\n",
      "Epoch 241/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.7837 - acc: 0.7569 - val_loss: 0.9878 - val_acc: 0.6444\n",
      "Epoch 242/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8265 - acc: 0.7196 - val_loss: 0.9863 - val_acc: 0.6333\n",
      "Epoch 243/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 3s 6ms/step - loss: 0.8174 - acc: 0.7216 - val_loss: 0.9865 - val_acc: 0.6333\n",
      "Epoch 244/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8102 - acc: 0.7216 - val_loss: 0.9861 - val_acc: 0.6333\n",
      "Epoch 245/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8145 - acc: 0.7210Epoch 00245: val_loss improved from 0.99050 to 0.98457, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8048 - acc: 0.7255 - val_loss: 0.9846 - val_acc: 0.6222\n",
      "Epoch 246/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7754 - acc: 0.7333 - val_loss: 0.9850 - val_acc: 0.6222\n",
      "Epoch 247/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.8178 - acc: 0.7157 - val_loss: 0.9791 - val_acc: 0.6333\n",
      "Epoch 248/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.7786 - acc: 0.7471 - val_loss: 0.9786 - val_acc: 0.6333\n",
      "Epoch 249/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.8122 - acc: 0.7235 - val_loss: 0.9780 - val_acc: 0.6222\n",
      "Epoch 250/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7679 - acc: 0.7545Epoch 00250: val_loss improved from 0.98457 to 0.97653, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7708 - acc: 0.7471 - val_loss: 0.9765 - val_acc: 0.6222\n",
      "Epoch 251/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8081 - acc: 0.7039 - val_loss: 0.9762 - val_acc: 0.6222\n",
      "Epoch 252/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7722 - acc: 0.7608 - val_loss: 0.9732 - val_acc: 0.6222\n",
      "Epoch 253/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8077 - acc: 0.7059 - val_loss: 0.9747 - val_acc: 0.6222\n",
      "Epoch 254/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.7775 - acc: 0.7137 - val_loss: 0.9706 - val_acc: 0.6333\n",
      "Epoch 255/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7565 - acc: 0.7545Epoch 00255: val_loss improved from 0.97653 to 0.96918, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7616 - acc: 0.7510 - val_loss: 0.9692 - val_acc: 0.6333\n",
      "Epoch 256/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.7772 - acc: 0.7314 - val_loss: 0.9705 - val_acc: 0.6333\n",
      "Epoch 257/3000\n",
      "510/510 [==============================] - 3s 6ms/step - loss: 0.7821 - acc: 0.7412 - val_loss: 0.9697 - val_acc: 0.6222\n",
      "Epoch 258/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7783 - acc: 0.7490 - val_loss: 0.9680 - val_acc: 0.6222\n",
      "Epoch 259/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.7832 - acc: 0.7275 - val_loss: 0.9665 - val_acc: 0.6222\n",
      "Epoch 260/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8124 - acc: 0.7366Epoch 00260: val_loss improved from 0.96918 to 0.96389, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.8154 - acc: 0.7412 - val_loss: 0.9639 - val_acc: 0.6222\n",
      "Epoch 261/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7791 - acc: 0.7451 - val_loss: 0.9623 - val_acc: 0.6111\n",
      "Epoch 262/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.8142 - acc: 0.7275 - val_loss: 0.9616 - val_acc: 0.6111\n",
      "Epoch 263/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.7483 - acc: 0.7608 - val_loss: 0.9639 - val_acc: 0.6111\n",
      "Epoch 264/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.7567 - acc: 0.7431 - val_loss: 0.9650 - val_acc: 0.6333\n",
      "Epoch 265/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7669 - acc: 0.7366Epoch 00265: val_loss improved from 0.96389 to 0.96077, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.7723 - acc: 0.7431 - val_loss: 0.9608 - val_acc: 0.6333\n",
      "Epoch 266/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7487 - acc: 0.7647 - val_loss: 0.9628 - val_acc: 0.6222\n",
      "Epoch 267/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.7623 - acc: 0.7490 - val_loss: 0.9605 - val_acc: 0.6333\n",
      "Epoch 268/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.7305 - acc: 0.7627 - val_loss: 0.9564 - val_acc: 0.6333\n",
      "Epoch 269/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7697 - acc: 0.7353 - val_loss: 0.9549 - val_acc: 0.6333\n",
      "Epoch 270/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7714 - acc: 0.7411Epoch 00270: val_loss improved from 0.96077 to 0.95661, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7553 - acc: 0.7510 - val_loss: 0.9566 - val_acc: 0.6333\n",
      "Epoch 271/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.7845 - acc: 0.7451 - val_loss: 0.9560 - val_acc: 0.6333\n",
      "Epoch 272/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.7391 - acc: 0.7706 - val_loss: 0.9543 - val_acc: 0.6222\n",
      "Epoch 273/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7467 - acc: 0.7490 - val_loss: 0.9552 - val_acc: 0.6222\n",
      "Epoch 274/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7378 - acc: 0.7471 - val_loss: 0.9518 - val_acc: 0.6222\n",
      "Epoch 275/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7512 - acc: 0.7388Epoch 00275: val_loss improved from 0.95661 to 0.94996, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.7510 - acc: 0.7353 - val_loss: 0.9500 - val_acc: 0.6333\n",
      "Epoch 276/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7153 - acc: 0.7706 - val_loss: 0.9474 - val_acc: 0.6333\n",
      "Epoch 277/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.7385 - acc: 0.7765 - val_loss: 0.9497 - val_acc: 0.6333\n",
      "Epoch 278/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7567 - acc: 0.7373 - val_loss: 0.9471 - val_acc: 0.6333\n",
      "Epoch 279/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7710 - acc: 0.7353 - val_loss: 0.9474 - val_acc: 0.6333\n",
      "Epoch 280/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7342 - acc: 0.7746Epoch 00280: val_loss improved from 0.94996 to 0.94325, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.7393 - acc: 0.7686 - val_loss: 0.9432 - val_acc: 0.6333\n",
      "Epoch 281/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.7262 - acc: 0.7627 - val_loss: 0.9438 - val_acc: 0.6333\n",
      "Epoch 282/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.7332 - acc: 0.7784 - val_loss: 0.9430 - val_acc: 0.6333\n",
      "Epoch 283/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.7175 - acc: 0.7431 - val_loss: 0.9454 - val_acc: 0.6333\n",
      "Epoch 284/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.7241 - acc: 0.7471 - val_loss: 0.9432 - val_acc: 0.6333\n",
      "Epoch 285/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7449 - acc: 0.7366Epoch 00285: val_loss improved from 0.94325 to 0.94207, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.7492 - acc: 0.7412 - val_loss: 0.9421 - val_acc: 0.6333\n",
      "Epoch 286/3000\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.7581 - acc: 0.7431 - val_loss: 0.9404 - val_acc: 0.6333\n",
      "Epoch 287/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.7368 - acc: 0.7275 - val_loss: 0.9403 - val_acc: 0.6333\n",
      "Epoch 288/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7281 - acc: 0.7765 - val_loss: 0.9417 - val_acc: 0.6333\n",
      "Epoch 289/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.7350 - acc: 0.7647 - val_loss: 0.9398 - val_acc: 0.6444\n",
      "Epoch 290/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7295 - acc: 0.7679Epoch 00290: val_loss improved from 0.94207 to 0.93871, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.7296 - acc: 0.7647 - val_loss: 0.9387 - val_acc: 0.6333\n",
      "Epoch 291/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 2s 5ms/step - loss: 0.7427 - acc: 0.7451 - val_loss: 0.9388 - val_acc: 0.6333\n",
      "Epoch 292/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.7161 - acc: 0.7706 - val_loss: 0.9355 - val_acc: 0.6333\n",
      "Epoch 293/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6725 - acc: 0.7980 - val_loss: 0.9337 - val_acc: 0.6444\n",
      "Epoch 294/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7548 - acc: 0.7412 - val_loss: 0.9284 - val_acc: 0.6333\n",
      "Epoch 295/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7392 - acc: 0.7455Epoch 00295: val_loss improved from 0.93871 to 0.92801, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7313 - acc: 0.7490 - val_loss: 0.9280 - val_acc: 0.6333\n",
      "Epoch 296/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7081 - acc: 0.7882 - val_loss: 0.9301 - val_acc: 0.6222\n",
      "Epoch 297/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6932 - acc: 0.7824 - val_loss: 0.9237 - val_acc: 0.6333\n",
      "Epoch 298/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6969 - acc: 0.7824 - val_loss: 0.9219 - val_acc: 0.6333\n",
      "Epoch 299/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7072 - acc: 0.7608 - val_loss: 0.9214 - val_acc: 0.6222\n",
      "Epoch 300/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6953 - acc: 0.7567Epoch 00300: val_loss improved from 0.92801 to 0.91629, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6910 - acc: 0.7647 - val_loss: 0.9163 - val_acc: 0.6333\n",
      "Epoch 301/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7313 - acc: 0.7686 - val_loss: 0.9150 - val_acc: 0.6333\n",
      "Epoch 302/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6834 - acc: 0.7843 - val_loss: 0.9148 - val_acc: 0.6444\n",
      "Epoch 303/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6847 - acc: 0.7863 - val_loss: 0.9153 - val_acc: 0.6556\n",
      "Epoch 304/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.7148 - acc: 0.7608 - val_loss: 0.9163 - val_acc: 0.6556\n",
      "Epoch 305/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7165 - acc: 0.7679Epoch 00305: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7074 - acc: 0.7686 - val_loss: 0.9193 - val_acc: 0.6444\n",
      "Epoch 306/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.7371 - acc: 0.7667 - val_loss: 0.9201 - val_acc: 0.6556\n",
      "Epoch 307/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6870 - acc: 0.7843 - val_loss: 0.9210 - val_acc: 0.6556\n",
      "Epoch 308/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.7188 - acc: 0.7471 - val_loss: 0.9241 - val_acc: 0.6667\n",
      "Epoch 309/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6975 - acc: 0.7529 - val_loss: 0.9214 - val_acc: 0.6667\n",
      "Epoch 310/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6852 - acc: 0.7879Epoch 00310: val_loss improved from 0.91629 to 0.91527, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6865 - acc: 0.7863 - val_loss: 0.9153 - val_acc: 0.6556\n",
      "Epoch 311/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.7003 - acc: 0.7529 - val_loss: 0.9164 - val_acc: 0.6556\n",
      "Epoch 312/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.7023 - acc: 0.7627 - val_loss: 0.9178 - val_acc: 0.6444\n",
      "Epoch 313/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6898 - acc: 0.8020 - val_loss: 0.9178 - val_acc: 0.6444\n",
      "Epoch 314/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6910 - acc: 0.7980 - val_loss: 0.9137 - val_acc: 0.6556\n",
      "Epoch 315/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6873 - acc: 0.7812Epoch 00315: val_loss improved from 0.91527 to 0.91250, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6867 - acc: 0.7804 - val_loss: 0.9125 - val_acc: 0.6667\n",
      "Epoch 316/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6646 - acc: 0.8059 - val_loss: 0.9042 - val_acc: 0.6556\n",
      "Epoch 317/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6394 - acc: 0.8176 - val_loss: 0.9045 - val_acc: 0.6556\n",
      "Epoch 318/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6808 - acc: 0.7882 - val_loss: 0.9055 - val_acc: 0.6444\n",
      "Epoch 319/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6548 - acc: 0.8039 - val_loss: 0.9106 - val_acc: 0.6444\n",
      "Epoch 320/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6743 - acc: 0.8058Epoch 00320: val_loss improved from 0.91250 to 0.90565, saving model to top_weight.h5\n",
      "510/510 [==============================] - 3s 5ms/step - loss: 0.6711 - acc: 0.8000 - val_loss: 0.9056 - val_acc: 0.6556\n",
      "Epoch 321/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.6744 - acc: 0.7804 - val_loss: 0.9059 - val_acc: 0.6556\n",
      "Epoch 322/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6446 - acc: 0.8020 - val_loss: 0.9061 - val_acc: 0.6556\n",
      "Epoch 323/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6671 - acc: 0.8020 - val_loss: 0.9084 - val_acc: 0.6444\n",
      "Epoch 324/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.6872 - acc: 0.7667 - val_loss: 0.9057 - val_acc: 0.6444\n",
      "Epoch 325/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6628 - acc: 0.8080Epoch 00325: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6547 - acc: 0.8078 - val_loss: 0.9072 - val_acc: 0.6333\n",
      "Epoch 326/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6698 - acc: 0.8039 - val_loss: 0.9028 - val_acc: 0.6556\n",
      "Epoch 327/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6349 - acc: 0.7902 - val_loss: 0.8999 - val_acc: 0.6556\n",
      "Epoch 328/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6674 - acc: 0.7882 - val_loss: 0.9004 - val_acc: 0.6556\n",
      "Epoch 329/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6563 - acc: 0.7941 - val_loss: 0.9000 - val_acc: 0.6556\n",
      "Epoch 330/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6858 - acc: 0.7790Epoch 00330: val_loss improved from 0.90565 to 0.90235, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6761 - acc: 0.7804 - val_loss: 0.9023 - val_acc: 0.6556\n",
      "Epoch 331/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.6578 - acc: 0.8118 - val_loss: 0.9020 - val_acc: 0.6556\n",
      "Epoch 332/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6751 - acc: 0.7745 - val_loss: 0.9016 - val_acc: 0.6556\n",
      "Epoch 333/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6538 - acc: 0.8000 - val_loss: 0.9008 - val_acc: 0.6556\n",
      "Epoch 334/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6573 - acc: 0.7765 - val_loss: 0.8979 - val_acc: 0.6333\n",
      "Epoch 335/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6423 - acc: 0.8013Epoch 00335: val_loss improved from 0.90235 to 0.89660, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.6618 - acc: 0.7922 - val_loss: 0.8966 - val_acc: 0.6444\n",
      "Epoch 336/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6406 - acc: 0.8039 - val_loss: 0.8983 - val_acc: 0.6444\n",
      "Epoch 337/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6654 - acc: 0.7882 - val_loss: 0.8991 - val_acc: 0.6444\n",
      "Epoch 338/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6640 - acc: 0.7902 - val_loss: 0.8960 - val_acc: 0.6444\n",
      "Epoch 339/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6503 - acc: 0.7922 - val_loss: 0.8969 - val_acc: 0.6444\n",
      "Epoch 340/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6694 - acc: 0.8080Epoch 00340: val_loss did not improve\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6700 - acc: 0.8059 - val_loss: 0.9008 - val_acc: 0.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6366 - acc: 0.7824 - val_loss: 0.8974 - val_acc: 0.6444\n",
      "Epoch 342/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.6290 - acc: 0.8059 - val_loss: 0.8914 - val_acc: 0.6556\n",
      "Epoch 343/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6308 - acc: 0.8078 - val_loss: 0.8895 - val_acc: 0.6667\n",
      "Epoch 344/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6798 - acc: 0.7627 - val_loss: 0.8907 - val_acc: 0.6778\n",
      "Epoch 345/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6232 - acc: 0.8080Epoch 00345: val_loss improved from 0.89660 to 0.88923, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6390 - acc: 0.7980 - val_loss: 0.8892 - val_acc: 0.6778\n",
      "Epoch 346/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6140 - acc: 0.8098 - val_loss: 0.8845 - val_acc: 0.6667\n",
      "Epoch 347/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.6534 - acc: 0.7941 - val_loss: 0.8835 - val_acc: 0.6667\n",
      "Epoch 348/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6102 - acc: 0.8176 - val_loss: 0.8859 - val_acc: 0.6667\n",
      "Epoch 349/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6203 - acc: 0.8196 - val_loss: 0.8834 - val_acc: 0.6556\n",
      "Epoch 350/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6572 - acc: 0.8170Epoch 00350: val_loss improved from 0.88923 to 0.88138, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6436 - acc: 0.8196 - val_loss: 0.8814 - val_acc: 0.6556\n",
      "Epoch 351/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6587 - acc: 0.8137 - val_loss: 0.8862 - val_acc: 0.6556\n",
      "Epoch 352/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.6655 - acc: 0.7765 - val_loss: 0.8846 - val_acc: 0.6444\n",
      "Epoch 353/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6145 - acc: 0.8000 - val_loss: 0.8821 - val_acc: 0.6667\n",
      "Epoch 354/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.6146 - acc: 0.8098 - val_loss: 0.8820 - val_acc: 0.6667\n",
      "Epoch 355/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6383 - acc: 0.7746Epoch 00355: val_loss improved from 0.88138 to 0.87923, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6395 - acc: 0.7765 - val_loss: 0.8792 - val_acc: 0.6667\n",
      "Epoch 356/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6355 - acc: 0.7941 - val_loss: 0.8746 - val_acc: 0.6667\n",
      "Epoch 357/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6162 - acc: 0.8059 - val_loss: 0.8727 - val_acc: 0.6778\n",
      "Epoch 358/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6062 - acc: 0.8176 - val_loss: 0.8721 - val_acc: 0.6778\n",
      "Epoch 359/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.6145 - acc: 0.8098 - val_loss: 0.8705 - val_acc: 0.6889\n",
      "Epoch 360/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6156 - acc: 0.7902Epoch 00360: val_loss improved from 0.87923 to 0.86988, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6103 - acc: 0.7922 - val_loss: 0.8699 - val_acc: 0.6889\n",
      "Epoch 361/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6315 - acc: 0.7980 - val_loss: 0.8714 - val_acc: 0.6889\n",
      "Epoch 362/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6264 - acc: 0.7980 - val_loss: 0.8719 - val_acc: 0.6889\n",
      "Epoch 363/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6128 - acc: 0.8020 - val_loss: 0.8681 - val_acc: 0.7000\n",
      "Epoch 364/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6426 - acc: 0.8039 - val_loss: 0.8685 - val_acc: 0.6889\n",
      "Epoch 365/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5954 - acc: 0.8304Epoch 00365: val_loss improved from 0.86988 to 0.86210, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5991 - acc: 0.8235 - val_loss: 0.8621 - val_acc: 0.6889\n",
      "Epoch 366/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6252 - acc: 0.7941 - val_loss: 0.8591 - val_acc: 0.6889\n",
      "Epoch 367/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5922 - acc: 0.8137 - val_loss: 0.8594 - val_acc: 0.6889\n",
      "Epoch 368/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6337 - acc: 0.7922 - val_loss: 0.8589 - val_acc: 0.6889\n",
      "Epoch 369/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.6096 - acc: 0.8137 - val_loss: 0.8610 - val_acc: 0.6889\n",
      "Epoch 370/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6257 - acc: 0.7857Epoch 00370: val_loss improved from 0.86210 to 0.85825, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6150 - acc: 0.7902 - val_loss: 0.8582 - val_acc: 0.6889\n",
      "Epoch 371/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5989 - acc: 0.8373 - val_loss: 0.8578 - val_acc: 0.6889\n",
      "Epoch 372/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6279 - acc: 0.7941 - val_loss: 0.8577 - val_acc: 0.6889\n",
      "Epoch 373/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6094 - acc: 0.8039 - val_loss: 0.8577 - val_acc: 0.7000\n",
      "Epoch 374/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6158 - acc: 0.7882 - val_loss: 0.8585 - val_acc: 0.7000\n",
      "Epoch 375/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5979 - acc: 0.8237Epoch 00375: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5973 - acc: 0.8196 - val_loss: 0.8605 - val_acc: 0.6889\n",
      "Epoch 376/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5925 - acc: 0.8176 - val_loss: 0.8533 - val_acc: 0.6889\n",
      "Epoch 377/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6026 - acc: 0.8196 - val_loss: 0.8552 - val_acc: 0.6889\n",
      "Epoch 378/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5719 - acc: 0.8275 - val_loss: 0.8546 - val_acc: 0.6889\n",
      "Epoch 379/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5868 - acc: 0.8451 - val_loss: 0.8530 - val_acc: 0.6889\n",
      "Epoch 380/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6374 - acc: 0.7879Epoch 00380: val_loss improved from 0.85825 to 0.85070, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6275 - acc: 0.7941 - val_loss: 0.8507 - val_acc: 0.7000\n",
      "Epoch 381/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5790 - acc: 0.8294 - val_loss: 0.8501 - val_acc: 0.7000\n",
      "Epoch 382/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5935 - acc: 0.8294 - val_loss: 0.8506 - val_acc: 0.7000\n",
      "Epoch 383/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6107 - acc: 0.8078 - val_loss: 0.8502 - val_acc: 0.7000\n",
      "Epoch 384/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5916 - acc: 0.8137 - val_loss: 0.8514 - val_acc: 0.7000\n",
      "Epoch 385/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5947 - acc: 0.8013Epoch 00385: val_loss improved from 0.85070 to 0.84975, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5955 - acc: 0.8078 - val_loss: 0.8497 - val_acc: 0.7000\n",
      "Epoch 386/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5945 - acc: 0.8255 - val_loss: 0.8489 - val_acc: 0.7000\n",
      "Epoch 387/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6048 - acc: 0.8118 - val_loss: 0.8500 - val_acc: 0.7000\n",
      "Epoch 388/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6227 - acc: 0.7961 - val_loss: 0.8506 - val_acc: 0.6889\n",
      "Epoch 389/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6014 - acc: 0.8314 - val_loss: 0.8478 - val_acc: 0.6889\n",
      "Epoch 390/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5810 - acc: 0.8326Epoch 00390: val_loss improved from 0.84975 to 0.84516, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5787 - acc: 0.8392 - val_loss: 0.8452 - val_acc: 0.7000\n",
      "Epoch 391/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6241 - acc: 0.8059 - val_loss: 0.8450 - val_acc: 0.7000\n",
      "Epoch 392/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6002 - acc: 0.8059 - val_loss: 0.8418 - val_acc: 0.7000\n",
      "Epoch 393/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5700 - acc: 0.8255 - val_loss: 0.8426 - val_acc: 0.7000\n",
      "Epoch 394/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.6119 - acc: 0.8020 - val_loss: 0.8444 - val_acc: 0.7000\n",
      "Epoch 395/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5704 - acc: 0.8326Epoch 00395: val_loss improved from 0.84516 to 0.84412, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5754 - acc: 0.8314 - val_loss: 0.8441 - val_acc: 0.7000\n",
      "Epoch 396/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5715 - acc: 0.8333 - val_loss: 0.8418 - val_acc: 0.7000\n",
      "Epoch 397/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5832 - acc: 0.7922 - val_loss: 0.8387 - val_acc: 0.7000\n",
      "Epoch 398/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5800 - acc: 0.8294 - val_loss: 0.8403 - val_acc: 0.7000\n",
      "Epoch 399/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5787 - acc: 0.8314 - val_loss: 0.8374 - val_acc: 0.7111\n",
      "Epoch 400/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5792 - acc: 0.8348Epoch 00400: val_loss improved from 0.84412 to 0.84158, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5866 - acc: 0.8255 - val_loss: 0.8416 - val_acc: 0.7000\n",
      "Epoch 401/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5591 - acc: 0.8412 - val_loss: 0.8425 - val_acc: 0.7222\n",
      "Epoch 402/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5961 - acc: 0.8216 - val_loss: 0.8342 - val_acc: 0.7222\n",
      "Epoch 403/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5625 - acc: 0.8294 - val_loss: 0.8356 - val_acc: 0.7333\n",
      "Epoch 404/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5708 - acc: 0.8471 - val_loss: 0.8363 - val_acc: 0.7111\n",
      "Epoch 405/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6373 - acc: 0.7924Epoch 00405: val_loss did not improve\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.6283 - acc: 0.7922 - val_loss: 0.8426 - val_acc: 0.7111\n",
      "Epoch 406/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5864 - acc: 0.8000 - val_loss: 0.8352 - val_acc: 0.7000\n",
      "Epoch 407/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5692 - acc: 0.8353 - val_loss: 0.8338 - val_acc: 0.7000\n",
      "Epoch 408/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5821 - acc: 0.8392 - val_loss: 0.8325 - val_acc: 0.7000\n",
      "Epoch 409/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5447 - acc: 0.8510 - val_loss: 0.8314 - val_acc: 0.7000\n",
      "Epoch 410/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5765 - acc: 0.8281Epoch 00410: val_loss improved from 0.84158 to 0.82935, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5761 - acc: 0.8314 - val_loss: 0.8294 - val_acc: 0.7000\n",
      "Epoch 411/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.6022 - acc: 0.8059 - val_loss: 0.8271 - val_acc: 0.7000\n",
      "Epoch 412/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5678 - acc: 0.8373 - val_loss: 0.8263 - val_acc: 0.7000\n",
      "Epoch 413/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5526 - acc: 0.8412 - val_loss: 0.8275 - val_acc: 0.7111\n",
      "Epoch 414/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5705 - acc: 0.8294 - val_loss: 0.8266 - val_acc: 0.7000\n",
      "Epoch 415/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5660 - acc: 0.8326Epoch 00415: val_loss improved from 0.82935 to 0.82771, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5511 - acc: 0.8412 - val_loss: 0.8277 - val_acc: 0.7111\n",
      "Epoch 416/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5613 - acc: 0.8255 - val_loss: 0.8272 - val_acc: 0.7000\n",
      "Epoch 417/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5324 - acc: 0.8549 - val_loss: 0.8292 - val_acc: 0.7111\n",
      "Epoch 418/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5775 - acc: 0.8176 - val_loss: 0.8260 - val_acc: 0.7111\n",
      "Epoch 419/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5308 - acc: 0.8431 - val_loss: 0.8235 - val_acc: 0.7111\n",
      "Epoch 420/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5474 - acc: 0.8438Epoch 00420: val_loss improved from 0.82771 to 0.82698, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5558 - acc: 0.8412 - val_loss: 0.8270 - val_acc: 0.7222\n",
      "Epoch 421/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5570 - acc: 0.8216 - val_loss: 0.8259 - val_acc: 0.7333\n",
      "Epoch 422/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5681 - acc: 0.8373 - val_loss: 0.8267 - val_acc: 0.7222\n",
      "Epoch 423/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5562 - acc: 0.8216 - val_loss: 0.8264 - val_acc: 0.7111\n",
      "Epoch 424/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5427 - acc: 0.8373 - val_loss: 0.8240 - val_acc: 0.7222\n",
      "Epoch 425/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5306 - acc: 0.8393Epoch 00425: val_loss improved from 0.82698 to 0.82405, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5537 - acc: 0.8294 - val_loss: 0.8241 - val_acc: 0.7222\n",
      "Epoch 426/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5539 - acc: 0.8275 - val_loss: 0.8237 - val_acc: 0.7000\n",
      "Epoch 427/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5350 - acc: 0.8431 - val_loss: 0.8253 - val_acc: 0.7000\n",
      "Epoch 428/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5567 - acc: 0.8392 - val_loss: 0.8251 - val_acc: 0.7000\n",
      "Epoch 429/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5275 - acc: 0.8510 - val_loss: 0.8212 - val_acc: 0.7000\n",
      "Epoch 430/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5395 - acc: 0.8549Epoch 00430: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5422 - acc: 0.8471 - val_loss: 0.8286 - val_acc: 0.6889\n",
      "Epoch 431/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5758 - acc: 0.8216 - val_loss: 0.8194 - val_acc: 0.7000\n",
      "Epoch 432/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5511 - acc: 0.8431 - val_loss: 0.8163 - val_acc: 0.7000\n",
      "Epoch 433/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5345 - acc: 0.8392 - val_loss: 0.8167 - val_acc: 0.7000\n",
      "Epoch 434/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5228 - acc: 0.8431 - val_loss: 0.8190 - val_acc: 0.7000\n",
      "Epoch 435/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5222 - acc: 0.8728Epoch 00435: val_loss improved from 0.82405 to 0.81210, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5429 - acc: 0.8510 - val_loss: 0.8121 - val_acc: 0.7333\n",
      "Epoch 436/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5573 - acc: 0.8294 - val_loss: 0.8115 - val_acc: 0.7000\n",
      "Epoch 437/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5348 - acc: 0.8294 - val_loss: 0.8125 - val_acc: 0.7111\n",
      "Epoch 438/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5884 - acc: 0.8078 - val_loss: 0.8100 - val_acc: 0.7111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 439/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5288 - acc: 0.8451 - val_loss: 0.8109 - val_acc: 0.7222\n",
      "Epoch 440/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5801 - acc: 0.8304Epoch 00440: val_loss improved from 0.81210 to 0.80929, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5735 - acc: 0.8314 - val_loss: 0.8093 - val_acc: 0.7111\n",
      "Epoch 441/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5362 - acc: 0.8471 - val_loss: 0.8089 - val_acc: 0.7111\n",
      "Epoch 442/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5695 - acc: 0.8235 - val_loss: 0.8094 - val_acc: 0.7111\n",
      "Epoch 443/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5364 - acc: 0.8294 - val_loss: 0.8074 - val_acc: 0.7111\n",
      "Epoch 444/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5426 - acc: 0.8235 - val_loss: 0.8104 - val_acc: 0.7000\n",
      "Epoch 445/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5056 - acc: 0.8638Epoch 00445: val_loss improved from 0.80929 to 0.80239, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5095 - acc: 0.8588 - val_loss: 0.8024 - val_acc: 0.7111\n",
      "Epoch 446/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5477 - acc: 0.8118 - val_loss: 0.8045 - val_acc: 0.7222\n",
      "Epoch 447/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5485 - acc: 0.8392 - val_loss: 0.8041 - val_acc: 0.7222\n",
      "Epoch 448/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5479 - acc: 0.8353 - val_loss: 0.8052 - val_acc: 0.7222\n",
      "Epoch 449/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5400 - acc: 0.8392 - val_loss: 0.8059 - val_acc: 0.7222\n",
      "Epoch 450/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5142 - acc: 0.8594Epoch 00450: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5250 - acc: 0.8471 - val_loss: 0.8028 - val_acc: 0.7222\n",
      "Epoch 451/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5178 - acc: 0.8627 - val_loss: 0.8028 - val_acc: 0.7111\n",
      "Epoch 452/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5314 - acc: 0.8392 - val_loss: 0.8019 - val_acc: 0.7222\n",
      "Epoch 453/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5224 - acc: 0.8451 - val_loss: 0.8011 - val_acc: 0.7111\n",
      "Epoch 454/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5654 - acc: 0.8059 - val_loss: 0.8033 - val_acc: 0.7222\n",
      "Epoch 455/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5593 - acc: 0.8080Epoch 00455: val_loss improved from 0.80239 to 0.80218, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5549 - acc: 0.8157 - val_loss: 0.8022 - val_acc: 0.7333\n",
      "Epoch 456/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5170 - acc: 0.8490 - val_loss: 0.8001 - val_acc: 0.7333\n",
      "Epoch 457/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5266 - acc: 0.8314 - val_loss: 0.7946 - val_acc: 0.7333\n",
      "Epoch 458/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5246 - acc: 0.8451 - val_loss: 0.7925 - val_acc: 0.7333\n",
      "Epoch 459/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5217 - acc: 0.8510 - val_loss: 0.7934 - val_acc: 0.7333\n",
      "Epoch 460/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5266 - acc: 0.8371Epoch 00460: val_loss improved from 0.80218 to 0.79827, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5189 - acc: 0.8392 - val_loss: 0.7983 - val_acc: 0.7333\n",
      "Epoch 461/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5211 - acc: 0.8412 - val_loss: 0.7974 - val_acc: 0.7222\n",
      "Epoch 462/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5581 - acc: 0.8196 - val_loss: 0.7981 - val_acc: 0.7333\n",
      "Epoch 463/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5323 - acc: 0.8431 - val_loss: 0.7987 - val_acc: 0.7333\n",
      "Epoch 464/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5201 - acc: 0.8588 - val_loss: 0.8004 - val_acc: 0.7111\n",
      "Epoch 465/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5299 - acc: 0.8125Epoch 00465: val_loss improved from 0.79827 to 0.79187, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5276 - acc: 0.8216 - val_loss: 0.7919 - val_acc: 0.7333\n",
      "Epoch 466/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5483 - acc: 0.8196 - val_loss: 0.7931 - val_acc: 0.7333\n",
      "Epoch 467/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5059 - acc: 0.8490 - val_loss: 0.7931 - val_acc: 0.7333\n",
      "Epoch 468/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5344 - acc: 0.8510 - val_loss: 0.7933 - val_acc: 0.7222\n",
      "Epoch 469/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4924 - acc: 0.8431 - val_loss: 0.7934 - val_acc: 0.7222\n",
      "Epoch 470/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5375 - acc: 0.8214Epoch 00470: val_loss improved from 0.79187 to 0.78952, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5312 - acc: 0.8275 - val_loss: 0.7895 - val_acc: 0.7111\n",
      "Epoch 471/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5061 - acc: 0.8510 - val_loss: 0.7884 - val_acc: 0.7222\n",
      "Epoch 472/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5432 - acc: 0.8275 - val_loss: 0.7900 - val_acc: 0.7222\n",
      "Epoch 473/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5113 - acc: 0.8549 - val_loss: 0.7925 - val_acc: 0.7222\n",
      "Epoch 474/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5087 - acc: 0.8353 - val_loss: 0.7947 - val_acc: 0.7222\n",
      "Epoch 475/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5115 - acc: 0.8594Epoch 00475: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5054 - acc: 0.8569 - val_loss: 0.7896 - val_acc: 0.7111\n",
      "Epoch 476/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4932 - acc: 0.8510 - val_loss: 0.7927 - val_acc: 0.7000\n",
      "Epoch 477/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5095 - acc: 0.8392 - val_loss: 0.7931 - val_acc: 0.7111\n",
      "Epoch 478/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4947 - acc: 0.8412 - val_loss: 0.7940 - val_acc: 0.7222\n",
      "Epoch 479/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4944 - acc: 0.8569 - val_loss: 0.7921 - val_acc: 0.7222\n",
      "Epoch 480/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5224 - acc: 0.8460Epoch 00480: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5129 - acc: 0.8529 - val_loss: 0.7913 - val_acc: 0.7222\n",
      "Epoch 481/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5160 - acc: 0.8471 - val_loss: 0.7927 - val_acc: 0.7222\n",
      "Epoch 482/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4947 - acc: 0.8627 - val_loss: 0.7918 - val_acc: 0.7111\n",
      "Epoch 483/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5013 - acc: 0.8451 - val_loss: 0.7890 - val_acc: 0.7222\n",
      "Epoch 484/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5045 - acc: 0.8412 - val_loss: 0.7882 - val_acc: 0.7222\n",
      "Epoch 485/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5032 - acc: 0.8482Epoch 00485: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5047 - acc: 0.8451 - val_loss: 0.7899 - val_acc: 0.7222\n",
      "Epoch 486/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5151 - acc: 0.8353 - val_loss: 0.7875 - val_acc: 0.7222\n",
      "Epoch 487/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4890 - acc: 0.8608 - val_loss: 0.7853 - val_acc: 0.7222\n",
      "Epoch 488/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4787 - acc: 0.8686 - val_loss: 0.7835 - val_acc: 0.7222\n",
      "Epoch 489/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5000 - acc: 0.8353 - val_loss: 0.7800 - val_acc: 0.7222\n",
      "Epoch 490/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5201 - acc: 0.8304Epoch 00490: val_loss improved from 0.78952 to 0.78180, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5135 - acc: 0.8275 - val_loss: 0.7818 - val_acc: 0.7333\n",
      "Epoch 491/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5109 - acc: 0.8431 - val_loss: 0.7821 - val_acc: 0.7444\n",
      "Epoch 492/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4957 - acc: 0.8588 - val_loss: 0.7843 - val_acc: 0.7333\n",
      "Epoch 493/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4986 - acc: 0.8549 - val_loss: 0.7855 - val_acc: 0.7222\n",
      "Epoch 494/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4711 - acc: 0.8686 - val_loss: 0.7852 - val_acc: 0.7222\n",
      "Epoch 495/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4933 - acc: 0.8504Epoch 00495: val_loss did not improve\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4951 - acc: 0.8490 - val_loss: 0.7851 - val_acc: 0.7222\n",
      "Epoch 496/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4621 - acc: 0.8745 - val_loss: 0.7839 - val_acc: 0.7222\n",
      "Epoch 497/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4902 - acc: 0.8627 - val_loss: 0.7818 - val_acc: 0.7222\n",
      "Epoch 498/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4982 - acc: 0.8510 - val_loss: 0.7810 - val_acc: 0.7222\n",
      "Epoch 499/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5008 - acc: 0.8333 - val_loss: 0.7808 - val_acc: 0.7222\n",
      "Epoch 500/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4626 - acc: 0.8638Epoch 00500: val_loss improved from 0.78180 to 0.77922, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4779 - acc: 0.8510 - val_loss: 0.7792 - val_acc: 0.7222\n",
      "Epoch 501/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5062 - acc: 0.8667 - val_loss: 0.7813 - val_acc: 0.7222\n",
      "Epoch 502/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4932 - acc: 0.8647 - val_loss: 0.7795 - val_acc: 0.7333\n",
      "Epoch 503/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4954 - acc: 0.8412 - val_loss: 0.7792 - val_acc: 0.7333\n",
      "Epoch 504/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.5156 - acc: 0.8549 - val_loss: 0.7788 - val_acc: 0.7333\n",
      "Epoch 505/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5007 - acc: 0.8549Epoch 00505: val_loss improved from 0.77922 to 0.77777, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5000 - acc: 0.8529 - val_loss: 0.7778 - val_acc: 0.7333\n",
      "Epoch 506/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4792 - acc: 0.8667 - val_loss: 0.7777 - val_acc: 0.7333\n",
      "Epoch 507/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4655 - acc: 0.8647 - val_loss: 0.7755 - val_acc: 0.7222\n",
      "Epoch 508/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4669 - acc: 0.8667 - val_loss: 0.7797 - val_acc: 0.7222\n",
      "Epoch 509/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4827 - acc: 0.8471 - val_loss: 0.7794 - val_acc: 0.7222\n",
      "Epoch 510/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4543 - acc: 0.8862Epoch 00510: val_loss improved from 0.77777 to 0.77620, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4605 - acc: 0.8784 - val_loss: 0.7762 - val_acc: 0.7111\n",
      "Epoch 511/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4895 - acc: 0.8647 - val_loss: 0.7763 - val_acc: 0.7111\n",
      "Epoch 512/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4930 - acc: 0.8627 - val_loss: 0.7794 - val_acc: 0.7111\n",
      "Epoch 513/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4455 - acc: 0.8765 - val_loss: 0.7745 - val_acc: 0.7222\n",
      "Epoch 514/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5181 - acc: 0.8196 - val_loss: 0.7696 - val_acc: 0.7222\n",
      "Epoch 515/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4907 - acc: 0.8460Epoch 00515: val_loss improved from 0.77620 to 0.77181, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4815 - acc: 0.8529 - val_loss: 0.7718 - val_acc: 0.7111\n",
      "Epoch 516/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4752 - acc: 0.8706 - val_loss: 0.7735 - val_acc: 0.7111\n",
      "Epoch 517/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4841 - acc: 0.8412 - val_loss: 0.7739 - val_acc: 0.7111\n",
      "Epoch 518/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4831 - acc: 0.8627 - val_loss: 0.7695 - val_acc: 0.7333\n",
      "Epoch 519/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4900 - acc: 0.8490 - val_loss: 0.7640 - val_acc: 0.7333\n",
      "Epoch 520/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4548 - acc: 0.8772Epoch 00520: val_loss improved from 0.77181 to 0.76698, saving model to top_weight.h5\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4646 - acc: 0.8706 - val_loss: 0.7670 - val_acc: 0.7222\n",
      "Epoch 521/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4720 - acc: 0.8667 - val_loss: 0.7692 - val_acc: 0.7333\n",
      "Epoch 522/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4538 - acc: 0.8765 - val_loss: 0.7687 - val_acc: 0.7222\n",
      "Epoch 523/3000\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4616 - acc: 0.8784 - val_loss: 0.7666 - val_acc: 0.7111\n",
      "Epoch 524/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4685 - acc: 0.8608 - val_loss: 0.7691 - val_acc: 0.7000\n",
      "Epoch 525/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4786 - acc: 0.8571Epoch 00525: val_loss improved from 0.76698 to 0.76662, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4686 - acc: 0.8647 - val_loss: 0.7666 - val_acc: 0.7111\n",
      "Epoch 526/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4730 - acc: 0.8745 - val_loss: 0.7651 - val_acc: 0.7222\n",
      "Epoch 527/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4870 - acc: 0.8490 - val_loss: 0.7637 - val_acc: 0.7222\n",
      "Epoch 528/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4705 - acc: 0.8490 - val_loss: 0.7683 - val_acc: 0.7111\n",
      "Epoch 529/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4602 - acc: 0.8627 - val_loss: 0.7689 - val_acc: 0.7111\n",
      "Epoch 530/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4722 - acc: 0.8504Epoch 00530: val_loss did not improve\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4652 - acc: 0.8471 - val_loss: 0.7736 - val_acc: 0.7000\n",
      "Epoch 531/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4668 - acc: 0.8490 - val_loss: 0.7651 - val_acc: 0.7222\n",
      "Epoch 532/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4720 - acc: 0.8529 - val_loss: 0.7637 - val_acc: 0.7333\n",
      "Epoch 533/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4700 - acc: 0.8529 - val_loss: 0.7667 - val_acc: 0.7333\n",
      "Epoch 534/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4505 - acc: 0.8686 - val_loss: 0.7640 - val_acc: 0.7444\n",
      "Epoch 535/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4806 - acc: 0.8616Epoch 00535: val_loss improved from 0.76662 to 0.76373, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4805 - acc: 0.8647 - val_loss: 0.7637 - val_acc: 0.7444\n",
      "Epoch 536/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.4938 - acc: 0.8294 - val_loss: 0.7622 - val_acc: 0.7444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 537/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4734 - acc: 0.8627 - val_loss: 0.7668 - val_acc: 0.7333\n",
      "Epoch 538/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4951 - acc: 0.8412 - val_loss: 0.7687 - val_acc: 0.7111\n",
      "Epoch 539/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4755 - acc: 0.8569 - val_loss: 0.7585 - val_acc: 0.7444\n",
      "Epoch 540/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4791 - acc: 0.8571Epoch 00540: val_loss improved from 0.76373 to 0.75644, saving model to top_weight.h5\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4699 - acc: 0.8569 - val_loss: 0.7564 - val_acc: 0.7444\n",
      "Epoch 541/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4703 - acc: 0.8529 - val_loss: 0.7576 - val_acc: 0.7222\n",
      "Epoch 542/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4583 - acc: 0.8843 - val_loss: 0.7577 - val_acc: 0.7111\n",
      "Epoch 543/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4525 - acc: 0.8725 - val_loss: 0.7633 - val_acc: 0.7111\n",
      "Epoch 544/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.4803 - acc: 0.8431 - val_loss: 0.7641 - val_acc: 0.7333\n",
      "Epoch 545/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4615 - acc: 0.8705Epoch 00545: val_loss did not improve\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4548 - acc: 0.8804 - val_loss: 0.7589 - val_acc: 0.7444\n",
      "Epoch 546/3000\n",
      "510/510 [==============================] - 2s 5ms/step - loss: 0.4614 - acc: 0.8569 - val_loss: 0.7557 - val_acc: 0.7667\n",
      "Epoch 547/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4682 - acc: 0.8471 - val_loss: 0.7582 - val_acc: 0.7333\n",
      "Epoch 548/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4469 - acc: 0.8549 - val_loss: 0.7587 - val_acc: 0.7444\n",
      "Epoch 549/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4759 - acc: 0.8569 - val_loss: 0.7612 - val_acc: 0.7444\n",
      "Epoch 550/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4452 - acc: 0.8728Epoch 00550: val_loss did not improve\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.4308 - acc: 0.8765 - val_loss: 0.7656 - val_acc: 0.7333\n",
      "Epoch 551/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4397 - acc: 0.8784 - val_loss: 0.7625 - val_acc: 0.7222\n",
      "Epoch 552/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4714 - acc: 0.8647 - val_loss: 0.7637 - val_acc: 0.7556\n",
      "Epoch 553/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4742 - acc: 0.8451 - val_loss: 0.7625 - val_acc: 0.7333\n",
      "Epoch 554/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4578 - acc: 0.8451 - val_loss: 0.7662 - val_acc: 0.7111\n",
      "Epoch 555/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4339 - acc: 0.8795Epoch 00555: val_loss did not improve\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4430 - acc: 0.8725 - val_loss: 0.7659 - val_acc: 0.7111\n",
      "Epoch 556/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4355 - acc: 0.8784 - val_loss: 0.7702 - val_acc: 0.7222\n",
      "Epoch 557/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4675 - acc: 0.8510 - val_loss: 0.7651 - val_acc: 0.7222\n",
      "Epoch 558/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4316 - acc: 0.8745 - val_loss: 0.7649 - val_acc: 0.7333\n",
      "Epoch 559/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4362 - acc: 0.8706 - val_loss: 0.7638 - val_acc: 0.7222\n",
      "Epoch 560/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4705 - acc: 0.8326Epoch 00560: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4607 - acc: 0.8451 - val_loss: 0.7616 - val_acc: 0.7222\n",
      "Epoch 561/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4521 - acc: 0.8549 - val_loss: 0.7622 - val_acc: 0.7222\n",
      "Epoch 562/3000\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4610 - acc: 0.8588 - val_loss: 0.7636 - val_acc: 0.7444\n",
      "Epoch 563/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4613 - acc: 0.8725 - val_loss: 0.7619 - val_acc: 0.7333\n",
      "Epoch 564/3000\n",
      "510/510 [==============================] - 1s 2ms/step - loss: 0.4430 - acc: 0.8706 - val_loss: 0.7599 - val_acc: 0.7333\n",
      "Epoch 565/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4506 - acc: 0.8661Epoch 00565: val_loss did not improve\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4554 - acc: 0.8686 - val_loss: 0.7591 - val_acc: 0.7333\n",
      "Epoch 566/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.4508 - acc: 0.8784 - val_loss: 0.7612 - val_acc: 0.7333\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(patience=20, mode='auto', monitor='val_loss')\n",
    "checkPoint = ModelCheckpoint(filepath=\"top_weight.h5\", verbose=1, save_best_only=True, save_weights_only=True, period=5)\n",
    "Logs = CSVLogger('logs.csv', separator=',', append=True)\n",
    "history = model.fit(X_train, y_train, epochs=3000, batch_size=64, shuffle=True, validation_split=0.15, callbacks=[checkPoint, Logs, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKzg/wGANv3NaIWAagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f667c08ba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65f01a7850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting 了 QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 1s 4ms/step\n",
      "loss: 0.70\n",
      "acc: 75.38%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print 'loss: %.2f'%(loss)\n",
    "print 'acc: %.2f%%'%(acc*100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  3  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  6  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  7  2  1  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  8  3  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  2 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  4  3  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  2  6  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  3 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  3  9  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0 12]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.80      0.86        15\n",
      "          1       0.67      0.60      0.63        10\n",
      "          2       0.73      1.00      0.84         8\n",
      "          3       1.00      0.64      0.78        11\n",
      "          4       0.67      0.73      0.70        11\n",
      "          5       0.71      0.83      0.77        12\n",
      "          6       0.67      0.50      0.57         8\n",
      "          7       0.46      0.60      0.52        10\n",
      "          8       0.80      0.80      0.80        15\n",
      "          9       0.50      1.00      0.67         4\n",
      "         10       1.00      0.69      0.82        13\n",
      "         11       0.92      0.92      0.92        13\n",
      "\n",
      "avg / total       0.79      0.75      0.76       130\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD8CAYAAAD5TVjyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFNlJREFUeJzt3X+sX3V9x/Hni7YUi1J+1BFo69qkzKXoNvQKKNM5KlKU\nUZPBBKNjhqQuEX8vDvYHMqKJLAYkkZl1FEQk/EiF2GBHVZjRGamUH0NKIdyB0ltAKGAFHLS997U/\nvqfZ995v772H+z293/M99/VITnq+53u+n/OmhDef30e2iYhoggN6HUBERFWS0CKiMZLQIqIxktAi\nojGS0CKiMZLQIqIxktAiojGS0CKiMZLQIqIxZk/nwxYcPstLFs/pupyHn3hjBdHAAb99uZJyIuri\nFV5ml19VN2Wc+pcH+7nnh0vde88Dr260vbKb51VpWhPaksVz+MXGxV2X8+5PfqKCaGDerZsqKSei\nLjb5jq7LeO75YX6x8U2l7p111KMLun5ghaY1oUVE/RkYYaTXYUxJElpEjGLMbpdrctZNElpEdEgN\nLSIawZjhPt1WrKtpG5JWSnpE0qCkC6oKKiJ6awSXOupmyjU0SbOAK4FTgCHgbknrbT9UVXARMf0M\nDNcwWZXRTQ3teGDQ9mO2dwE3AquqCSsiemnG1dCAhcC2ts9DwAljb5K0GlgN8KaF6bKLqDsDu2di\nH1oZttfYHrA98MYjZu3vx0VEl4wZLnnUTTdVpu1A+7T/RcW1iOhnhuH65apSuqmh3Q0cI2mppAOB\ns4H11YQVEb3SWilQ7qibKdfQbO+RdD6wEZgFXG17S2WRRUSPiGG6Wt/eM1310tveAGyoKJaIqIHW\noMAMTGgR0TyteWhJaBHRECOpoUVEE6SGVtKjD8/ngyec3nU5T36+mr/sozvnAU9JNoqMJjFiuE93\n508NLSI6pMkZEY1gxC7356qeJLSIGKU1sTZNzohoiAwKREQj2GLYqaFFREOMpIYWEU3QGhToz9TQ\nn/XKiNhv9g4KlDnKmOzdI5LmSrqp+H6TpCXF9TmSrpX0S0lbJV042bOS0CKiw7BV6phM27tHTgOW\nA+dIWj7mtvOAF2wvAy4HLi2unwXMtf1W4O3AJ/Ymu/EkoUXEKHtXCpQ5Sijz7pFVwLXF+TpghSTR\nqiweLGk28DpgF/C7iR6WhBYRHUZ8QKkDWCBpc9uxekxR+3r3yMLx7rG9B9gJHEErub0MPAU8AXzN\n9vMTxd2fPX8Rsd+0FqeXruvssD2wn0I5HhgGjgYOA34q6Ue2HxvvB0loETGKEburW/pU5t0je+8Z\nKpqX84HngI8At9veDTwj6WfAADBuQkuTMyJGsWHYB5Q6Sijz7pH1wLnF+ZnAnbZNq5l5MoCkg4ET\ngYcnelhqaBExhiqbWDveu0ckXQJstr0eWAtcJ2kQeJ5W0oPW6Og1krYAAq6x/cBEz0tCi4hRDJUu\nfdrXu0dsX9R2/gqtKRpjf/fSvq5PJAktIjpkg8cSvGs3e7YNdV3Oss91XwbAzg3LKimHW6spJqIO\njLLBY0Q0Q+s1dv2ZGvoz6ojYj2boi4YjonkMe1cB9J0ktIjokBpaRDSCrZlXQ5O0GPg2cCStWuoa\n21dUFVhE9EZrUGDmvfVpD/AF2/dKegNwj6Qf2n6ootgioidm4DsFbD9Fa1sPbL8oaSutbUCS0CL6\nWGtQYAb3oRW7SB4HbKqivIjorRm7UkDS64HvAp+13bGbZLHh22qAg5jX7eMiYj+bsSsFJM2hlcyu\nt33Lvu6xvQZYA3CIDnc3z4uI6THj3pxe7Pm9Fthq+7LqQoqIXrJh90h/JrRuoj4J+BhwsqT7i+MD\nFcUVET3SanKWfqdArXQzyvlf0KfTiSNiQlkpEBGNMOOnbUREk8zApU8R0VxVvVNgus3ohDb/A4OV\nlDN43XGVlAOw7MrhSsqZvX3C97GWVsUOwwCzFy+qpJyq4onxtUY5Z95azohooBk7sTYimilNzoho\nhIxyRkSjZJQzIhrBFnuS0CKiKdLkjIhGSB9aRDRKElpENELmoUVEo2QeWkQ0gg17+nSDxyS0iOiQ\nJmdENEL60CKiUZyEFhFNkUGBiGgEO31oEdEYYjijnBHRFOlDm8GWfey+ysrauWFZJeXM+feFlZQz\nr6Itr7N1dv+oei2npJXAFcAs4CrbXx3z/Vzg28DbgeeAD9v+VfHdnwD/BhwCjADvsP3KeM/qz3pl\nROw/bvWjlTkmI2kWcCVwGrAcOEfS8jG3nQe8YHsZcDlwafHb2cB3gL+3fSzwXmD3RM9LQouIDiOo\n1FHC8cCg7cds7wJuBFaNuWcVcG1xvg5YIUnA+4EHbP83gO3nbE/4FqEktIgYxcWgQJkDWCBpc9ux\nekxxC4FtbZ+Himv7vMf2HmAncATwR4AlbZR0r6QvThZ7+tAiokOZ5mRhh+2B/RTGbODPgXcAvwfu\nkHSP7TvG+0HXNTRJsyTdJ+m2bsuKiHqwVeooYTuwuO3zouLaPu8p+s3m0xocGAJ+YnuH7d8DG4C3\nTfSwKpqcnwG2VlBORNRAq8O/soR2N3CMpKWSDgTOBtaPuWc9cG5xfiZwp20DG4G3SppXJLq/AB6a\n6GFdJTRJi4APAld1U05E1MuIVeqYTNEndj6t5LQVuNn2FkmXSDqjuG0tcISkQeDzwAXFb18ALqOV\nFO8H7rX9/Yme120f2teBLwJv6LKciKiR19CHVqIsb6DVXGy/dlHb+SvAWeP89ju0pm6UMuUamqTT\ngWds3zPJfav3joDs5tWpPi4ipokRIyMHlDrqppuITgLOkPQrWnNLTpbUkUltr7E9YHtgDnO7eFxE\nTBeXPOpmygnN9oW2F9leQquj707bH60ssojojWoHBaZV5qFFRKc6Vr9KqCSh2f4x8OMqyoqI3qtj\n7auM1NAiYhQDIyNJaBHRBAZSQ4uIpqhyHtp0SkKLiE5JaFGF+ZfMq6Sc09feXkk5t916WCXlRD+p\n55SMMpLQIqJTamgR0QgGZ5QzIpojCS0imiJNzohojCS0iGiETKyNiCbJxNqIaI6MckZEUyg1tIho\nhLpuR1tCElpEjKEMCkREg6SGFhGNMdLrAKYmCS0iRss8tIhokoxyRkRz9GlCq9+rjyMipig1tLq5\n64FKirnt2Gp2mt2x+p2VlDPv2Wp6mQ/ZvL2ScvZsG6qknKZKkzMimsFk6VNENEhqaBHRFGlyRkRz\n9GlC62qUU9KhktZJeljSVknV9CBHRG+55FEz3dbQrgBut32mpAOBal4qGRE9I8/AJqek+cB7gL8D\nsL0L2FVNWBHRU306ytlNk3Mp8CxwjaT7JF0l6eCK4oqIHtpbS5vsqJtuEtps4G3AN20fB7wMXDD2\nJkmrJW2WtHk3r3bxuIiYNn3ah9ZNQhsChmxvKj6vo5XgRrG9xvaA7YE5zO3icRExLUrWzhpVQ7P9\nNLBN0puLSyuAhyqJKiJ6q8IamqSVkh6RNChpX624uZJuKr7fJGnJmO/fJOklSf8w2bO6HeX8FHB9\nMcL5GPDxLsuLiBpQRRs8SpoFXAmcQqtVd7ek9bbbKz/nAS/YXibpbOBS4MNt318G/EeZ53WV0Gzf\nDwx0U0ZENNrxwKDtxwAk3QisYnRrbhVwcXG+DviGJNm2pA8Bj9Pqo59Utg+KiE7VNTkXAtvaPg8V\n1/Z5j+09wE7gCEmvB/4R+OeyYWfpU0SM9to6/BdI2tz2eY3tNRVFcjFwue2XpHLz4pLQIqJT+YS2\nw/ZE3U7bgcVtnxcV1/Z1z5Ck2cB84DngBOBMSf8CHAqMSHrF9jfGe1gSWkR0qm5Kxt3AMZKW0kpc\nZwMfGXPPeuBc4OfAmcCdtg28e+8Nki4GXpoomUESWkxiwQOl+mInNfjhapb5znvq8ErKITvWjktU\nN8ppe4+k84GNwCzgattbJF0CbLa9HlgLXCdpEHieVtKbkiS0iBit4kmztjcAG8Zcu6jt/BXgrEnK\nuLjMs5LQIqJTDVcBlJGEFhGdktAioinquE6zjCS0iOiUhBYRjeDqRjmnWxJaRHRKDS0imiJ9aBHR\nHEloEdEINd1eu4wktIgYRaTJGRENkoQWEc2RhBYRjZGEFhGNUNNX1JWRhBYRnZLQIqIpsvQpmumu\nByop5uijTqiknI23fLuSck49+s8qKaep0uSMiGbIxNqIaJQktIhogqwUiIhG0Uh/ZrQktIgYrY/7\n0A7o5seSPidpi6QHJd0g6aCqAouI3pHLHXUz5YQmaSHwaWDA9ltovUR0yi8IjYgaccmjZrptcs4G\nXidpNzAPeLL7kCKi1+pY+ypjyjU029uBrwFPAE8BO23/YOx9klZL2ixp825enXqkETF9+rSG1k2T\n8zBgFbAUOBo4WNJHx95ne43tAdsDc5g79UgjYnoUb30qc9RNN4MC7wMet/2s7d3ALcC7qgkrInpl\n7zy0fhwU6KYP7QngREnzgP8FVgCbK4kqInrLNcxWJUw5odneJGkdcC+wB7gPWFNVYBHRO3WsfZXR\n1Sin7S8BX6ooloiog5p2+JeRlQIR0aGOHf5lJKFFRIcktIhoBjPzBgUiXot5t26qpJxTb61mp9kd\nq99ZSTkAC9b8vLKy6mJGDgpEREMloUVEE2SDx4hoDrtvN3jsaj+0iGioChenS1op6RFJg5Iu2Mf3\ncyXdVHy/SdKS4vopku6R9Mviz5Mne1YSWkR0qGotp6RZwJXAacBy4BxJy8fcdh7wgu1lwOXApcX1\nHcBf2X4rcC5w3WTPS0KLiNEMjLjcMbnjgUHbj9neBdxIa5eedquAa4vzdcAKSbJ9n+29eyxuobX3\n4oRb9iShRUSn6pqcC4FtbZ+Himv7vMf2HmAncMSYe/4auNf2hJsqZlAgIjq8hlHOBZLad9lZY7vS\nTSokHUurGfr+ye5NQouIDq9hlHOH7YEJvt8OLG77vKi4tq97hiTNBuYDzwFIWgTcCvyt7f+ZLJg0\nOSNitLLNzXI5727gGElLJR1I60VK68fcs55Wpz/AmcCdti3pUOD7wAW2f1bmYUloETFKa2KtSx2T\nKfrEzgc2AluBm21vkXSJpDOK29YCR0gaBD4P7J3acT6wDLhI0v3F8QcTPS9NzojoVOFuG7Y3ABvG\nXLuo7fwV4Kx9/O7LwJdfy7OS0CKiQ5naVx0loUXEaNmxNiKao3/XciahRUSnNDkjohGcLbgjoklS\nQ4voH/Oera4KcvRdb6iknGfOmt91GXp6TgWRkEGBiGgOjfRnmzMJLSJGM5VOrJ1OSWgRMYoot6yp\njpLQIqJTnya0SRenS7pa0jOSHmy7drikH0p6tPjzsP0bZkRMK7vcUTNldtv4FrByzLULgDtsHwPc\nwf+vjo+Ifre3D63MUTOTJjTbPwGeH3O5fQ/wa4EPVRxXRPSQRkZKHXUz1T60I20/VZw/DRxZUTwR\n0XP1bE6W0fWgQLGz5Lj/9JJWA6sBDmJet4+LiP3N9G1Cm+qOtb+RdBRA8ecz491oe43tAdsDc5jw\nDVQRURdN7UMbR/se4OcC36smnIiog6q24J5uZaZt3AD8HHizpCFJ5wFfBU6R9CjwvuJzRDRFn07b\nmLQPzfY543y1ouJYIqIObBiuYXuyhKwUiIhONax9lZGEFhGdktAiohEM5J0CEdEMBqcPLaJvHLJ5\ne2VlPXnii5WUs/HJn3ZdxvGn7uw+EJNBgYhokPShRURjJKFFRDPUc9JsGUloETGagRpuDVRGElpE\ndEoNLSKaIUufIqIpDM48tIhojKwUiIjGSB9aRDSCnVHOiGiQ1NAiohmMh4d7HcSUJKFFxGh9vH3Q\nVF+SEhFN5pFyRwmSVkp6RNKgpAv28f1cSTcV32+StKTtuwuL649IOnWyZyWhRcQoBjziUsdkJM0C\nrgROA5YD50haPua284AXbC8DLgcuLX67HDgbOBZYCfxrUd64ktAiYjS7yhra8cCg7cds7wJuBFaN\nuWcVcG1xvg5YIUnF9Rttv2r7cWCwKG9c6UOLiA4VDgosBLa1fR4CThjvHtt7JO0Ejiiu3zXmtwsn\neti0JrQXeWHHj7zu15PctgDYMR3xlJR4Jle3mCaP54npCaRQ6u9n1lGVPOsPuy3gRV7Y+COvW1Dy\n9oMkbW77vMb2mm5jmKppTWi23zjZPZI22x6YjnjKSDyTq1tMiac7tldWWNx2YHHb50XFtX3dMyRp\nNjAfeK7kb0dJH1pE7E93A8dIWirpQFqd/OvH3LMeOLc4PxO407aL62cXo6BLgWOAX0z0sPShRcR+\nU/SJnQ9sBGYBV9veIukSYLPt9cBa4DpJg8DztJIexX03Aw8Be4BP2p6wc6+OCa1n7e9xJJ7J1S2m\nxFMjtjcAG8Zcu6jt/BXgrHF++xXgK2WfJffpmq2IiLHShxYRjVGbhDbZ8ogexLNY0n9KekjSFkmf\n6XVM0Jp5Lek+SbfVIJZDJa2T9LCkrZLe2eN4Plf8u3pQ0g2SDupBDFdLekbSg23XDpf0Q0mPFn8e\nNt1xzRS1SGgll0dMtz3AF2wvB04EPlmDmAA+A2ztdRCFK4Dbbf8x8Kf0MC5JC4FPAwO230KrA/rs\nHoTyLVrLdNpdANxh+xjgjuJz7Ae1SGiUWx4xrWw/Zfve4vxFWv+xTjhLeX+TtAj4IHBVL+MoYpkP\nvIfWCBW2d9n+bW+jYjbwumIu0zzgyekOwPZPaI3UtWtf2nMt8KFpDWoGqUtC29fyiJ4mj3bF6v/j\ngE29jYSvA18E6rCd6FLgWeCaogl8laSDexWM7e3A12itAXgK2Gn7B72KZ4wjbT9VnD8NHNnLYJqs\nLgmttiS9Hvgu8Fnbv+thHKcDz9i+p1cxjDEbeBvwTdvHAS/Tw6ZU0S+1ilaiPRo4WNJHexXPeIoJ\no5lasJ/UJaG95iUO00HSHFrJ7Hrbt/Q4nJOAMyT9ilaT/GRJ3+lhPEPAkO29tdZ1tBJcr7wPeNz2\ns7Z3A7cA7+phPO1+I+kogOLPZ3ocT2PVJaGVWR4xrYrtS9YCW21f1stYAGxfaHuR7SW0/n7utN2z\nGojtp4Ftkt5cXFpBa0Z3rzwBnChpXvHvbgX1GTxpX9pzLvC9HsbSaLVYKTDe8ogeh3US8DHgl5Lu\nL679UzHrOVo+BVxf/E/oMeDjvQrE9iZJ64B7aY1Q30cPZuhLugF4L7BA0hDwJeCrwM2SzgN+DfzN\ndMc1U2SlQEQ0Rl2anBERXUtCi4jGSEKLiMZIQouIxkhCi4jGSEKLiMZIQouIxkhCi4jG+D8TG1g9\nR8GtrwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f667c08b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "real = np.argmax(y_test, axis=1)\n",
    "confuse = np.asarray(confusion_matrix(real,pred))\n",
    "print confuse\n",
    "print classification_report(real,pred)\n",
    "confuse = confuse / float(confuse.sum())\n",
    "plt.imshow(confuse, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
