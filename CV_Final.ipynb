{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDNN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loadmat('./dataset.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__globals__', 'test_data', 'test_label', '__version__', 'train_label', 'train_data']\n"
     ]
    }
   ],
   "source": [
    "print list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 林敬翔 之銘言：\n",
    "跟助教要的 dataset\n",
    "\n",
    "第一維是數量 第二維是時間序 第三維是資料量\n",
    "\n",
    "Label 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ....\n",
    "\n",
    "1-10左手腕 11-20右手腕 21-30左手臂 31-40右手臂\n",
    "\n",
    "220代表frame數\n",
    "\n",
    "我是取一秒25frame\n",
    "\n",
    "動作數量是以動作為單位沒錯 1*220*40表是某一個動作的完整資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下資料維度\n",
    "\n",
    "data: (?, 220, 40) = (幾筆資料, 時間步, 資料維度)\n",
    "\n",
    "label: (?, 1) = (幾筆資料, 動作label)\n",
    "\n",
    "動作 label: 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: __header__\n",
      "not an array\n",
      "key: __globals__\n",
      "not an array\n",
      "key: test_data\n",
      "(790, 220, 40)\n",
      "key: test_label\n",
      "(790, 1)\n",
      "key: __version__\n",
      "not an array\n",
      "key: train_label\n",
      "(3300, 1)\n",
      "key: train_data\n",
      "(3300, 220, 40)\n"
     ]
    }
   ],
   "source": [
    "for key, value in data.iteritems():\n",
    "    print 'key: %s'%key\n",
    "    if hasattr(value , 'shape'):\n",
    "        print value.shape\n",
    "    else:\n",
    "        print 'not an array'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列資料 + 預測 -> RNN? / HMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Keras CuDNNLSTM\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "import keras\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Flatten, Conv1D, MaxPool1D\n",
    "from keras.layers import LSTM, CuDNNLSTM, RepeatVector, TimeDistributed, Bidirectional\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.utils import to_categorical # one-hot encoding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, actions=40, cell_size=128, lstm_layer_n = 3, conv_filters=64, \n",
    "                conv_kernel=7, pooling_step=2, learning_rate=0.01, dropout_r=0.14, \n",
    "                optimizer=keras.optimizers.RMSprop, clipnorm=1., \n",
    "                tLSTM=LSTM, use_temporal_subsampling=False, gaussian_noise_std=1., reg_l2=0.01):\n",
    "    if lstm_layer_n<1: raise ValueError('lstm_layer_n must >= 1')\n",
    "    optimizer = optimizer(lr=learning_rate, clipnorm=clipnorm)\n",
    "    model = Sequential()\n",
    "    model.add(GaussianNoise(gaussian_noise_std, input_shape=input_shape))\n",
    "    if use_temporal_subsampling:\n",
    "        model.add(Conv1D(filters=conv_filters, kernel_size=conv_kernel, padding='same', activation='relu'))\n",
    "        model.add(MaxPool1D(pool_size=pooling_step))\n",
    "        model.add(Dropout(dropout_r))\n",
    "    for _ in xrange(1, lstm_layer_n):\n",
    "        model.add(#Bidirectional(\n",
    "                                tLSTM(cell_size, return_sequences=True,\n",
    "                                      unit_forget_bias=True, recurrent_regularizer=regularizers.l2(reg_l2))#,\n",
    "                                #merge_mode='sum'\n",
    "                               )#)\n",
    "        model.add(Dropout(dropout_r))\n",
    "    model.add(#Bidirectional(\n",
    "                            tLSTM(cell_size, return_sequences=False, unit_forget_bias=True, \n",
    "                                  recurrent_regularizer=regularizers.l2(reg_l2))#,\n",
    "                            #merge_mode='sum'\n",
    "                           )#)\n",
    "    model.add(Dropout(dropout_r))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(dropout_r))\n",
    "    model.add(Dense(actions, activation='softmax'))\n",
    "    model.compile(\n",
    "            loss = 'categorical_crossentropy',\n",
    "            optimizer=optimizer, metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info for 0 pca: 1.00\n",
      "info for 1 pca: 1.00\n",
      "info for 2 pca: 1.00\n",
      "info for 3 pca: 1.00\n",
      "info for 4 pca: 1.00\n",
      "info for 5 pca: 1.00\n",
      "info for 6 pca: 1.00\n",
      "info for 7 pca: 1.00\n",
      "info for 8 pca: 1.00\n",
      "info for 9 pca: 1.00\n",
      "info for 10 pca: 1.00\n",
      "info for 11 pca: 1.00\n",
      "info for 12 pca: 1.00\n",
      "info for 13 pca: 1.00\n",
      "info for 14 pca: 1.00\n",
      "info for 15 pca: 1.00\n",
      "info for 16 pca: 1.00\n",
      "info for 17 pca: 1.00\n",
      "info for 18 pca: 1.00\n",
      "info for 19 pca: 1.00\n",
      "info for 20 pca: 1.00\n",
      "info for 21 pca: 1.00\n",
      "info for 22 pca: 1.00\n",
      "info for 23 pca: 1.00\n",
      "info for 24 pca: 1.00\n",
      "info for 25 pca: 1.00\n",
      "info for 26 pca: 1.00\n",
      "info for 27 pca: 1.00\n",
      "info for 28 pca: 1.00\n",
      "info for 29 pca: 1.00\n",
      "info for 30 pca: 1.00\n",
      "info for 31 pca: 1.00\n",
      "info for 32 pca: 1.00\n",
      "info for 33 pca: 1.00\n",
      "info for 34 pca: 1.00\n",
      "info for 35 pca: 1.00\n",
      "info for 36 pca: 1.00\n",
      "info for 37 pca: 1.00\n",
      "info for 38 pca: 1.00\n",
      "info for 39 pca: 1.00\n",
      "info for 40 pca: 1.00\n",
      "info for 41 pca: 1.00\n",
      "info for 42 pca: 1.00\n",
      "info for 43 pca: 1.00\n",
      "info for 44 pca: 1.00\n",
      "info for 45 pca: 1.00\n",
      "info for 46 pca: 1.00\n",
      "info for 47 pca: 1.00\n",
      "info for 48 pca: 1.00\n",
      "info for 49 pca: 1.00\n",
      "info for 50 pca: 1.00\n",
      "info for 51 pca: 1.00\n",
      "info for 52 pca: 1.00\n",
      "info for 53 pca: 1.00\n",
      "info for 54 pca: 1.00\n",
      "info for 55 pca: 1.00\n",
      "info for 56 pca: 1.00\n",
      "info for 57 pca: 1.00\n",
      "info for 58 pca: 1.00\n",
      "info for 59 pca: 1.00\n",
      "info for 60 pca: 1.00\n",
      "info for 61 pca: 1.00\n",
      "info for 62 pca: 1.00\n",
      "info for 63 pca: 1.00\n",
      "info for 64 pca: 1.00\n",
      "info for 65 pca: 1.00\n",
      "info for 66 pca: 1.00\n",
      "info for 67 pca: 1.00\n",
      "info for 68 pca: 1.00\n",
      "info for 69 pca: 1.00\n",
      "info for 70 pca: 1.00\n",
      "info for 71 pca: 1.00\n",
      "info for 72 pca: 1.00\n",
      "info for 73 pca: 1.00\n",
      "info for 74 pca: 1.00\n",
      "info for 75 pca: 1.00\n",
      "info for 76 pca: 1.00\n",
      "info for 77 pca: 1.00\n",
      "info for 78 pca: 1.00\n",
      "info for 79 pca: 1.00\n",
      "info for 80 pca: 1.00\n",
      "info for 81 pca: 1.00\n",
      "info for 82 pca: 1.00\n",
      "info for 83 pca: 1.00\n",
      "info for 84 pca: 1.00\n",
      "info for 85 pca: 1.00\n",
      "info for 86 pca: 1.00\n",
      "info for 87 pca: 1.00\n",
      "info for 88 pca: 1.00\n",
      "info for 89 pca: 1.00\n",
      "info for 90 pca: 1.00\n",
      "info for 91 pca: 1.00\n",
      "info for 92 pca: 1.00\n",
      "info for 93 pca: 1.00\n",
      "info for 94 pca: 1.00\n",
      "info for 95 pca: 1.00\n",
      "info for 96 pca: 1.00\n",
      "info for 97 pca: 1.00\n",
      "info for 98 pca: 1.00\n",
      "info for 99 pca: 1.00\n",
      "info for 100 pca: 1.00\n",
      "info for 101 pca: 1.00\n",
      "info for 102 pca: 1.00\n",
      "info for 103 pca: 1.00\n",
      "info for 104 pca: 1.00\n",
      "info for 105 pca: 1.00\n",
      "info for 106 pca: 1.00\n",
      "info for 107 pca: 1.00\n",
      "info for 108 pca: 1.00\n",
      "info for 109 pca: 1.00\n",
      "info for 110 pca: 1.00\n",
      "info for 111 pca: 1.00\n",
      "info for 112 pca: 1.00\n",
      "info for 113 pca: 1.00\n",
      "info for 114 pca: 1.00\n",
      "info for 115 pca: 1.00\n",
      "info for 116 pca: 1.00\n",
      "info for 117 pca: 1.00\n",
      "info for 118 pca: 1.00\n",
      "info for 119 pca: 1.00\n",
      "info for 120 pca: 1.00\n",
      "info for 121 pca: 1.00\n",
      "info for 122 pca: 1.00\n",
      "info for 123 pca: 1.00\n",
      "info for 124 pca: 1.00\n",
      "info for 125 pca: 1.00\n",
      "info for 126 pca: 1.00\n",
      "info for 127 pca: 1.00\n",
      "info for 128 pca: 1.00\n",
      "info for 129 pca: 1.00\n",
      "info for 130 pca: 1.00\n",
      "info for 131 pca: 1.00\n",
      "info for 132 pca: 1.00\n",
      "info for 133 pca: 1.00\n",
      "info for 134 pca: 1.00\n",
      "info for 135 pca: 1.00\n",
      "info for 136 pca: 1.00\n",
      "info for 137 pca: 1.00\n",
      "info for 138 pca: 1.00\n",
      "info for 139 pca: 1.00\n",
      "info for 140 pca: 1.00\n",
      "info for 141 pca: 1.00\n",
      "info for 142 pca: 1.00\n",
      "info for 143 pca: 1.00\n",
      "info for 144 pca: 1.00\n",
      "info for 145 pca: 1.00\n",
      "info for 146 pca: 1.00\n",
      "info for 147 pca: 1.00\n",
      "info for 148 pca: 1.00\n",
      "info for 149 pca: 1.00\n",
      "info for 150 pca: 1.00\n",
      "info for 151 pca: 1.00\n",
      "info for 152 pca: 1.00\n",
      "info for 153 pca: 1.00\n",
      "info for 154 pca: 1.00\n",
      "info for 155 pca: 1.00\n",
      "info for 156 pca: 1.00\n",
      "info for 157 pca: 1.00\n",
      "info for 158 pca: 1.00\n",
      "info for 159 pca: 1.00\n",
      "info for 160 pca: 1.00\n",
      "info for 161 pca: 1.00\n",
      "info for 162 pca: 1.00\n",
      "info for 163 pca: 1.00\n",
      "info for 164 pca: 1.00\n",
      "info for 165 pca: 1.00\n",
      "info for 166 pca: 1.00\n",
      "info for 167 pca: 1.00\n",
      "info for 168 pca: 1.00\n",
      "info for 169 pca: 1.00\n",
      "info for 170 pca: 1.00\n",
      "info for 171 pca: 1.00\n",
      "info for 172 pca: 1.00\n",
      "info for 173 pca: 1.00\n",
      "info for 174 pca: 1.00\n",
      "info for 175 pca: 1.00\n",
      "info for 176 pca: 1.00\n",
      "info for 177 pca: 1.00\n",
      "info for 178 pca: 1.00\n",
      "info for 179 pca: 1.00\n",
      "info for 180 pca: 1.00\n",
      "info for 181 pca: 1.00\n",
      "info for 182 pca: 1.00\n",
      "info for 183 pca: 1.00\n",
      "info for 184 pca: 1.00\n",
      "info for 185 pca: 1.00\n",
      "info for 186 pca: 1.00\n",
      "info for 187 pca: 1.00\n",
      "info for 188 pca: 1.00\n",
      "info for 189 pca: 1.00\n",
      "info for 190 pca: 1.00\n",
      "info for 191 pca: 1.00\n",
      "info for 192 pca: 1.00\n",
      "info for 193 pca: 1.00\n",
      "info for 194 pca: 1.00\n",
      "info for 195 pca: 1.00\n",
      "info for 196 pca: 1.00\n",
      "info for 197 pca: 1.00\n",
      "info for 198 pca: 1.00\n",
      "info for 199 pca: 1.00\n",
      "info for 200 pca: 1.00\n",
      "info for 201 pca: 1.00\n",
      "info for 202 pca: 1.00\n",
      "info for 203 pca: 1.00\n",
      "info for 204 pca: 1.00\n",
      "info for 205 pca: 1.00\n",
      "info for 206 pca: 1.00\n",
      "info for 207 pca: 1.00\n",
      "info for 208 pca: 1.00\n",
      "info for 209 pca: 1.00\n",
      "info for 210 pca: 1.00\n",
      "info for 211 pca: 1.00\n",
      "info for 212 pca: 1.00\n",
      "info for 213 pca: 1.00\n",
      "info for 214 pca: 1.00\n",
      "info for 215 pca: 1.00\n",
      "info for 216 pca: 1.00\n",
      "info for 217 pca: 1.00\n",
      "info for 218 pca: 1.00\n",
      "info for 219 pca: 1.00\n",
      "(3300, 220, 30)\n",
      "(790, 220, 30)\n",
      "(3300, 12)\n",
      "(790, 12)\n",
      "49.25015918\n",
      "5.01249722458e-09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = data['train_data'], data['test_data'], data['train_label'], data['test_label']\n",
    "label_enc = OHE(sparse=False) # One-hot encoder, which has attribute [transform, inverse_transform]\n",
    "y_test = label_enc.fit_transform(y_test)\n",
    "y_train = label_enc.transform(y_train)\n",
    "\n",
    "pcas = []\n",
    "n_comp = 30\n",
    "X_train_new = np.zeros((X_train.shape[0], X_train.shape[1], n_comp))\n",
    "X_test_new = np.zeros((X_test.shape[0], X_test.shape[1], n_comp))\n",
    "for i in xrange(X_train.shape[-2]):\n",
    "    pca = PCA(n_components=n_comp).fit(X_train[:,i,:])\n",
    "    pcas.append( (pca, np.sum(pca.explained_variance_ratio_)) )\n",
    "    X_train_new[:,i,:] = pcas[-1][0].transform(X_train[:,i,:])\n",
    "    X_test_new[:,i,:] = pcas[-1][0].transform(X_test[:,i,:])\n",
    "\n",
    "X_train = X_train_new\n",
    "X_test = X_test_new\n",
    "del X_train_new\n",
    "del X_test_new\n",
    "\n",
    "for i, pca in enumerate(pcas):\n",
    "    print 'info for %d pca: %.2f'%(i,pca[1])\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape\n",
    "print X_train.std()\n",
    "print np.abs(X_train).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pcas.pkl','wb') as pak:\n",
    "    pickle.dump(pcas, pak, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_1 (GaussianNo (None, 220, 30)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 220, 256)          294912    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 220, 256)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 256)               526336    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 855,692\n",
      "Trainable params: 855,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(input_shape=X_test.shape[1:] ,actions=y_test.shape[-1], cell_size=256, lstm_layer_n=2,\n",
    "                    learning_rate=0.0001, dropout_r=0.5,\n",
    "                    tLSTM=CuDNNLSTM if USE_CUDNN else LSTM, use_temporal_subsampling=False,\n",
    "                    gaussian_noise_std=1e-7, reg_l2=0.5\n",
    "                   )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2970 samples, validate on 330 samples\n",
      "Epoch 1/3000\n",
      "2970/2970 [==============================] - 7s 2ms/step - loss: 221.9961 - acc: 0.1758 - val_loss: 193.7374 - val_acc: 0.3485\n",
      "Epoch 2/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 171.8499 - acc: 0.2663 - val_loss: 150.1989 - val_acc: 0.4030\n",
      "Epoch 3/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 132.7582 - acc: 0.3094 - val_loss: 115.1850 - val_acc: 0.4485\n",
      "Epoch 4/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 101.0213 - acc: 0.3431 - val_loss: 87.0176 - val_acc: 0.4788\n",
      "Epoch 5/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 76.3668 - acc: 0.3702Epoch 00005: val_loss improved from inf to 65.30662, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 76.2737 - acc: 0.3704 - val_loss: 65.3066 - val_acc: 0.4909\n",
      "Epoch 6/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 56.9387 - acc: 0.3650 - val_loss: 48.4102 - val_acc: 0.4939\n",
      "Epoch 7/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 41.8925 - acc: 0.4098 - val_loss: 35.4407 - val_acc: 0.5030\n",
      "Epoch 8/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 30.6588 - acc: 0.4212 - val_loss: 25.7076 - val_acc: 0.5000\n",
      "Epoch 9/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 22.3559 - acc: 0.4367 - val_loss: 18.7963 - val_acc: 0.5212\n",
      "Epoch 10/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 16.2606 - acc: 0.4524Epoch 00010: val_loss improved from 65.30662 to 13.58934, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 16.2390 - acc: 0.4525 - val_loss: 13.5893 - val_acc: 0.5333\n",
      "Epoch 11/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 11.7906 - acc: 0.4586 - val_loss: 9.7479 - val_acc: 0.5485\n",
      "Epoch 12/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 8.4602 - acc: 0.4852 - val_loss: 7.0096 - val_acc: 0.5455\n",
      "Epoch 13/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 6.1295 - acc: 0.5030 - val_loss: 5.0897 - val_acc: 0.5636\n",
      "Epoch 14/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 4.5587 - acc: 0.5047 - val_loss: 3.7854 - val_acc: 0.5667\n",
      "Epoch 15/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 3.4388 - acc: 0.5282Epoch 00015: val_loss improved from 13.58934 to 2.86710, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 3.4348 - acc: 0.5273 - val_loss: 2.8671 - val_acc: 0.5788\n",
      "Epoch 16/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 2.6248 - acc: 0.5316 - val_loss: 2.2299 - val_acc: 0.5879\n",
      "Epoch 17/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 2.0757 - acc: 0.5519 - val_loss: 1.7947 - val_acc: 0.5758\n",
      "Epoch 18/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 1.7415 - acc: 0.5542 - val_loss: 1.5012 - val_acc: 0.6121\n",
      "Epoch 19/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 1.4947 - acc: 0.5670 - val_loss: 1.3018 - val_acc: 0.6182\n",
      "Epoch 20/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 1.2987 - acc: 0.5985Epoch 00020: val_loss improved from 2.86710 to 1.15824, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 1.2967 - acc: 0.6000 - val_loss: 1.1582 - val_acc: 0.6182\n",
      "Epoch 21/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 1.1745 - acc: 0.6027 - val_loss: 1.0692 - val_acc: 0.6212\n",
      "Epoch 22/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 1.0896 - acc: 0.6128 - val_loss: 1.0062 - val_acc: 0.6515\n",
      "Epoch 23/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 1.0164 - acc: 0.6327 - val_loss: 0.9598 - val_acc: 0.6394\n",
      "Epoch 24/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.9940 - acc: 0.6290 - val_loss: 0.9316 - val_acc: 0.6364\n",
      "Epoch 25/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.9456 - acc: 0.6471Epoch 00025: val_loss improved from 1.15824 to 0.89328, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.9456 - acc: 0.6475 - val_loss: 0.8933 - val_acc: 0.6545\n",
      "Epoch 26/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.9062 - acc: 0.6623 - val_loss: 0.8688 - val_acc: 0.6545\n",
      "Epoch 27/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.8714 - acc: 0.6768 - val_loss: 0.8506 - val_acc: 0.6606\n",
      "Epoch 28/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.8577 - acc: 0.6754 - val_loss: 0.8251 - val_acc: 0.6697\n",
      "Epoch 29/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.8409 - acc: 0.6845 - val_loss: 0.8232 - val_acc: 0.6758\n",
      "Epoch 30/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.8366 - acc: 0.6753Epoch 00030: val_loss improved from 0.89328 to 0.80017, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.8363 - acc: 0.6754 - val_loss: 0.8002 - val_acc: 0.6758\n",
      "Epoch 31/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.7953 - acc: 0.6963 - val_loss: 0.7949 - val_acc: 0.6848\n",
      "Epoch 32/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.7750 - acc: 0.7088 - val_loss: 0.7794 - val_acc: 0.6848\n",
      "Epoch 33/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.7335 - acc: 0.7333 - val_loss: 0.7556 - val_acc: 0.6939\n",
      "Epoch 34/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.7380 - acc: 0.7212 - val_loss: 0.7433 - val_acc: 0.7030\n",
      "Epoch 35/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.7204 - acc: 0.7255Epoch 00035: val_loss improved from 0.80017 to 0.71779, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.7188 - acc: 0.7269 - val_loss: 0.7178 - val_acc: 0.7152\n",
      "Epoch 36/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.6878 - acc: 0.7465 - val_loss: 0.7195 - val_acc: 0.7121\n",
      "Epoch 37/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.6920 - acc: 0.7471 - val_loss: 0.6979 - val_acc: 0.7394\n",
      "Epoch 38/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.6563 - acc: 0.7579 - val_loss: 0.6889 - val_acc: 0.7333\n",
      "Epoch 39/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.6430 - acc: 0.7650 - val_loss: 0.6721 - val_acc: 0.7303\n",
      "Epoch 40/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.6376 - acc: 0.7656- Epoch 00040: val_loss improved from 0.71779 to 0.67795, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.6369 - acc: 0.7660 - val_loss: 0.6780 - val_acc: 0.7303\n",
      "Epoch 41/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.6217 - acc: 0.7697 - val_loss: 0.6554 - val_acc: 0.7364\n",
      "Epoch 42/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.6000 - acc: 0.7869 - val_loss: 0.6447 - val_acc: 0.7515\n",
      "Epoch 43/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.5865 - acc: 0.7926 - val_loss: 0.6371 - val_acc: 0.7485\n",
      "Epoch 44/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.5685 - acc: 0.7912 - val_loss: 0.6470 - val_acc: 0.7697\n",
      "Epoch 45/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.8057Epoch 00045: val_loss improved from 0.67795 to 0.62570, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.5323 - acc: 0.8064 - val_loss: 0.6257 - val_acc: 0.7545\n",
      "Epoch 46/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.5419 - acc: 0.7997 - val_loss: 0.6177 - val_acc: 0.7606\n",
      "Epoch 47/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.5571 - acc: 0.8003 - val_loss: 0.6285 - val_acc: 0.7727\n",
      "Epoch 48/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.5551 - acc: 0.7953 - val_loss: 0.6056 - val_acc: 0.7788\n",
      "Epoch 49/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.5141 - acc: 0.8189 - val_loss: 0.6170 - val_acc: 0.7697\n",
      "Epoch 50/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.8179Epoch 00050: val_loss improved from 0.62570 to 0.59280, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.5097 - acc: 0.8178 - val_loss: 0.5928 - val_acc: 0.7909\n",
      "Epoch 51/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.5211 - acc: 0.8168 - val_loss: 0.5831 - val_acc: 0.7939\n",
      "Epoch 52/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4955 - acc: 0.8178 - val_loss: 0.6212 - val_acc: 0.7576\n",
      "Epoch 53/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4890 - acc: 0.8290 - val_loss: 0.5813 - val_acc: 0.7758\n",
      "Epoch 54/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4599 - acc: 0.8343 - val_loss: 0.5953 - val_acc: 0.7697\n",
      "Epoch 55/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.4614 - acc: 0.8336Epoch 00055: val_loss improved from 0.59280 to 0.58658, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4592 - acc: 0.8347 - val_loss: 0.5866 - val_acc: 0.7818\n",
      "Epoch 56/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4438 - acc: 0.8451 - val_loss: 0.5988 - val_acc: 0.7788\n",
      "Epoch 57/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4452 - acc: 0.8444 - val_loss: 0.5639 - val_acc: 0.7758\n",
      "Epoch 58/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4473 - acc: 0.8441 - val_loss: 0.5803 - val_acc: 0.7788\n",
      "Epoch 59/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4207 - acc: 0.8556 - val_loss: 0.5647 - val_acc: 0.7939\n",
      "Epoch 60/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.8458Epoch 00060: val_loss improved from 0.58658 to 0.54772, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4316 - acc: 0.8468 - val_loss: 0.5477 - val_acc: 0.8152\n",
      "Epoch 61/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4059 - acc: 0.8552 - val_loss: 0.5832 - val_acc: 0.8061\n",
      "Epoch 62/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4016 - acc: 0.8572 - val_loss: 0.5489 - val_acc: 0.8030\n",
      "Epoch 63/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.4071 - acc: 0.8596 - val_loss: 0.5428 - val_acc: 0.8030\n",
      "Epoch 64/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3931 - acc: 0.8650 - val_loss: 0.5342 - val_acc: 0.8121\n",
      "Epoch 65/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8635Epoch 00065: val_loss improved from 0.54772 to 0.53832, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3912 - acc: 0.8636 - val_loss: 0.5383 - val_acc: 0.8152\n",
      "Epoch 66/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3564 - acc: 0.8727 - val_loss: 0.5224 - val_acc: 0.8242\n",
      "Epoch 67/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3605 - acc: 0.8724 - val_loss: 0.5266 - val_acc: 0.8091\n",
      "Epoch 68/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3627 - acc: 0.8747 - val_loss: 0.5245 - val_acc: 0.8242\n",
      "Epoch 69/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3546 - acc: 0.8832 - val_loss: 0.5500 - val_acc: 0.8030\n",
      "Epoch 70/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8794- ETA: 2s - loss: 0 - ETA: 0s - loss: 0.3513 - accEpoch 00070: val_loss improved from 0.53832 to 0.53518, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3530 - acc: 0.8798 - val_loss: 0.5352 - val_acc: 0.8212\n",
      "Epoch 71/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3340 - acc: 0.8872 - val_loss: 0.5102 - val_acc: 0.8303\n",
      "Epoch 72/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3252 - acc: 0.8886 - val_loss: 0.5244 - val_acc: 0.8273\n",
      "Epoch 73/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3306 - acc: 0.8835 - val_loss: 0.5104 - val_acc: 0.8273\n",
      "Epoch 74/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3194 - acc: 0.8842 - val_loss: 0.4972 - val_acc: 0.8394\n",
      "Epoch 75/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8849Epoch 00075: val_loss improved from 0.53518 to 0.51458, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3238 - acc: 0.8859 - val_loss: 0.5146 - val_acc: 0.8212\n",
      "Epoch 76/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2974 - acc: 0.9007 - val_loss: 0.5103 - val_acc: 0.8303\n",
      "Epoch 77/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2892 - acc: 0.9017 - val_loss: 0.5019 - val_acc: 0.8364\n",
      "Epoch 78/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.3056 - acc: 0.8966 - val_loss: 0.5050 - val_acc: 0.8212\n",
      "Epoch 79/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2977 - acc: 0.9037 - val_loss: 0.4921 - val_acc: 0.8455\n",
      "Epoch 80/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9022Epoch 00080: val_loss improved from 0.51458 to 0.47828, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2880 - acc: 0.9024 - val_loss: 0.4783 - val_acc: 0.8576\n",
      "Epoch 81/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2785 - acc: 0.9071 - val_loss: 0.4998 - val_acc: 0.8394\n",
      "Epoch 82/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2969 - acc: 0.8946 - val_loss: 0.4883 - val_acc: 0.8303\n",
      "Epoch 83/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2893 - acc: 0.8973 - val_loss: 0.4953 - val_acc: 0.8424\n",
      "Epoch 84/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2724 - acc: 0.9077 - val_loss: 0.4962 - val_acc: 0.8424\n",
      "Epoch 85/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9069Epoch 00085: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2697 - acc: 0.9064 - val_loss: 0.5011 - val_acc: 0.8424\n",
      "Epoch 86/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2715 - acc: 0.9071 - val_loss: 0.5138 - val_acc: 0.8152\n",
      "Epoch 87/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2401 - acc: 0.9182 - val_loss: 0.4651 - val_acc: 0.8667\n",
      "Epoch 88/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2532 - acc: 0.9178 - val_loss: 0.4794 - val_acc: 0.8485\n",
      "Epoch 89/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2482 - acc: 0.9158 - val_loss: 0.4639 - val_acc: 0.8485\n",
      "Epoch 90/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9290Epoch 00090: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2320 - acc: 0.9293 - val_loss: 0.4845 - val_acc: 0.8303\n",
      "Epoch 91/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2564 - acc: 0.9141 - val_loss: 0.4803 - val_acc: 0.8636\n",
      "Epoch 92/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2269 - acc: 0.9229 - val_loss: 0.4850 - val_acc: 0.8455\n",
      "Epoch 93/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2398 - acc: 0.9222 - val_loss: 0.4734 - val_acc: 0.8697\n",
      "Epoch 94/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2321 - acc: 0.9212 - val_loss: 0.4744 - val_acc: 0.8606\n",
      "Epoch 95/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9243Epoch 00095: val_loss improved from 0.47828 to 0.45580, saving model to top_weight.h5\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2425 - acc: 0.9246 - val_loss: 0.4558 - val_acc: 0.8606\n",
      "Epoch 96/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2195 - acc: 0.9313 - val_loss: 0.4574 - val_acc: 0.8667\n",
      "Epoch 97/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2123 - acc: 0.9296 - val_loss: 0.4919 - val_acc: 0.8424\n",
      "Epoch 98/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2296 - acc: 0.9226 - val_loss: 0.4743 - val_acc: 0.8515\n",
      "Epoch 99/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2109 - acc: 0.9306 - val_loss: 0.4898 - val_acc: 0.8394\n",
      "Epoch 100/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9314- ETA: 0s - loss: 0.2062 - acc: Epoch 00100: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2096 - acc: 0.9316 - val_loss: 0.4949 - val_acc: 0.8515\n",
      "Epoch 101/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2217 - acc: 0.9316 - val_loss: 0.5177 - val_acc: 0.8424\n",
      "Epoch 102/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2070 - acc: 0.9327 - val_loss: 0.4741 - val_acc: 0.8636\n",
      "Epoch 103/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2019 - acc: 0.9323 - val_loss: 0.4814 - val_acc: 0.8515\n",
      "Epoch 104/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.2154 - acc: 0.9286 - val_loss: 0.4978 - val_acc: 0.8424\n",
      "Epoch 105/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9436Epoch 00105: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1853 - acc: 0.9428 - val_loss: 0.4991 - val_acc: 0.8485\n",
      "Epoch 106/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1909 - acc: 0.9323 - val_loss: 0.4728 - val_acc: 0.8697\n",
      "Epoch 107/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1865 - acc: 0.9367 - val_loss: 0.4570 - val_acc: 0.8697\n",
      "Epoch 108/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1778 - acc: 0.9431 - val_loss: 0.4581 - val_acc: 0.8485\n",
      "Epoch 109/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1848 - acc: 0.9414 - val_loss: 0.4757 - val_acc: 0.8576\n",
      "Epoch 110/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9351Epoch 00110: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1968 - acc: 0.9354 - val_loss: 0.5516 - val_acc: 0.8394\n",
      "Epoch 111/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1805 - acc: 0.9455 - val_loss: 0.4968 - val_acc: 0.8576\n",
      "Epoch 112/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1769 - acc: 0.9428 - val_loss: 0.5105 - val_acc: 0.8576\n",
      "Epoch 113/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1800 - acc: 0.9434 - val_loss: 0.5167 - val_acc: 0.8455\n",
      "Epoch 114/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1640 - acc: 0.9485 - val_loss: 0.4919 - val_acc: 0.8545\n",
      "Epoch 115/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9457Epoch 00115: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1702 - acc: 0.9451 - val_loss: 0.5377 - val_acc: 0.8394\n",
      "Epoch 116/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1647 - acc: 0.9505 - val_loss: 0.4998 - val_acc: 0.8606\n",
      "Epoch 117/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1638 - acc: 0.9455 - val_loss: 0.5296 - val_acc: 0.8545\n",
      "Epoch 118/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1828 - acc: 0.9387 - val_loss: 0.5468 - val_acc: 0.8455\n",
      "Epoch 119/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1645 - acc: 0.9438 - val_loss: 0.5493 - val_acc: 0.8424\n",
      "Epoch 120/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9514Epoch 00120: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1627 - acc: 0.9519 - val_loss: 0.5303 - val_acc: 0.8545\n",
      "Epoch 121/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1683 - acc: 0.9428 - val_loss: 0.5733 - val_acc: 0.8576\n",
      "Epoch 122/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1769 - acc: 0.9475 - val_loss: 0.5479 - val_acc: 0.8515\n",
      "Epoch 123/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1801 - acc: 0.9401 - val_loss: 0.5232 - val_acc: 0.8485\n",
      "Epoch 124/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1693 - acc: 0.9468 - val_loss: 0.5117 - val_acc: 0.8697\n",
      "Epoch 125/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9497Epoch 00125: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1607 - acc: 0.9498 - val_loss: 0.5140 - val_acc: 0.8606\n",
      "Epoch 126/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1461 - acc: 0.9556 - val_loss: 0.4984 - val_acc: 0.8606\n",
      "Epoch 127/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1668 - acc: 0.9481 - val_loss: 0.5225 - val_acc: 0.8636\n",
      "Epoch 128/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1431 - acc: 0.9525 - val_loss: 0.5229 - val_acc: 0.8636\n",
      "Epoch 129/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1536 - acc: 0.9512 - val_loss: 0.4915 - val_acc: 0.8727\n",
      "Epoch 130/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9575Epoch 00130: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1317 - acc: 0.9572 - val_loss: 0.4930 - val_acc: 0.8606\n",
      "Epoch 131/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1482 - acc: 0.9525 - val_loss: 0.5144 - val_acc: 0.8758\n",
      "Epoch 132/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1408 - acc: 0.9582 - val_loss: 0.5492 - val_acc: 0.8545\n",
      "Epoch 133/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1453 - acc: 0.9515 - val_loss: 0.5115 - val_acc: 0.8667\n",
      "Epoch 134/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1359 - acc: 0.9556 - val_loss: 0.5873 - val_acc: 0.8576\n",
      "Epoch 135/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9575Epoch 00135: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1377 - acc: 0.9579 - val_loss: 0.5480 - val_acc: 0.8576\n",
      "Epoch 136/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1420 - acc: 0.9545 - val_loss: 0.5806 - val_acc: 0.8485\n",
      "Epoch 137/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1232 - acc: 0.9636 - val_loss: 0.5783 - val_acc: 0.8576\n",
      "Epoch 138/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1300 - acc: 0.9623 - val_loss: 0.5604 - val_acc: 0.8667\n",
      "Epoch 139/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1494 - acc: 0.9576 - val_loss: 0.5470 - val_acc: 0.8455\n",
      "Epoch 140/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9637Epoch 00140: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1206 - acc: 0.9636 - val_loss: 0.5181 - val_acc: 0.8788\n",
      "Epoch 141/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1329 - acc: 0.9579 - val_loss: 0.5522 - val_acc: 0.8545\n",
      "Epoch 142/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1341 - acc: 0.9559 - val_loss: 0.5479 - val_acc: 0.8727\n",
      "Epoch 143/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1313 - acc: 0.9549 - val_loss: 0.5492 - val_acc: 0.8667\n",
      "Epoch 144/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1241 - acc: 0.9630 - val_loss: 0.5536 - val_acc: 0.8667\n",
      "Epoch 145/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9609Epoch 00145: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1284 - acc: 0.9609 - val_loss: 0.5585 - val_acc: 0.8576\n",
      "Epoch 146/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1360 - acc: 0.9586 - val_loss: 0.5382 - val_acc: 0.8788\n",
      "Epoch 147/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1225 - acc: 0.9643 - val_loss: 0.5340 - val_acc: 0.8576\n",
      "Epoch 148/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1272 - acc: 0.9636 - val_loss: 0.4945 - val_acc: 0.8727\n",
      "Epoch 149/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1155 - acc: 0.9643 - val_loss: 0.5410 - val_acc: 0.8697\n",
      "Epoch 150/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9582Epoch 00150: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1242 - acc: 0.9586 - val_loss: 0.5129 - val_acc: 0.8758\n",
      "Epoch 151/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1190 - acc: 0.9636 - val_loss: 0.5269 - val_acc: 0.8788\n",
      "Epoch 152/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1139 - acc: 0.9623 - val_loss: 0.5773 - val_acc: 0.8545\n",
      "Epoch 153/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1226 - acc: 0.9599 - val_loss: 0.5616 - val_acc: 0.8606\n",
      "Epoch 154/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1090 - acc: 0.9643 - val_loss: 0.5902 - val_acc: 0.8515\n",
      "Epoch 155/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9650Epoch 00155: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1098 - acc: 0.9653 - val_loss: 0.5720 - val_acc: 0.8667\n",
      "Epoch 156/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1225 - acc: 0.9633 - val_loss: 0.5492 - val_acc: 0.8606\n",
      "Epoch 157/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1056 - acc: 0.9677 - val_loss: 0.5745 - val_acc: 0.8697\n",
      "Epoch 158/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1131 - acc: 0.9660 - val_loss: 0.5665 - val_acc: 0.8697\n",
      "Epoch 159/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1131 - acc: 0.9687 - val_loss: 0.6006 - val_acc: 0.8455\n",
      "Epoch 160/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9657Epoch 00160: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1155 - acc: 0.9657 - val_loss: 0.6122 - val_acc: 0.8636\n",
      "Epoch 161/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1001 - acc: 0.9704 - val_loss: 0.5806 - val_acc: 0.8636\n",
      "Epoch 162/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0999 - acc: 0.9677 - val_loss: 0.5760 - val_acc: 0.8667\n",
      "Epoch 163/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0971 - acc: 0.9700 - val_loss: 0.5499 - val_acc: 0.8697\n",
      "Epoch 164/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1195 - acc: 0.9636 - val_loss: 0.5399 - val_acc: 0.8818\n",
      "Epoch 165/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9688- ETA: 0s - loss: 0.0969 - acc: 0.96 - ETA: 0s - loss: 0.0952 - acc: 0.Epoch 00165: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1009 - acc: 0.9687 - val_loss: 0.5565 - val_acc: 0.8758\n",
      "Epoch 166/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0978 - acc: 0.9694 - val_loss: 0.5303 - val_acc: 0.8909\n",
      "Epoch 167/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0937 - acc: 0.9714 - val_loss: 0.5371 - val_acc: 0.8727\n",
      "Epoch 168/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0951 - acc: 0.9710 - val_loss: 0.6032 - val_acc: 0.8667\n",
      "Epoch 169/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1092 - acc: 0.9707 - val_loss: 0.5668 - val_acc: 0.8697\n",
      "Epoch 170/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9694Epoch 00170: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1023 - acc: 0.9697 - val_loss: 0.6052 - val_acc: 0.8636\n",
      "Epoch 171/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0840 - acc: 0.9744 - val_loss: 0.6304 - val_acc: 0.8667\n",
      "Epoch 172/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0888 - acc: 0.9717 - val_loss: 0.5857 - val_acc: 0.8788\n",
      "Epoch 173/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1099 - acc: 0.9623 - val_loss: 0.6519 - val_acc: 0.8606\n",
      "Epoch 174/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0984 - acc: 0.9694 - val_loss: 0.6168 - val_acc: 0.8545\n",
      "Epoch 175/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9718Epoch 00175: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0992 - acc: 0.9721 - val_loss: 0.6260 - val_acc: 0.8576\n",
      "Epoch 176/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1131 - acc: 0.9680 - val_loss: 0.6636 - val_acc: 0.8606\n",
      "Epoch 177/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0850 - acc: 0.9707 - val_loss: 0.6564 - val_acc: 0.8636\n",
      "Epoch 178/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0814 - acc: 0.9754 - val_loss: 0.6204 - val_acc: 0.8727\n",
      "Epoch 179/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0793 - acc: 0.9768 - val_loss: 0.5848 - val_acc: 0.8879\n",
      "Epoch 180/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9755Epoch 00180: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0923 - acc: 0.9758 - val_loss: 0.6619 - val_acc: 0.8606\n",
      "Epoch 181/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0899 - acc: 0.9737 - val_loss: 0.5991 - val_acc: 0.8788\n",
      "Epoch 182/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.1008 - acc: 0.9710 - val_loss: 0.5868 - val_acc: 0.8697\n",
      "Epoch 183/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0793 - acc: 0.9758 - val_loss: 0.6371 - val_acc: 0.8636\n",
      "Epoch 184/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0975 - acc: 0.9727 - val_loss: 0.5670 - val_acc: 0.8788\n",
      "Epoch 185/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9762- ETA: 2s - losEpoch 00185: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0841 - acc: 0.9758 - val_loss: 0.6121 - val_acc: 0.8727\n",
      "Epoch 186/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0867 - acc: 0.9734 - val_loss: 0.6176 - val_acc: 0.8697\n",
      "Epoch 187/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0815 - acc: 0.9754 - val_loss: 0.6741 - val_acc: 0.8727\n",
      "Epoch 188/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0848 - acc: 0.9721 - val_loss: 0.6656 - val_acc: 0.8727\n",
      "Epoch 189/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0952 - acc: 0.9697 - val_loss: 0.6367 - val_acc: 0.8788\n",
      "Epoch 190/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9759Epoch 00190: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0788 - acc: 0.9761 - val_loss: 0.6467 - val_acc: 0.8636\n",
      "Epoch 191/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0739 - acc: 0.9764 - val_loss: 0.6476 - val_acc: 0.8606\n",
      "Epoch 192/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0866 - acc: 0.9754 - val_loss: 0.6708 - val_acc: 0.8606\n",
      "Epoch 193/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0842 - acc: 0.9724 - val_loss: 0.6112 - val_acc: 0.8818\n",
      "Epoch 194/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0814 - acc: 0.9754 - val_loss: 0.6139 - val_acc: 0.8758\n",
      "Epoch 195/3000\n",
      "2944/2970 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9745Epoch 00195: val_loss did not improve\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0890 - acc: 0.9744 - val_loss: 0.6328 - val_acc: 0.8697\n",
      "Epoch 196/3000\n",
      "2970/2970 [==============================] - 5s 2ms/step - loss: 0.0653 - acc: 0.9805 - val_loss: 0.6711 - val_acc: 0.8636\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(patience=30, mode='auto', monitor='val_acc')\n",
    "checkPoint = ModelCheckpoint(filepath=\"top_weight.h5\", verbose=1, save_best_only=True, save_weights_only=True, period=5)\n",
    "Logs = CSVLogger('logs.csv', separator=',', append=True)\n",
    "idx = np.random.permutation(X_train.shape[0]) ## random shuffle\n",
    "history = model.fit(X_train[idx], y_train[idx], epochs=3000, batch_size=64, shuffle=False, validation_split=0.1, callbacks=[checkPoint, Logs, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9z9fZalv5wN6tgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1031b8a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1030e6ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting 了 QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/790 [==============================] - 1s 938us/step\n",
      "loss: 0.64\n",
      "acc: 88.35%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print 'loss: %.2f'%(loss)\n",
    "print 'acc: %.2f%%'%(acc*100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61  0  0  0  0  0  2  0  0  2  0  0]\n",
      " [ 2 62  3  1  0  0  0  1  0  0  2  0]\n",
      " [ 0  4 69  0  0  2  0  0  1  0  0  0]\n",
      " [ 0  0  0 58  3  0  1  0  0  3  0  0]\n",
      " [ 0  0  0  1 61  3  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  1 60  0  0  4  0  0  0]\n",
      " [ 0  0  0  2  0  0 52  7  0  1  0  0]\n",
      " [ 0  1  0  0  0  0  2 63  9  0  1  0]\n",
      " [ 0  0  0  0  0  1  0  2 56  0  0  2]\n",
      " [ 1  0  1  5  0  0  3  0  0 55  2  1]\n",
      " [ 0  0  0  0  0  0  1  0  0  2 50  3]\n",
      " [ 0  0  1  0  0  0  0  0  1  0  6 51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.94      0.95        65\n",
      "          1       0.93      0.87      0.90        71\n",
      "          2       0.92      0.91      0.91        76\n",
      "          3       0.87      0.89      0.88        65\n",
      "          4       0.94      0.94      0.94        65\n",
      "          5       0.91      0.91      0.91        66\n",
      "          6       0.85      0.84      0.85        62\n",
      "          7       0.86      0.83      0.85        76\n",
      "          8       0.79      0.92      0.85        61\n",
      "          9       0.87      0.81      0.84        68\n",
      "         10       0.82      0.89      0.85        56\n",
      "         11       0.89      0.86      0.88        59\n",
      "\n",
      "avg / total       0.89      0.88      0.88       790\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADMJJREFUeJzt3X+o3fV9x/HnKzfxR1Lnj1qCJjIDOkXETXfXWqVlU8fs\nWkwHY+imuLaQf1ZrS6Ho/pH+V0qRdpsUgtoKFWVEmdKJraQt3WCGRg00JhaddRqbmFhRW7Pm133v\nj3sK6UXJ5Z7P+ZF9ng8I95yTL+/zOvfe1/2cX/dzU1VI6suySQeQNH4WX+qQxZc6ZPGlDll8qUMW\nX+qQxZc6ZPGlDll8qUPLx3llv3fG8lq9ZsXQc17bfnKDNFqMLGuzNtTcXJM5rbS6XTBdt+03vMPB\nOpBjHTfW4q9es4I7Hzlv6Dn/dN6FDdJoMZadvLLJnLn9+5vMaaXV7YLpum1bavOijvOuvtQhiy91\nyOJLHbL4UoeGKn6Sa5P8LMkLSW5rFUrSaC25+ElmgLuAjwEXATckuahVMEmjM8yK/0Hghap6saoO\nAg8C69vEkjRKwxR/DfDKUed3DS77HUk2JNmaZOtbbxwZ4uoktTLyJ/eqamNVzVbV7KlnzIz66iQt\nwjDFfxU456jzaweXSZpywxT/J8D5SdYlOQG4Hni0TSxJo7Tk9+pX1eEknwW+B8wA91bVs82SSRqZ\noX5Jp6oeAx5rlEXSmPjOPalDFl/qkMWXOjTWjTj27ljFv1xy2dBzbv/vJxukga/+8UeazJn79TtN\n5gDU4cPNZrXQapOJLG/zrdbq89Ny84xlK4+/zUpc8aUOWXypQxZf6pDFlzpk8aUOWXypQxZf6pDF\nlzpk8aUOWXypQxZf6pDFlzpk8aUOWXypQxZf6pDFlzpk8aUOjXUHnpqba7LLyFc/9GcN0sBjOzY3\nmfMXZ/9RkzktTduuMK12zpm2nXxgvDvntOKKL3XI4ksdsvhShyy+1CGLL3VoycVPck6SHybZkeTZ\nJLe2DCZpdIZ5beQw8MWqejrJKcBTSZ6oqh2NskkakSWv+FW1u6qeHpz+FbATWNMqmKTRafIYP8m5\nwKXAlhbzJI3W0G+DSvI+4CHg81X19rv8/wZgA8BJtHk3maThDLXiJ1nBfOnvr6qH3+2YqtpYVbNV\nNbuCE4e5OkmNDPOsfoB7gJ1VdWe7SJJGbZgV/0rgJuCqJNsG//6yUS5JI7Tkx/hV9Z9AGmaRNCa+\nc0/qkMWXOmTxpQ6NdQeeVo68/ssmc1rtnPNXO/Y1mQPw6BXnNZlz5M23msyZth1vWs2ZOe3UJnOg\n3ed6nFzxpQ5ZfKlDFl/qkMWXOmTxpQ5ZfKlDFl/qkMWXOmTxpQ5ZfKlDFl/qkMWXOmTxpQ5ZfKlD\nFl/qkMWXOmTxpQ5ZfKlDx+XWW9Pm3y45q9msW55r8+cH75r9UJM5x+O2Uovx//V2LZYrvtQhiy91\nyOJLHbL4UoeGLn6SmSTPJPlui0CSRq/Fin8rsLPBHEljMlTxk6wFPg7c3SaOpHEYdsX/OvAlYK5B\nFkljsuTiJ/kEsLeqnjrGcRuSbE2y9RAHlnp1khoaZsW/ErguyUvAg8BVSb6z8KCq2lhVs1U1u4IT\nh7g6Sa0sufhVdXtVra2qc4HrgR9U1Y3NkkkaGV/HlzrU5Jd0qupHwI9azJI0eq74UocsvtQhiy91\nyOJLHep6B54sb3Pz6/DhJnMA/vnCi5vM+fSObU3m3PMH65rMaWXmzPc3mXPk9V82mXO8csWXOmTx\npQ5ZfKlDFl/qkMWXOmTxpQ5ZfKlDFl/qkMWXOmTxpQ5ZfKlDFl/qkMWXOmTxpQ5ZfKlDFl/qkMWX\nOtT1Djytds5ZtnJlkzkAc/v3N5nTauecs588pcmcvX/bZuecwy++1GROq92XoO0OTOPiii91yOJL\nHbL4UocsvtShoYqf5LQkm5I8l2Rnkg+3CiZpdIZ9avMbwONV9ddJTgDaPb0taWSWXPwkpwIfBf4e\noKoOAgfbxJI0SsPc1V8H7AO+leSZJHcnWdUol6QRGqb4y4HLgG9W1aXAO8BtCw9KsiHJ1iRbD3Fg\niKuT1Mowxd8F7KqqLYPzm5j/QfA7qmpjVc1W1ewKThzi6iS1suTiV9Ue4JUkFwwuuhrY0SSVpJEa\n9ln9W4D7B8/ovwh8avhIkkZtqOJX1TZgtlEWSWPiO/ekDll8qUMWX+qQxZc6dFzuwNNq95RWO6e0\n2jVnGu256kiTOXdsf6jJnC9f93dN5sxtf67JHJi+78fFcMWXOmTxpQ5ZfKlDFl/qkMWXOmTxpQ5Z\nfKlDFl/qkMWXOmTxpQ5ZfKlDFl/qkMWXOmTxpQ5ZfKlDFl/qkMWXOnRc7sAzzp1KjletdoVptbvQ\nly/+SJM5Vz65rcmc/7jkpCZzoN3347KVw/+x6fzv4tZyV3ypQxZf6pDFlzpk8aUOWXypQ0MVP8kX\nkjybZHuSB5K0e6pU0sgsufhJ1gCfA2ar6mJgBri+VTBJozPsXf3lwMlJlgMrgV8MH0nSqC25+FX1\nKvA14GVgN/BWVX1/4XFJNiTZmmTrIQ4sPamkZoa5q386sB5YB5wNrEpy48LjqmpjVc1W1ewKTlx6\nUknNDHNX/xrg51W1r6oOAQ8DV7SJJWmUhin+y8DlSVYmCXA1sLNNLEmjNMxj/C3AJuBp4KeDWRsb\n5ZI0QkP9CldV3QHc0SiLpDHxnXtShyy+1CGLL3VovDvwpM3OMK12PGm1S83MB85sMgfg8O49Tea0\n+hzNnHZqkzlH3nyryZxWO+f8ybYjTeYAPHXFKU3m1MGDw8+ouUUd54ovdcjiSx2y+FKHLL7UIYsv\ndcjiSx2y+FKHLL7UIYsvdcjiSx2y+FKHLL7UIYsvdcjiSx2y+FKHLL7UIYsvdcjiSx0a79Zb1W5L\nqBZaZWm1XdY0mvv1O5OOMBKttssCOPnxVU3m/Oa6FUPPyNszizrOFV/qkMWXOmTxpQ5ZfKlDxyx+\nknuT7E2y/ajLzkjyRJLnBx9PH21MSS0tZsX/NnDtgstuAzZX1fnA5sF5SceJYxa/qn4MvLHg4vXA\nfYPT9wGfbJxL0ggt9TH+6qraPTi9B1jdKI+kMRj6yb2qKqDe6/+TbEiyNcnWQxwY9uokNbDU4r+W\n5CyAwce973VgVW2sqtmqml3BiUu8OkktLbX4jwI3D07fDDzSJo6kcVjMy3kPAP8FXJBkV5LPAF8B\n/jzJ88A1g/OSjhPH/CWdqrrhPf7r6sZZJI2J79yTOmTxpQ5ZfKlDFl/q0Hh34JkyWd7m5k/TrkKt\nTdtta/U1m9u/v8kcgAM3vb/JnNP+ffjP9cynF3ecK77UIYsvdcjiSx2y+FKHLL7UIYsvdcjiSx2y\n+FKHLL7UIYsvdcjiSx2y+FKHLL7UIYsvdcjiSx2y+FKHLL7Uocz/BawxXVmyD/ifYxx2JvD6GOIs\nlnmObdoy9Zzn96vqA8c6aKzFX4wkW6tqdtI5fss8xzZtmcxzbN7Vlzpk8aUOTWPxN046wALmObZp\ny2SeY5i6x/iSRm8aV3xJIzY1xU9ybZKfJXkhyW1TkOecJD9MsiPJs0lunXQmgCQzSZ5J8t0pyHJa\nkk1JnkuyM8mHJ5znC4Ov1fYkDyQ5aQIZ7k2yN8n2oy47I8kTSZ4ffDx93LkWmoriJ5kB7gI+BlwE\n3JDkosmm4jDwxaq6CLgc+IcpyARwK7Bz0iEGvgE8XlUXAn/IBHMlWQN8DpitqouBGeD6CUT5NnDt\ngstuAzZX1fnA5sH5iZqK4gMfBF6oqher6iDwILB+koGqandVPT04/Svmv6nXTDJTkrXAx4G7J5lj\nkOVU4KPAPQBVdbCq3pxsKpYDJydZDqwEfjHuAFX1Y+CNBRevB+4bnL4P+ORYQ72LaSn+GuCVo87v\nYsIlO1qSc4FLgS2TTcLXgS8BcxPOAbAO2Ad8a/DQ4+4kqyYVpqpeBb4GvAzsBt6qqu9PKs8Cq6tq\n9+D0HmD1JMPA9BR/aiV5H/AQ8PmqenuCOT4B7K2qpyaVYYHlwGXAN6vqUuAdJngXdvC4eT3zP5DO\nBlYluXFSed5Lzb+MNvGX0qal+K8C5xx1fu3gsolKsoL50t9fVQ9POM6VwHVJXmL+odBVSb4zwTy7\ngF1V9dt7QZuY/0EwKdcAP6+qfVV1CHgYuGKCeY72WpKzAAYf9044z9QU/yfA+UnWJTmB+SdlHp1k\noCRh/vHrzqq6c5JZAKrq9qpaW1XnMv/5+UFVTWxFq6o9wCtJLhhcdDWwY1J5mL+Lf3mSlYOv3dVM\nz5OgjwI3D07fDDwywSzA/N21iauqw0k+C3yP+Wdj762qZycc60rgJuCnSbYNLvvHqnpsgpmmzS3A\n/YMf1i8Cn5pUkKrakmQT8DTzr8g8wwTeMZfkAeBPgTOT7ALuAL4C/GuSzzD/26l/M+5cC/nOPalD\n03JXX9IYWXypQxZf6pDFlzpk8aUOWXypQxZf6pDFlzr0f9jMIRgkgViKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1015ac950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "real = np.argmax(y_test, axis=1)\n",
    "confuse = np.asarray(confusion_matrix(real,pred))\n",
    "print confuse\n",
    "print classification_report(real,pred)\n",
    "confuse = confuse / float(confuse.sum())\n",
    "plt.imshow(confuse, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
