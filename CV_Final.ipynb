{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDNN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loadmat('./dataset.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__globals__', 'test_data', 'test_label', '__version__', 'train_label', 'train_data']\n"
     ]
    }
   ],
   "source": [
    "print list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 林敬翔 之銘言：\n",
    "跟助教要的 dataset\n",
    "\n",
    "第一維是數量 第二維是時間序 第三維是資料量\n",
    "\n",
    "Label 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ....\n",
    "\n",
    "1-10左手腕 11-20右手腕 21-30左手臂 31-40右手臂\n",
    "\n",
    "220代表frame數\n",
    "\n",
    "我是取一秒25frame\n",
    "\n",
    "動作數量是以動作為單位沒錯 1*220*40表是某一個動作的完整資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下資料維度\n",
    "\n",
    "data: (?, 220, 40) = (幾筆資料, 時間步, 資料維度)\n",
    "\n",
    "label: (?, 1) = (幾筆資料, 動作label)\n",
    "\n",
    "動作 label: 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: __header__\n",
      "not an array\n",
      "key: __globals__\n",
      "not an array\n",
      "key: test_data\n",
      "(130, 220, 40)\n",
      "key: test_label\n",
      "(130, 1)\n",
      "key: __version__\n",
      "not an array\n",
      "key: train_label\n",
      "(600, 1)\n",
      "key: train_data\n",
      "(600, 220, 40)\n"
     ]
    }
   ],
   "source": [
    "for key, value in data.iteritems():\n",
    "    print 'key: %s'%key\n",
    "    if hasattr(value , 'shape'):\n",
    "        print value.shape\n",
    "    else:\n",
    "        print 'not an array'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列資料 + 預測 -> RNN? / HMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Keras CuDNNLSTM\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "import keras\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Flatten, Conv1D, MaxPool1D\n",
    "from keras.layers import LSTM, CuDNNLSTM, RepeatVector, TimeDistributed, Bidirectional\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.utils import to_categorical # one-hot encoding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, actions=40, cell_size=128, lstm_layer_n = 3, conv_filters=64, \n",
    "                conv_kernel=7, pooling_step=2, learning_rate=0.01, dropout_r=0.14, \n",
    "                optimizer=keras.optimizers.RMSprop, clipnorm=1., \n",
    "                tLSTM=LSTM, use_temporal_subsampling=False):\n",
    "    if lstm_layer_n<1: raise ValueError('lstm_layer_n must >= 1')\n",
    "    optimizer = optimizer(lr=learning_rate, clipnorm=clipnorm)\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0, input_shape=input_shape))\n",
    "    if use_temporal_subsampling:\n",
    "        model.add(Conv1D(filters=conv_filters, kernel_size=conv_kernel, padding='same', activation='relu'))\n",
    "        model.add(MaxPool1D(pool_size=pooling_step))\n",
    "        model.add(Dropout(dropout_r))\n",
    "    for _ in xrange(1, lstm_layer_n):\n",
    "        model.add(#Bidirectional(\n",
    "                                tLSTM(cell_size, return_sequences=True,\n",
    "                                      unit_forget_bias=True, recurrent_regularizer=regularizers.l2(0.001))#,\n",
    "                                #merge_mode='sum'\n",
    "                               )#)\n",
    "        model.add(Dropout(dropout_r))\n",
    "    model.add(#Bidirectional(\n",
    "                            tLSTM(cell_size, return_sequences=False, unit_forget_bias=True, \n",
    "                                  recurrent_regularizer=regularizers.l2(0.001))#,\n",
    "                            #merge_mode='sum'\n",
    "                           )#)\n",
    "    model.add(Dropout(dropout_r))\n",
    "    model.add(Dense(actions, activation='softmax'))\n",
    "    model.compile(\n",
    "            loss = 'categorical_crossentropy',\n",
    "            optimizer=optimizer, metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info for 0 pca: 1.00\n",
      "info for 1 pca: 1.00\n",
      "info for 2 pca: 1.00\n",
      "info for 3 pca: 1.00\n",
      "info for 4 pca: 1.00\n",
      "info for 5 pca: 1.00\n",
      "info for 6 pca: 1.00\n",
      "info for 7 pca: 1.00\n",
      "info for 8 pca: 1.00\n",
      "info for 9 pca: 1.00\n",
      "info for 10 pca: 1.00\n",
      "info for 11 pca: 1.00\n",
      "info for 12 pca: 1.00\n",
      "info for 13 pca: 1.00\n",
      "info for 14 pca: 1.00\n",
      "info for 15 pca: 1.00\n",
      "info for 16 pca: 1.00\n",
      "info for 17 pca: 1.00\n",
      "info for 18 pca: 1.00\n",
      "info for 19 pca: 1.00\n",
      "info for 20 pca: 1.00\n",
      "info for 21 pca: 1.00\n",
      "info for 22 pca: 1.00\n",
      "info for 23 pca: 1.00\n",
      "info for 24 pca: 1.00\n",
      "info for 25 pca: 1.00\n",
      "info for 26 pca: 1.00\n",
      "info for 27 pca: 1.00\n",
      "info for 28 pca: 1.00\n",
      "info for 29 pca: 1.00\n",
      "info for 30 pca: 1.00\n",
      "info for 31 pca: 1.00\n",
      "info for 32 pca: 1.00\n",
      "info for 33 pca: 1.00\n",
      "info for 34 pca: 1.00\n",
      "info for 35 pca: 1.00\n",
      "info for 36 pca: 1.00\n",
      "info for 37 pca: 1.00\n",
      "info for 38 pca: 1.00\n",
      "info for 39 pca: 1.00\n",
      "info for 40 pca: 1.00\n",
      "info for 41 pca: 1.00\n",
      "info for 42 pca: 1.00\n",
      "info for 43 pca: 1.00\n",
      "info for 44 pca: 1.00\n",
      "info for 45 pca: 1.00\n",
      "info for 46 pca: 1.00\n",
      "info for 47 pca: 1.00\n",
      "info for 48 pca: 1.00\n",
      "info for 49 pca: 1.00\n",
      "info for 50 pca: 1.00\n",
      "info for 51 pca: 1.00\n",
      "info for 52 pca: 1.00\n",
      "info for 53 pca: 1.00\n",
      "info for 54 pca: 1.00\n",
      "info for 55 pca: 1.00\n",
      "info for 56 pca: 1.00\n",
      "info for 57 pca: 1.00\n",
      "info for 58 pca: 1.00\n",
      "info for 59 pca: 1.00\n",
      "info for 60 pca: 1.00\n",
      "info for 61 pca: 1.00\n",
      "info for 62 pca: 1.00\n",
      "info for 63 pca: 1.00\n",
      "info for 64 pca: 1.00\n",
      "info for 65 pca: 1.00\n",
      "info for 66 pca: 1.00\n",
      "info for 67 pca: 1.00\n",
      "info for 68 pca: 1.00\n",
      "info for 69 pca: 1.00\n",
      "info for 70 pca: 1.00\n",
      "info for 71 pca: 1.00\n",
      "info for 72 pca: 1.00\n",
      "info for 73 pca: 1.00\n",
      "info for 74 pca: 1.00\n",
      "info for 75 pca: 1.00\n",
      "info for 76 pca: 1.00\n",
      "info for 77 pca: 1.00\n",
      "info for 78 pca: 1.00\n",
      "info for 79 pca: 1.00\n",
      "info for 80 pca: 1.00\n",
      "info for 81 pca: 1.00\n",
      "info for 82 pca: 1.00\n",
      "info for 83 pca: 1.00\n",
      "info for 84 pca: 1.00\n",
      "info for 85 pca: 1.00\n",
      "info for 86 pca: 1.00\n",
      "info for 87 pca: 1.00\n",
      "info for 88 pca: 1.00\n",
      "info for 89 pca: 1.00\n",
      "info for 90 pca: 1.00\n",
      "info for 91 pca: 1.00\n",
      "info for 92 pca: 1.00\n",
      "info for 93 pca: 1.00\n",
      "info for 94 pca: 1.00\n",
      "info for 95 pca: 1.00\n",
      "info for 96 pca: 1.00\n",
      "info for 97 pca: 1.00\n",
      "info for 98 pca: 1.00\n",
      "info for 99 pca: 1.00\n",
      "info for 100 pca: 1.00\n",
      "info for 101 pca: 1.00\n",
      "info for 102 pca: 1.00\n",
      "info for 103 pca: 1.00\n",
      "info for 104 pca: 1.00\n",
      "info for 105 pca: 1.00\n",
      "info for 106 pca: 1.00\n",
      "info for 107 pca: 1.00\n",
      "info for 108 pca: 1.00\n",
      "info for 109 pca: 1.00\n",
      "info for 110 pca: 1.00\n",
      "info for 111 pca: 1.00\n",
      "info for 112 pca: 1.00\n",
      "info for 113 pca: 1.00\n",
      "info for 114 pca: 1.00\n",
      "info for 115 pca: 1.00\n",
      "info for 116 pca: 1.00\n",
      "info for 117 pca: 1.00\n",
      "info for 118 pca: 1.00\n",
      "info for 119 pca: 1.00\n",
      "info for 120 pca: 1.00\n",
      "info for 121 pca: 1.00\n",
      "info for 122 pca: 1.00\n",
      "info for 123 pca: 1.00\n",
      "info for 124 pca: 1.00\n",
      "info for 125 pca: 1.00\n",
      "info for 126 pca: 1.00\n",
      "info for 127 pca: 1.00\n",
      "info for 128 pca: 1.00\n",
      "info for 129 pca: 1.00\n",
      "info for 130 pca: 1.00\n",
      "info for 131 pca: 1.00\n",
      "info for 132 pca: 1.00\n",
      "info for 133 pca: 1.00\n",
      "info for 134 pca: 1.00\n",
      "info for 135 pca: 1.00\n",
      "info for 136 pca: 1.00\n",
      "info for 137 pca: 1.00\n",
      "info for 138 pca: 1.00\n",
      "info for 139 pca: 1.00\n",
      "info for 140 pca: 1.00\n",
      "info for 141 pca: 1.00\n",
      "info for 142 pca: 1.00\n",
      "info for 143 pca: 1.00\n",
      "info for 144 pca: 1.00\n",
      "info for 145 pca: 1.00\n",
      "info for 146 pca: 1.00\n",
      "info for 147 pca: 1.00\n",
      "info for 148 pca: 1.00\n",
      "info for 149 pca: 1.00\n",
      "info for 150 pca: 1.00\n",
      "info for 151 pca: 1.00\n",
      "info for 152 pca: 1.00\n",
      "info for 153 pca: 1.00\n",
      "info for 154 pca: 1.00\n",
      "info for 155 pca: 1.00\n",
      "info for 156 pca: 1.00\n",
      "info for 157 pca: 1.00\n",
      "info for 158 pca: 1.00\n",
      "info for 159 pca: 1.00\n",
      "info for 160 pca: 1.00\n",
      "info for 161 pca: 1.00\n",
      "info for 162 pca: 1.00\n",
      "info for 163 pca: 1.00\n",
      "info for 164 pca: 1.00\n",
      "info for 165 pca: 1.00\n",
      "info for 166 pca: 1.00\n",
      "info for 167 pca: 1.00\n",
      "info for 168 pca: 1.00\n",
      "info for 169 pca: 1.00\n",
      "info for 170 pca: 1.00\n",
      "info for 171 pca: 1.00\n",
      "info for 172 pca: 1.00\n",
      "info for 173 pca: 1.00\n",
      "info for 174 pca: 1.00\n",
      "info for 175 pca: 1.00\n",
      "info for 176 pca: 1.00\n",
      "info for 177 pca: 1.00\n",
      "info for 178 pca: 1.00\n",
      "info for 179 pca: 1.00\n",
      "info for 180 pca: 1.00\n",
      "info for 181 pca: 1.00\n",
      "info for 182 pca: 1.00\n",
      "info for 183 pca: 1.00\n",
      "info for 184 pca: 1.00\n",
      "info for 185 pca: 1.00\n",
      "info for 186 pca: 1.00\n",
      "info for 187 pca: 1.00\n",
      "info for 188 pca: 1.00\n",
      "info for 189 pca: 1.00\n",
      "info for 190 pca: 1.00\n",
      "info for 191 pca: 1.00\n",
      "info for 192 pca: 1.00\n",
      "info for 193 pca: 1.00\n",
      "info for 194 pca: 1.00\n",
      "info for 195 pca: 1.00\n",
      "info for 196 pca: 1.00\n",
      "info for 197 pca: 1.00\n",
      "info for 198 pca: 1.00\n",
      "info for 199 pca: 1.00\n",
      "info for 200 pca: 1.00\n",
      "info for 201 pca: 1.00\n",
      "info for 202 pca: 1.00\n",
      "info for 203 pca: 1.00\n",
      "info for 204 pca: 1.00\n",
      "info for 205 pca: 1.00\n",
      "info for 206 pca: 1.00\n",
      "info for 207 pca: 1.00\n",
      "info for 208 pca: 1.00\n",
      "info for 209 pca: 1.00\n",
      "info for 210 pca: 1.00\n",
      "info for 211 pca: 1.00\n",
      "info for 212 pca: 1.00\n",
      "info for 213 pca: 1.00\n",
      "info for 214 pca: 1.00\n",
      "info for 215 pca: 1.00\n",
      "info for 216 pca: 1.00\n",
      "info for 217 pca: 1.00\n",
      "info for 218 pca: 1.00\n",
      "info for 219 pca: 1.00\n",
      "(600, 220, 12)\n",
      "(130, 220, 12)\n",
      "(600, 12)\n",
      "(130, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = data['train_data'], data['test_data'], data['train_label'], data['test_label']\n",
    "label_enc = OHE(sparse=False) # One-hot encoder, which has attribute [transform, inverse_transform]\n",
    "y_test = label_enc.fit_transform(y_test)\n",
    "y_train = label_enc.transform(y_train)\n",
    "\n",
    "pcas = []\n",
    "n_comp = 12\n",
    "X_train_new = np.zeros((X_train.shape[0], X_train.shape[1], n_comp))\n",
    "X_test_new = np.zeros((X_test.shape[0], X_test.shape[1], n_comp))\n",
    "for i in xrange(X_train.shape[-2]):\n",
    "    pca = PCA(n_components=n_comp).fit(X_train[:,i,:])\n",
    "    pcas.append( (pca, np.sum(pca.explained_variance_ratio_)) )\n",
    "    X_train_new[:,i,:] = pcas[-1][0].transform(X_train[:,i,:])\n",
    "    X_test_new[:,i,:] = pcas[-1][0].transform(X_test[:,i,:])\n",
    "\n",
    "X_train = X_train_new\n",
    "X_test = X_test_new\n",
    "del X_train_new\n",
    "del X_test_new\n",
    "\n",
    "for i, pca in enumerate(pcas):\n",
    "    print 'info for %d pca: %.2f'%(i,pca[1])\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_1 (Dropout)          (None, 220, 12)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 100)               45600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 46,812\n",
      "Trainable params: 46,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(input_shape=X_test.shape[1:] ,actions=y_test.shape[-1], cell_size=100, lstm_layer_n=1,\n",
    "                    learning_rate=0.0001, dropout_r=0.5,\n",
    "                    tLSTM=CuDNNLSTM if USE_CUDNN else LSTM, use_temporal_subsampling=False\n",
    "                   )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 510 samples, validate on 90 samples\n",
      "Epoch 1/3000\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 2.8409 - acc: 0.1020 - val_loss: 2.6361 - val_acc: 0.1556\n",
      "Epoch 2/3000\n",
      "510/510 [==============================] - 0s 697us/step - loss: 2.7316 - acc: 0.1255 - val_loss: 2.5889 - val_acc: 0.1667\n",
      "Epoch 3/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 2.6795 - acc: 0.1294 - val_loss: 2.5491 - val_acc: 0.1778\n",
      "Epoch 4/3000\n",
      "510/510 [==============================] - 0s 643us/step - loss: 2.6959 - acc: 0.1235 - val_loss: 2.5153 - val_acc: 0.1778\n",
      "Epoch 5/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 2.5881 - acc: 0.1589Epoch 00005: val_loss improved from inf to 2.48270, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 655us/step - loss: 2.6000 - acc: 0.1451 - val_loss: 2.4827 - val_acc: 0.2111\n",
      "Epoch 6/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 2.5739 - acc: 0.1373 - val_loss: 2.4524 - val_acc: 0.2222\n",
      "Epoch 7/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 2.5659 - acc: 0.1627 - val_loss: 2.4217 - val_acc: 0.2222\n",
      "Epoch 8/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 2.5391 - acc: 0.1627 - val_loss: 2.3986 - val_acc: 0.2222\n",
      "Epoch 9/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 2.5142 - acc: 0.1608 - val_loss: 2.3732 - val_acc: 0.2333\n",
      "Epoch 10/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.4537 - acc: 0.2143Epoch 00010: val_loss improved from 2.48270 to 2.34670, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 646us/step - loss: 2.4518 - acc: 0.2118 - val_loss: 2.3467 - val_acc: 0.2444\n",
      "Epoch 11/3000\n",
      "510/510 [==============================] - 0s 665us/step - loss: 2.3981 - acc: 0.1980 - val_loss: 2.3264 - val_acc: 0.2444\n",
      "Epoch 12/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 2.3717 - acc: 0.2118 - val_loss: 2.2989 - val_acc: 0.2556\n",
      "Epoch 13/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 2.3232 - acc: 0.2118 - val_loss: 2.2783 - val_acc: 0.2667\n",
      "Epoch 14/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 2.3105 - acc: 0.2471 - val_loss: 2.2592 - val_acc: 0.2889\n",
      "Epoch 15/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.2981 - acc: 0.2366Epoch 00015: val_loss improved from 2.34670 to 2.23872, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 611us/step - loss: 2.2812 - acc: 0.2529 - val_loss: 2.2387 - val_acc: 0.2778\n",
      "Epoch 16/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 2.2803 - acc: 0.2412 - val_loss: 2.2094 - val_acc: 0.2889\n",
      "Epoch 17/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 2.2690 - acc: 0.2353 - val_loss: 2.1879 - val_acc: 0.3000\n",
      "Epoch 18/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 2.2343 - acc: 0.2667 - val_loss: 2.1679 - val_acc: 0.3000\n",
      "Epoch 19/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 2.1737 - acc: 0.2843 - val_loss: 2.1475 - val_acc: 0.3222\n",
      "Epoch 20/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.1906 - acc: 0.2991Epoch 00020: val_loss improved from 2.23872 to 2.13012, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 657us/step - loss: 2.1778 - acc: 0.3000 - val_loss: 2.1301 - val_acc: 0.3111\n",
      "Epoch 21/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 2.1225 - acc: 0.3235 - val_loss: 2.1068 - val_acc: 0.3222\n",
      "Epoch 22/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 2.1242 - acc: 0.2843 - val_loss: 2.0892 - val_acc: 0.3333\n",
      "Epoch 23/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 2.1275 - acc: 0.2784 - val_loss: 2.0708 - val_acc: 0.3333\n",
      "Epoch 24/3000\n",
      "510/510 [==============================] - 0s 691us/step - loss: 2.0665 - acc: 0.3392 - val_loss: 2.0553 - val_acc: 0.3444\n",
      "Epoch 25/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 2.0270 - acc: 0.3058Epoch 00025: val_loss improved from 2.13012 to 2.04335, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 672us/step - loss: 2.0362 - acc: 0.3039 - val_loss: 2.0433 - val_acc: 0.3333\n",
      "Epoch 26/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 2.0590 - acc: 0.3431 - val_loss: 2.0237 - val_acc: 0.3444\n",
      "Epoch 27/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 2.0337 - acc: 0.3176 - val_loss: 2.0019 - val_acc: 0.3667\n",
      "Epoch 28/3000\n",
      "510/510 [==============================] - 0s 720us/step - loss: 2.0125 - acc: 0.3431 - val_loss: 1.9868 - val_acc: 0.3667\n",
      "Epoch 29/3000\n",
      "510/510 [==============================] - 0s 667us/step - loss: 1.9604 - acc: 0.3588 - val_loss: 1.9712 - val_acc: 0.3778\n",
      "Epoch 30/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 1.9490 - acc: 0.3542Epoch 00030: val_loss improved from 2.04335 to 1.95754, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 817us/step - loss: 1.9507 - acc: 0.3569 - val_loss: 1.9575 - val_acc: 0.3778\n",
      "Epoch 31/3000\n",
      "510/510 [==============================] - 0s 803us/step - loss: 1.9437 - acc: 0.3627 - val_loss: 1.9398 - val_acc: 0.4111\n",
      "Epoch 32/3000\n",
      "510/510 [==============================] - 0s 710us/step - loss: 1.9420 - acc: 0.3588 - val_loss: 1.9291 - val_acc: 0.4000\n",
      "Epoch 33/3000\n",
      "510/510 [==============================] - 0s 709us/step - loss: 1.8854 - acc: 0.3765 - val_loss: 1.9115 - val_acc: 0.4222\n",
      "Epoch 34/3000\n",
      "510/510 [==============================] - 0s 777us/step - loss: 1.8733 - acc: 0.3627 - val_loss: 1.8954 - val_acc: 0.4222\n",
      "Epoch 35/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 1.9129 - acc: 0.3438Epoch 00035: val_loss improved from 1.95754 to 1.88487, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 804us/step - loss: 1.9083 - acc: 0.3510 - val_loss: 1.8849 - val_acc: 0.4333\n",
      "Epoch 36/3000\n",
      "510/510 [==============================] - 0s 772us/step - loss: 1.8609 - acc: 0.4098 - val_loss: 1.8677 - val_acc: 0.4444\n",
      "Epoch 37/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 1.8805 - acc: 0.4039 - val_loss: 1.8574 - val_acc: 0.4444\n",
      "Epoch 38/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 1.8854 - acc: 0.3765 - val_loss: 1.8484 - val_acc: 0.4444\n",
      "Epoch 39/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 1.7906 - acc: 0.4118 - val_loss: 1.8378 - val_acc: 0.4444\n",
      "Epoch 40/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.7979 - acc: 0.4375Epoch 00040: val_loss improved from 1.88487 to 1.82008, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 631us/step - loss: 1.8076 - acc: 0.4255 - val_loss: 1.8201 - val_acc: 0.4444\n",
      "Epoch 41/3000\n",
      "510/510 [==============================] - 0s 654us/step - loss: 1.8206 - acc: 0.3941 - val_loss: 1.8097 - val_acc: 0.4444\n",
      "Epoch 42/3000\n",
      "510/510 [==============================] - 0s 695us/step - loss: 1.8029 - acc: 0.4157 - val_loss: 1.7992 - val_acc: 0.4444\n",
      "Epoch 43/3000\n",
      "510/510 [==============================] - 0s 748us/step - loss: 1.7671 - acc: 0.4235 - val_loss: 1.7883 - val_acc: 0.4444\n",
      "Epoch 44/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 1.7644 - acc: 0.4196 - val_loss: 1.7748 - val_acc: 0.4444\n",
      "Epoch 45/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.7473 - acc: 0.4129Epoch 00045: val_loss improved from 1.82008 to 1.75953, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 614us/step - loss: 1.7398 - acc: 0.4196 - val_loss: 1.7595 - val_acc: 0.4556\n",
      "Epoch 46/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 1.7369 - acc: 0.4392 - val_loss: 1.7477 - val_acc: 0.4778\n",
      "Epoch 47/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 1.7263 - acc: 0.4294 - val_loss: 1.7351 - val_acc: 0.4778\n",
      "Epoch 48/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 1.6461 - acc: 0.4686 - val_loss: 1.7216 - val_acc: 0.4778\n",
      "Epoch 49/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 1.6598 - acc: 0.4353 - val_loss: 1.7112 - val_acc: 0.4778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.6411 - acc: 0.4688Epoch 00050: val_loss improved from 1.75953 to 1.70319, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 604us/step - loss: 1.6511 - acc: 0.4627 - val_loss: 1.7032 - val_acc: 0.5000\n",
      "Epoch 51/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 1.6709 - acc: 0.4471 - val_loss: 1.6925 - val_acc: 0.5000\n",
      "Epoch 52/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 1.6690 - acc: 0.4765 - val_loss: 1.6835 - val_acc: 0.5111\n",
      "Epoch 53/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 1.6394 - acc: 0.4824 - val_loss: 1.6712 - val_acc: 0.5000\n",
      "Epoch 54/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 1.6161 - acc: 0.4725 - val_loss: 1.6618 - val_acc: 0.5000\n",
      "Epoch 55/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.6175 - acc: 0.4710Epoch 00055: val_loss improved from 1.70319 to 1.65136, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 593us/step - loss: 1.6121 - acc: 0.4824 - val_loss: 1.6514 - val_acc: 0.4889\n",
      "Epoch 56/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 1.6289 - acc: 0.4510 - val_loss: 1.6424 - val_acc: 0.4889\n",
      "Epoch 57/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 1.5963 - acc: 0.4980 - val_loss: 1.6350 - val_acc: 0.4667\n",
      "Epoch 58/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 1.6114 - acc: 0.4647 - val_loss: 1.6253 - val_acc: 0.5000\n",
      "Epoch 59/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 1.6080 - acc: 0.4294 - val_loss: 1.6175 - val_acc: 0.5000\n",
      "Epoch 60/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.5636 - acc: 0.4978Epoch 00060: val_loss improved from 1.65136 to 1.60819, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 568us/step - loss: 1.5604 - acc: 0.5020 - val_loss: 1.6082 - val_acc: 0.5222\n",
      "Epoch 61/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 1.5891 - acc: 0.4686 - val_loss: 1.5997 - val_acc: 0.5222\n",
      "Epoch 62/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 1.5471 - acc: 0.4941 - val_loss: 1.5906 - val_acc: 0.5333\n",
      "Epoch 63/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 1.5736 - acc: 0.4510 - val_loss: 1.5803 - val_acc: 0.5444\n",
      "Epoch 64/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 1.5381 - acc: 0.5020 - val_loss: 1.5700 - val_acc: 0.5444\n",
      "Epoch 65/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.4973 - acc: 0.4866Epoch 00065: val_loss improved from 1.60819 to 1.56180, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 614us/step - loss: 1.5007 - acc: 0.4902 - val_loss: 1.5618 - val_acc: 0.5444\n",
      "Epoch 66/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 1.5051 - acc: 0.5039 - val_loss: 1.5478 - val_acc: 0.5333\n",
      "Epoch 67/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 1.4865 - acc: 0.5255 - val_loss: 1.5446 - val_acc: 0.5444\n",
      "Epoch 68/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 1.5240 - acc: 0.4961 - val_loss: 1.5340 - val_acc: 0.5444\n",
      "Epoch 69/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 1.5147 - acc: 0.4843 - val_loss: 1.5268 - val_acc: 0.5333\n",
      "Epoch 70/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.4808 - acc: 0.4955Epoch 00070: val_loss improved from 1.56180 to 1.51821, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 574us/step - loss: 1.4861 - acc: 0.4882 - val_loss: 1.5182 - val_acc: 0.5444\n",
      "Epoch 71/3000\n",
      "510/510 [==============================] - 0s 568us/step - loss: 1.4901 - acc: 0.4941 - val_loss: 1.5002 - val_acc: 0.5778\n",
      "Epoch 72/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 1.4561 - acc: 0.5373 - val_loss: 1.4937 - val_acc: 0.5778\n",
      "Epoch 73/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 1.4149 - acc: 0.5471 - val_loss: 1.4909 - val_acc: 0.5889\n",
      "Epoch 74/3000\n",
      "510/510 [==============================] - 0s 540us/step - loss: 1.4460 - acc: 0.5275 - val_loss: 1.4775 - val_acc: 0.5778\n",
      "Epoch 75/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.4155 - acc: 0.5402Epoch 00075: val_loss improved from 1.51821 to 1.46815, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 614us/step - loss: 1.4257 - acc: 0.5373 - val_loss: 1.4682 - val_acc: 0.5778\n",
      "Epoch 76/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 1.4236 - acc: 0.5255 - val_loss: 1.4627 - val_acc: 0.5778\n",
      "Epoch 77/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 1.4109 - acc: 0.5451 - val_loss: 1.4553 - val_acc: 0.5778\n",
      "Epoch 78/3000\n",
      "510/510 [==============================] - 0s 743us/step - loss: 1.4068 - acc: 0.5235 - val_loss: 1.4459 - val_acc: 0.5889\n",
      "Epoch 79/3000\n",
      "510/510 [==============================] - 0s 758us/step - loss: 1.4604 - acc: 0.5059 - val_loss: 1.4379 - val_acc: 0.5778\n",
      "Epoch 80/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.3988 - acc: 0.5536Epoch 00080: val_loss improved from 1.46815 to 1.42916, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 733us/step - loss: 1.4020 - acc: 0.5529 - val_loss: 1.4292 - val_acc: 0.5889\n",
      "Epoch 81/3000\n",
      "510/510 [==============================] - 0s 706us/step - loss: 1.3526 - acc: 0.5294 - val_loss: 1.4227 - val_acc: 0.5778\n",
      "Epoch 82/3000\n",
      "510/510 [==============================] - 0s 790us/step - loss: 1.3526 - acc: 0.5314 - val_loss: 1.4173 - val_acc: 0.5778\n",
      "Epoch 83/3000\n",
      "510/510 [==============================] - 0s 653us/step - loss: 1.3772 - acc: 0.5333 - val_loss: 1.4064 - val_acc: 0.6000\n",
      "Epoch 84/3000\n",
      "510/510 [==============================] - 0s 693us/step - loss: 1.4070 - acc: 0.5294 - val_loss: 1.4001 - val_acc: 0.5889\n",
      "Epoch 85/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.3612 - acc: 0.5469Epoch 00085: val_loss improved from 1.42916 to 1.39268, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 728us/step - loss: 1.3624 - acc: 0.5471 - val_loss: 1.3927 - val_acc: 0.6000\n",
      "Epoch 86/3000\n",
      "510/510 [==============================] - 0s 661us/step - loss: 1.3507 - acc: 0.5412 - val_loss: 1.3874 - val_acc: 0.5778\n",
      "Epoch 87/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 1.3158 - acc: 0.5882 - val_loss: 1.3819 - val_acc: 0.5889\n",
      "Epoch 88/3000\n",
      "510/510 [==============================] - 0s 642us/step - loss: 1.3649 - acc: 0.5608 - val_loss: 1.3763 - val_acc: 0.5778\n",
      "Epoch 89/3000\n",
      "510/510 [==============================] - 0s 665us/step - loss: 1.3245 - acc: 0.5608 - val_loss: 1.3711 - val_acc: 0.5778\n",
      "Epoch 90/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 1.3241 - acc: 0.5677Epoch 00090: val_loss improved from 1.39268 to 1.36465, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 734us/step - loss: 1.3157 - acc: 0.5706 - val_loss: 1.3646 - val_acc: 0.5889\n",
      "Epoch 91/3000\n",
      "510/510 [==============================] - 0s 718us/step - loss: 1.3453 - acc: 0.5373 - val_loss: 1.3577 - val_acc: 0.6111\n",
      "Epoch 92/3000\n",
      "510/510 [==============================] - 0s 689us/step - loss: 1.3219 - acc: 0.5765 - val_loss: 1.3529 - val_acc: 0.6111\n",
      "Epoch 93/3000\n",
      "510/510 [==============================] - 0s 756us/step - loss: 1.3433 - acc: 0.5451 - val_loss: 1.3453 - val_acc: 0.6111\n",
      "Epoch 94/3000\n",
      "510/510 [==============================] - 0s 689us/step - loss: 1.2835 - acc: 0.5902 - val_loss: 1.3401 - val_acc: 0.6000\n",
      "Epoch 95/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 1.2354 - acc: 0.5755Epoch 00095: val_loss improved from 1.36465 to 1.33462, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 715us/step - loss: 1.2345 - acc: 0.5824 - val_loss: 1.3346 - val_acc: 0.6111\n",
      "Epoch 96/3000\n",
      "510/510 [==============================] - 0s 646us/step - loss: 1.2878 - acc: 0.5725 - val_loss: 1.3255 - val_acc: 0.6111\n",
      "Epoch 97/3000\n",
      "510/510 [==============================] - 0s 671us/step - loss: 1.2596 - acc: 0.5863 - val_loss: 1.3199 - val_acc: 0.6222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 1.2661 - acc: 0.5608 - val_loss: 1.3159 - val_acc: 0.6111\n",
      "Epoch 99/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 1.2663 - acc: 0.5765 - val_loss: 1.3110 - val_acc: 0.6000\n",
      "Epoch 100/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2440 - acc: 0.6094Epoch 00100: val_loss improved from 1.33462 to 1.30925, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 612us/step - loss: 1.2490 - acc: 0.6098 - val_loss: 1.3093 - val_acc: 0.6111\n",
      "Epoch 101/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 1.2286 - acc: 0.5784 - val_loss: 1.3055 - val_acc: 0.6111\n",
      "Epoch 102/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 1.2489 - acc: 0.5706 - val_loss: 1.2983 - val_acc: 0.6222\n",
      "Epoch 103/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 1.2528 - acc: 0.5804 - val_loss: 1.2889 - val_acc: 0.6000\n",
      "Epoch 104/3000\n",
      "510/510 [==============================] - 0s 751us/step - loss: 1.2290 - acc: 0.5902 - val_loss: 1.2887 - val_acc: 0.6222\n",
      "Epoch 105/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.2032 - acc: 0.6116Epoch 00105: val_loss improved from 1.30925 to 1.28283, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 690us/step - loss: 1.2338 - acc: 0.5961 - val_loss: 1.2828 - val_acc: 0.6222\n",
      "Epoch 106/3000\n",
      "510/510 [==============================] - 0s 660us/step - loss: 1.2265 - acc: 0.5824 - val_loss: 1.2763 - val_acc: 0.6111\n",
      "Epoch 107/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 1.2095 - acc: 0.6294 - val_loss: 1.2732 - val_acc: 0.6111\n",
      "Epoch 108/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 1.1940 - acc: 0.5980 - val_loss: 1.2656 - val_acc: 0.6222\n",
      "Epoch 109/3000\n",
      "510/510 [==============================] - 0s 656us/step - loss: 1.1887 - acc: 0.6039 - val_loss: 1.2652 - val_acc: 0.6222\n",
      "Epoch 110/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.1919 - acc: 0.6317Epoch 00110: val_loss improved from 1.28283 to 1.26195, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 653us/step - loss: 1.2092 - acc: 0.6176 - val_loss: 1.2620 - val_acc: 0.6222\n",
      "Epoch 111/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 1.1788 - acc: 0.6078 - val_loss: 1.2589 - val_acc: 0.6222\n",
      "Epoch 112/3000\n",
      "510/510 [==============================] - 0s 746us/step - loss: 1.1935 - acc: 0.6098 - val_loss: 1.2514 - val_acc: 0.6222\n",
      "Epoch 113/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 1.1625 - acc: 0.6039 - val_loss: 1.2512 - val_acc: 0.6222\n",
      "Epoch 114/3000\n",
      "510/510 [==============================] - 0s 725us/step - loss: 1.1671 - acc: 0.6255 - val_loss: 1.2446 - val_acc: 0.6222\n",
      "Epoch 115/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.1878 - acc: 0.6183Epoch 00115: val_loss improved from 1.26195 to 1.23910, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 649us/step - loss: 1.1952 - acc: 0.6137 - val_loss: 1.2391 - val_acc: 0.6222\n",
      "Epoch 116/3000\n",
      "510/510 [==============================] - 0s 663us/step - loss: 1.1244 - acc: 0.6235 - val_loss: 1.2322 - val_acc: 0.6444\n",
      "Epoch 117/3000\n",
      "510/510 [==============================] - 0s 658us/step - loss: 1.1381 - acc: 0.6353 - val_loss: 1.2292 - val_acc: 0.6444\n",
      "Epoch 118/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 1.0986 - acc: 0.6529 - val_loss: 1.2245 - val_acc: 0.6556\n",
      "Epoch 119/3000\n",
      "510/510 [==============================] - 0s 657us/step - loss: 1.1059 - acc: 0.6549 - val_loss: 1.2234 - val_acc: 0.6556\n",
      "Epoch 120/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.1198 - acc: 0.6317Epoch 00120: val_loss improved from 1.23910 to 1.21750, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 659us/step - loss: 1.1188 - acc: 0.6333 - val_loss: 1.2175 - val_acc: 0.6556\n",
      "Epoch 121/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 1.1417 - acc: 0.6431 - val_loss: 1.2146 - val_acc: 0.6556\n",
      "Epoch 122/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 1.1426 - acc: 0.6294 - val_loss: 1.2087 - val_acc: 0.6556\n",
      "Epoch 123/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 1.1075 - acc: 0.6333 - val_loss: 1.2074 - val_acc: 0.6556\n",
      "Epoch 124/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 1.1133 - acc: 0.6627 - val_loss: 1.2021 - val_acc: 0.6556\n",
      "Epoch 125/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.1059 - acc: 0.6496Epoch 00125: val_loss improved from 1.21750 to 1.19628, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 650us/step - loss: 1.0996 - acc: 0.6529 - val_loss: 1.1963 - val_acc: 0.6444\n",
      "Epoch 126/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 1.1257 - acc: 0.6059 - val_loss: 1.1946 - val_acc: 0.6556\n",
      "Epoch 127/3000\n",
      "510/510 [==============================] - 0s 656us/step - loss: 1.0977 - acc: 0.6510 - val_loss: 1.1883 - val_acc: 0.6556\n",
      "Epoch 128/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 1.1065 - acc: 0.6333 - val_loss: 1.1850 - val_acc: 0.6444\n",
      "Epoch 129/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 1.0970 - acc: 0.6804 - val_loss: 1.1808 - val_acc: 0.6444\n",
      "Epoch 130/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0739 - acc: 0.6518Epoch 00130: val_loss improved from 1.19628 to 1.17979, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 638us/step - loss: 1.0786 - acc: 0.6510 - val_loss: 1.1798 - val_acc: 0.6444\n",
      "Epoch 131/3000\n",
      "510/510 [==============================] - 0s 641us/step - loss: 1.0842 - acc: 0.6431 - val_loss: 1.1764 - val_acc: 0.6444\n",
      "Epoch 132/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 1.0529 - acc: 0.6569 - val_loss: 1.1696 - val_acc: 0.6222\n",
      "Epoch 133/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 1.0777 - acc: 0.6471 - val_loss: 1.1651 - val_acc: 0.6444\n",
      "Epoch 134/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 1.0703 - acc: 0.6529 - val_loss: 1.1624 - val_acc: 0.6222\n",
      "Epoch 135/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0422 - acc: 0.6696Epoch 00135: val_loss improved from 1.17979 to 1.16101, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 618us/step - loss: 1.0379 - acc: 0.6706 - val_loss: 1.1610 - val_acc: 0.6222\n",
      "Epoch 136/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 1.0601 - acc: 0.6765 - val_loss: 1.1560 - val_acc: 0.6444\n",
      "Epoch 137/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 1.0430 - acc: 0.6725 - val_loss: 1.1526 - val_acc: 0.6000\n",
      "Epoch 138/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 1.0073 - acc: 0.6588 - val_loss: 1.1513 - val_acc: 0.6111\n",
      "Epoch 139/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 1.0512 - acc: 0.6588 - val_loss: 1.1448 - val_acc: 0.6222\n",
      "Epoch 140/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0568 - acc: 0.6406Epoch 00140: val_loss improved from 1.16101 to 1.14237, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 643us/step - loss: 1.0726 - acc: 0.6373 - val_loss: 1.1424 - val_acc: 0.6111\n",
      "Epoch 141/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 1.0317 - acc: 0.6608 - val_loss: 1.1400 - val_acc: 0.6111\n",
      "Epoch 142/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 1.0317 - acc: 0.6627 - val_loss: 1.1373 - val_acc: 0.6111\n",
      "Epoch 143/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 1.0437 - acc: 0.6686 - val_loss: 1.1337 - val_acc: 0.6333\n",
      "Epoch 144/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 1.0027 - acc: 0.6863 - val_loss: 1.1335 - val_acc: 0.6444\n",
      "Epoch 145/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0004 - acc: 0.6741Epoch 00145: val_loss improved from 1.14237 to 1.12870, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 635us/step - loss: 1.0000 - acc: 0.6824 - val_loss: 1.1287 - val_acc: 0.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/3000\n",
      "510/510 [==============================] - 0s 545us/step - loss: 1.0506 - acc: 0.6510 - val_loss: 1.1238 - val_acc: 0.6333\n",
      "Epoch 147/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 1.0470 - acc: 0.6647 - val_loss: 1.1218 - val_acc: 0.6333\n",
      "Epoch 148/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.9948 - acc: 0.6961 - val_loss: 1.1191 - val_acc: 0.6333\n",
      "Epoch 149/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 1.0136 - acc: 0.6588 - val_loss: 1.1148 - val_acc: 0.6222\n",
      "Epoch 150/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9926 - acc: 0.6786Epoch 00150: val_loss improved from 1.12870 to 1.11635, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.9992 - acc: 0.6706 - val_loss: 1.1163 - val_acc: 0.6222\n",
      "Epoch 151/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.9789 - acc: 0.6784 - val_loss: 1.1136 - val_acc: 0.6111\n",
      "Epoch 152/3000\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.9882 - acc: 0.6745 - val_loss: 1.1086 - val_acc: 0.6222\n",
      "Epoch 153/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 1.0161 - acc: 0.6569 - val_loss: 1.1026 - val_acc: 0.6222\n",
      "Epoch 154/3000\n",
      "510/510 [==============================] - 0s 547us/step - loss: 0.9939 - acc: 0.6706 - val_loss: 1.1057 - val_acc: 0.6000\n",
      "Epoch 155/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 1.0196 - acc: 0.6964Epoch 00155: val_loss improved from 1.11635 to 1.10269, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.9915 - acc: 0.7176 - val_loss: 1.1027 - val_acc: 0.6111\n",
      "Epoch 156/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.9608 - acc: 0.6941 - val_loss: 1.1003 - val_acc: 0.6111\n",
      "Epoch 157/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.9990 - acc: 0.6667 - val_loss: 1.0953 - val_acc: 0.6222\n",
      "Epoch 158/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.9600 - acc: 0.6980 - val_loss: 1.0879 - val_acc: 0.6222\n",
      "Epoch 159/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.9487 - acc: 0.6863 - val_loss: 1.0886 - val_acc: 0.6222\n",
      "Epoch 160/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9551 - acc: 0.6875Epoch 00160: val_loss improved from 1.10269 to 1.08947, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.9577 - acc: 0.6843 - val_loss: 1.0895 - val_acc: 0.6000\n",
      "Epoch 161/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.9699 - acc: 0.6843 - val_loss: 1.0868 - val_acc: 0.6000\n",
      "Epoch 162/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.9736 - acc: 0.6725 - val_loss: 1.0850 - val_acc: 0.6000\n",
      "Epoch 163/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.9495 - acc: 0.6882 - val_loss: 1.0820 - val_acc: 0.6222\n",
      "Epoch 164/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.9574 - acc: 0.6627 - val_loss: 1.0821 - val_acc: 0.6222\n",
      "Epoch 165/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9496 - acc: 0.7121Epoch 00165: val_loss improved from 1.08947 to 1.07807, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.9470 - acc: 0.7078 - val_loss: 1.0781 - val_acc: 0.6111\n",
      "Epoch 166/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.9337 - acc: 0.7039 - val_loss: 1.0726 - val_acc: 0.6111\n",
      "Epoch 167/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.9324 - acc: 0.6863 - val_loss: 1.0736 - val_acc: 0.6111\n",
      "Epoch 168/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.9041 - acc: 0.7039 - val_loss: 1.0693 - val_acc: 0.6000\n",
      "Epoch 169/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.9447 - acc: 0.6941 - val_loss: 1.0689 - val_acc: 0.6000\n",
      "Epoch 170/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9398 - acc: 0.6942Epoch 00170: val_loss improved from 1.07807 to 1.06800, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.9371 - acc: 0.7000 - val_loss: 1.0680 - val_acc: 0.6000\n",
      "Epoch 171/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.9528 - acc: 0.6804 - val_loss: 1.0625 - val_acc: 0.6111\n",
      "Epoch 172/3000\n",
      "510/510 [==============================] - 0s 659us/step - loss: 0.9349 - acc: 0.6941 - val_loss: 1.0627 - val_acc: 0.6000\n",
      "Epoch 173/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.9192 - acc: 0.7098 - val_loss: 1.0595 - val_acc: 0.6222\n",
      "Epoch 174/3000\n",
      "510/510 [==============================] - 0s 670us/step - loss: 0.9208 - acc: 0.6843 - val_loss: 1.0562 - val_acc: 0.6222\n",
      "Epoch 175/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.9294 - acc: 0.7076Epoch 00175: val_loss improved from 1.06800 to 1.05911, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.9297 - acc: 0.7000 - val_loss: 1.0591 - val_acc: 0.6222\n",
      "Epoch 176/3000\n",
      "510/510 [==============================] - 0s 675us/step - loss: 0.9274 - acc: 0.7059 - val_loss: 1.0556 - val_acc: 0.6111\n",
      "Epoch 177/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.9295 - acc: 0.7118 - val_loss: 1.0561 - val_acc: 0.6222\n",
      "Epoch 178/3000\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.9116 - acc: 0.694 - 0s 606us/step - loss: 0.8967 - acc: 0.7020 - val_loss: 1.0471 - val_acc: 0.6222\n",
      "Epoch 179/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.8907 - acc: 0.7314 - val_loss: 1.0497 - val_acc: 0.6222\n",
      "Epoch 180/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8868 - acc: 0.6875Epoch 00180: val_loss improved from 1.05911 to 1.04782, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.8882 - acc: 0.6961 - val_loss: 1.0478 - val_acc: 0.6111\n",
      "Epoch 181/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.9383 - acc: 0.6725 - val_loss: 1.0401 - val_acc: 0.6333\n",
      "Epoch 182/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.9014 - acc: 0.7118 - val_loss: 1.0394 - val_acc: 0.6222\n",
      "Epoch 183/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.9101 - acc: 0.6922 - val_loss: 1.0393 - val_acc: 0.6222\n",
      "Epoch 184/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.8937 - acc: 0.7059 - val_loss: 1.0347 - val_acc: 0.6111\n",
      "Epoch 185/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8900 - acc: 0.7054Epoch 00185: val_loss improved from 1.04782 to 1.03640, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.8938 - acc: 0.7059 - val_loss: 1.0364 - val_acc: 0.6111\n",
      "Epoch 186/3000\n",
      "510/510 [==============================] - 0s 698us/step - loss: 0.9013 - acc: 0.6922 - val_loss: 1.0338 - val_acc: 0.6111\n",
      "Epoch 187/3000\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.8734 - acc: 0.7157 - val_loss: 1.0301 - val_acc: 0.6000\n",
      "Epoch 188/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.8759 - acc: 0.7078 - val_loss: 1.0272 - val_acc: 0.6111\n",
      "Epoch 189/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.8837 - acc: 0.7314 - val_loss: 1.0276 - val_acc: 0.6111\n",
      "Epoch 190/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8780 - acc: 0.7188Epoch 00190: val_loss improved from 1.03640 to 1.02926, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 653us/step - loss: 0.8748 - acc: 0.7157 - val_loss: 1.0293 - val_acc: 0.6111\n",
      "Epoch 191/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.8666 - acc: 0.7275 - val_loss: 1.0242 - val_acc: 0.6111\n",
      "Epoch 192/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.8531 - acc: 0.7235 - val_loss: 1.0243 - val_acc: 0.6222\n",
      "Epoch 193/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.8393 - acc: 0.7569 - val_loss: 1.0214 - val_acc: 0.6111\n",
      "Epoch 194/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.8826 - acc: 0.6980 - val_loss: 1.0148 - val_acc: 0.6222\n",
      "Epoch 195/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8590 - acc: 0.7121Epoch 00195: val_loss improved from 1.02926 to 1.01137, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.8523 - acc: 0.7137 - val_loss: 1.0114 - val_acc: 0.6222\n",
      "Epoch 196/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.8772 - acc: 0.6863 - val_loss: 1.0098 - val_acc: 0.6222\n",
      "Epoch 197/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.8848 - acc: 0.6980 - val_loss: 1.0074 - val_acc: 0.6222\n",
      "Epoch 198/3000\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.8404 - acc: 0.7235 - val_loss: 1.0065 - val_acc: 0.6222\n",
      "Epoch 199/3000\n",
      "510/510 [==============================] - 0s 652us/step - loss: 0.8364 - acc: 0.7255 - val_loss: 1.0075 - val_acc: 0.6222\n",
      "Epoch 200/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8418 - acc: 0.7143Epoch 00200: val_loss improved from 1.01137 to 1.00362, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.8504 - acc: 0.7137 - val_loss: 1.0036 - val_acc: 0.6222\n",
      "Epoch 201/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.8723 - acc: 0.7176 - val_loss: 1.0018 - val_acc: 0.6222\n",
      "Epoch 202/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.8349 - acc: 0.7373 - val_loss: 1.0036 - val_acc: 0.6222\n",
      "Epoch 203/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.8600 - acc: 0.7059 - val_loss: 1.0036 - val_acc: 0.6111\n",
      "Epoch 204/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.8230 - acc: 0.7314 - val_loss: 0.9928 - val_acc: 0.6333\n",
      "Epoch 205/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8584 - acc: 0.7143Epoch 00205: val_loss improved from 1.00362 to 0.99096, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.8454 - acc: 0.7255 - val_loss: 0.9910 - val_acc: 0.6222\n",
      "Epoch 206/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.8508 - acc: 0.7333 - val_loss: 0.9939 - val_acc: 0.6333\n",
      "Epoch 207/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.8285 - acc: 0.7255 - val_loss: 0.9928 - val_acc: 0.6222\n",
      "Epoch 208/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.8190 - acc: 0.7471 - val_loss: 0.9911 - val_acc: 0.6222\n",
      "Epoch 209/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.8026 - acc: 0.7412 - val_loss: 0.9890 - val_acc: 0.6222\n",
      "Epoch 210/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8229 - acc: 0.7210Epoch 00210: val_loss improved from 0.99096 to 0.98096, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 667us/step - loss: 0.8120 - acc: 0.7294 - val_loss: 0.9810 - val_acc: 0.6333\n",
      "Epoch 211/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.8620 - acc: 0.7137 - val_loss: 0.9825 - val_acc: 0.6333\n",
      "Epoch 212/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.8460 - acc: 0.7176 - val_loss: 0.9791 - val_acc: 0.6222\n",
      "Epoch 213/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.8192 - acc: 0.7314 - val_loss: 0.9794 - val_acc: 0.6222\n",
      "Epoch 214/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.8335 - acc: 0.7275 - val_loss: 0.9806 - val_acc: 0.6111\n",
      "Epoch 215/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8235 - acc: 0.7545Epoch 00215: val_loss improved from 0.98096 to 0.98047, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 645us/step - loss: 0.8271 - acc: 0.7549 - val_loss: 0.9805 - val_acc: 0.6111\n",
      "Epoch 216/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.8392 - acc: 0.7373 - val_loss: 0.9728 - val_acc: 0.6222\n",
      "Epoch 217/3000\n",
      "510/510 [==============================] - 0s 610us/step - loss: 0.8123 - acc: 0.7392 - val_loss: 0.9716 - val_acc: 0.6222\n",
      "Epoch 218/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.7882 - acc: 0.7529 - val_loss: 0.9716 - val_acc: 0.6222\n",
      "Epoch 219/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.7976 - acc: 0.7451 - val_loss: 0.9680 - val_acc: 0.6333\n",
      "Epoch 220/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8127 - acc: 0.7210Epoch 00220: val_loss improved from 0.98047 to 0.96622, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 685us/step - loss: 0.8249 - acc: 0.7216 - val_loss: 0.9662 - val_acc: 0.6333\n",
      "Epoch 221/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.7861 - acc: 0.7667 - val_loss: 0.9675 - val_acc: 0.6222\n",
      "Epoch 222/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.7774 - acc: 0.7706 - val_loss: 0.9622 - val_acc: 0.6333\n",
      "Epoch 223/3000\n",
      "510/510 [==============================] - 0s 670us/step - loss: 0.8234 - acc: 0.7412 - val_loss: 0.9653 - val_acc: 0.6222\n",
      "Epoch 224/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.7802 - acc: 0.7412 - val_loss: 0.9638 - val_acc: 0.6222\n",
      "Epoch 225/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8218 - acc: 0.7411Epoch 00225: val_loss improved from 0.96622 to 0.96123, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.8166 - acc: 0.7431 - val_loss: 0.9612 - val_acc: 0.6222\n",
      "Epoch 226/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.7934 - acc: 0.7255 - val_loss: 0.9605 - val_acc: 0.6333\n",
      "Epoch 227/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.7771 - acc: 0.7608 - val_loss: 0.9581 - val_acc: 0.6333\n",
      "Epoch 228/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.8071 - acc: 0.7471 - val_loss: 0.9578 - val_acc: 0.6333\n",
      "Epoch 229/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.7626 - acc: 0.7529 - val_loss: 0.9546 - val_acc: 0.6222\n",
      "Epoch 230/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.8011 - acc: 0.7344Epoch 00230: val_loss improved from 0.96123 to 0.95558, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 661us/step - loss: 0.8125 - acc: 0.7255 - val_loss: 0.9556 - val_acc: 0.6222\n",
      "Epoch 231/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.7662 - acc: 0.7392 - val_loss: 0.9595 - val_acc: 0.6222\n",
      "Epoch 232/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.7770 - acc: 0.7275 - val_loss: 0.9518 - val_acc: 0.6333\n",
      "Epoch 233/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.7917 - acc: 0.7353 - val_loss: 0.9537 - val_acc: 0.6333\n",
      "Epoch 234/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.7890 - acc: 0.7529 - val_loss: 0.9486 - val_acc: 0.6333\n",
      "Epoch 235/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7810 - acc: 0.7455Epoch 00235: val_loss improved from 0.95558 to 0.94438, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.7728 - acc: 0.7529 - val_loss: 0.9444 - val_acc: 0.6333\n",
      "Epoch 236/3000\n",
      "510/510 [==============================] - 0s 638us/step - loss: 0.7817 - acc: 0.7333 - val_loss: 0.9435 - val_acc: 0.6333\n",
      "Epoch 237/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.7716 - acc: 0.7706 - val_loss: 0.9442 - val_acc: 0.6333\n",
      "Epoch 238/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.7710 - acc: 0.7706 - val_loss: 0.9423 - val_acc: 0.6222\n",
      "Epoch 239/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.7502 - acc: 0.7549 - val_loss: 0.9434 - val_acc: 0.6333\n",
      "Epoch 240/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7531 - acc: 0.7634Epoch 00240: val_loss did not improve\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.7517 - acc: 0.7588 - val_loss: 0.9510 - val_acc: 0.6222\n",
      "Epoch 241/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.7624 - acc: 0.7549 - val_loss: 0.9389 - val_acc: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.7919 - acc: 0.7137 - val_loss: 0.9366 - val_acc: 0.6333\n",
      "Epoch 243/3000\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.7870 - acc: 0.7176 - val_loss: 0.9351 - val_acc: 0.6333\n",
      "Epoch 244/3000\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.7687 - acc: 0.7196 - val_loss: 0.9332 - val_acc: 0.6333\n",
      "Epoch 245/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7820 - acc: 0.7388Epoch 00245: val_loss improved from 0.94438 to 0.93190, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.7696 - acc: 0.7510 - val_loss: 0.9319 - val_acc: 0.6222\n",
      "Epoch 246/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.7463 - acc: 0.7667 - val_loss: 0.9340 - val_acc: 0.6333\n",
      "Epoch 247/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.7811 - acc: 0.7392 - val_loss: 0.9329 - val_acc: 0.6444\n",
      "Epoch 248/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.7675 - acc: 0.7529 - val_loss: 0.9285 - val_acc: 0.6333\n",
      "Epoch 249/3000\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.7302 - acc: 0.7804 - val_loss: 0.9226 - val_acc: 0.6222\n",
      "Epoch 250/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7483 - acc: 0.7634Epoch 00250: val_loss improved from 0.93190 to 0.92224, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 680us/step - loss: 0.7479 - acc: 0.7667 - val_loss: 0.9222 - val_acc: 0.6444\n",
      "Epoch 251/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.7347 - acc: 0.7725 - val_loss: 0.9203 - val_acc: 0.6444\n",
      "Epoch 252/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.7785 - acc: 0.7353 - val_loss: 0.9229 - val_acc: 0.6333\n",
      "Epoch 253/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.7381 - acc: 0.7490 - val_loss: 0.9166 - val_acc: 0.6444\n",
      "Epoch 254/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.7283 - acc: 0.7627 - val_loss: 0.9177 - val_acc: 0.6444\n",
      "Epoch 255/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7694 - acc: 0.7232Epoch 00255: val_loss improved from 0.92224 to 0.91383, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.7767 - acc: 0.7157 - val_loss: 0.9138 - val_acc: 0.6556\n",
      "Epoch 256/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.7827 - acc: 0.7529 - val_loss: 0.9149 - val_acc: 0.6444\n",
      "Epoch 257/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.7227 - acc: 0.7765 - val_loss: 0.9150 - val_acc: 0.6444\n",
      "Epoch 258/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.7259 - acc: 0.7647 - val_loss: 0.9162 - val_acc: 0.6333\n",
      "Epoch 259/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.7494 - acc: 0.7608 - val_loss: 0.9117 - val_acc: 0.6444\n",
      "Epoch 260/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7682 - acc: 0.7455Epoch 00260: val_loss improved from 0.91383 to 0.90875, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 719us/step - loss: 0.7599 - acc: 0.7549 - val_loss: 0.9087 - val_acc: 0.6444\n",
      "Epoch 261/3000\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.7324 - acc: 0.7863 - val_loss: 0.9062 - val_acc: 0.6444\n",
      "Epoch 262/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.7725 - acc: 0.7471 - val_loss: 0.9084 - val_acc: 0.6333\n",
      "Epoch 263/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.7316 - acc: 0.7686 - val_loss: 0.9078 - val_acc: 0.6444\n",
      "Epoch 264/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.7383 - acc: 0.7824 - val_loss: 0.9063 - val_acc: 0.6333\n",
      "Epoch 265/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7054 - acc: 0.7812Epoch 00265: val_loss improved from 0.90875 to 0.90076, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.7143 - acc: 0.7784 - val_loss: 0.9008 - val_acc: 0.6444\n",
      "Epoch 266/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.7117 - acc: 0.7765 - val_loss: 0.8993 - val_acc: 0.6444\n",
      "Epoch 267/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.7350 - acc: 0.7608 - val_loss: 0.9000 - val_acc: 0.6444\n",
      "Epoch 268/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.6941 - acc: 0.7922 - val_loss: 0.8953 - val_acc: 0.6444\n",
      "Epoch 269/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.7255 - acc: 0.7863 - val_loss: 0.8939 - val_acc: 0.6444\n",
      "Epoch 270/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7003 - acc: 0.7790Epoch 00270: val_loss improved from 0.90076 to 0.89362, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 686us/step - loss: 0.7035 - acc: 0.7824 - val_loss: 0.8936 - val_acc: 0.6444\n",
      "Epoch 271/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.7481 - acc: 0.7686 - val_loss: 0.8963 - val_acc: 0.6444\n",
      "Epoch 272/3000\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.7289 - acc: 0.7627 - val_loss: 0.8964 - val_acc: 0.6444\n",
      "Epoch 273/3000\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.7329 - acc: 0.7471 - val_loss: 0.8955 - val_acc: 0.6444\n",
      "Epoch 274/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.6927 - acc: 0.7804 - val_loss: 0.8966 - val_acc: 0.6444\n",
      "Epoch 275/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6846 - acc: 0.8058Epoch 00275: val_loss improved from 0.89362 to 0.89312, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 662us/step - loss: 0.6938 - acc: 0.8000 - val_loss: 0.8931 - val_acc: 0.6444\n",
      "Epoch 276/3000\n",
      "510/510 [==============================] - 0s 670us/step - loss: 0.7420 - acc: 0.7608 - val_loss: 0.8954 - val_acc: 0.6444\n",
      "Epoch 277/3000\n",
      "510/510 [==============================] - 0s 673us/step - loss: 0.7539 - acc: 0.7333 - val_loss: 0.8926 - val_acc: 0.6444\n",
      "Epoch 278/3000\n",
      "510/510 [==============================] - 0s 637us/step - loss: 0.6991 - acc: 0.7882 - val_loss: 0.8917 - val_acc: 0.6444\n",
      "Epoch 279/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.6951 - acc: 0.7804 - val_loss: 0.8917 - val_acc: 0.6333\n",
      "Epoch 280/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6990 - acc: 0.7969Epoch 00280: val_loss did not improve\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.7034 - acc: 0.7863 - val_loss: 0.8935 - val_acc: 0.6444\n",
      "Epoch 281/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.7055 - acc: 0.7647 - val_loss: 0.8933 - val_acc: 0.6444\n",
      "Epoch 282/3000\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.7213 - acc: 0.7569 - val_loss: 0.8818 - val_acc: 0.6556\n",
      "Epoch 283/3000\n",
      "510/510 [==============================] - 0s 709us/step - loss: 0.7006 - acc: 0.7765 - val_loss: 0.8809 - val_acc: 0.6556\n",
      "Epoch 284/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.6666 - acc: 0.8020 - val_loss: 0.8856 - val_acc: 0.6444\n",
      "Epoch 285/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6735 - acc: 0.7991Epoch 00285: val_loss improved from 0.89312 to 0.88337, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.6885 - acc: 0.7882 - val_loss: 0.8834 - val_acc: 0.6556\n",
      "Epoch 286/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.6878 - acc: 0.7902 - val_loss: 0.8848 - val_acc: 0.6556\n",
      "Epoch 287/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.6979 - acc: 0.7745 - val_loss: 0.8880 - val_acc: 0.6444\n",
      "Epoch 288/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.6780 - acc: 0.8039 - val_loss: 0.8896 - val_acc: 0.6444\n",
      "Epoch 289/3000\n",
      "510/510 [==============================] - 0s 662us/step - loss: 0.6792 - acc: 0.7941 - val_loss: 0.8891 - val_acc: 0.6444\n",
      "Epoch 290/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.7179 - acc: 0.7630Epoch 00290: val_loss improved from 0.88337 to 0.87849, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 704us/step - loss: 0.7098 - acc: 0.7549 - val_loss: 0.8785 - val_acc: 0.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.6977 - acc: 0.7725 - val_loss: 0.8806 - val_acc: 0.6556\n",
      "Epoch 292/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.7030 - acc: 0.7784 - val_loss: 0.8829 - val_acc: 0.6556\n",
      "Epoch 293/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.6980 - acc: 0.7745 - val_loss: 0.8833 - val_acc: 0.6444\n",
      "Epoch 294/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.6909 - acc: 0.7882 - val_loss: 0.8798 - val_acc: 0.6556\n",
      "Epoch 295/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.7232 - acc: 0.7500Epoch 00295: val_loss did not improve\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.7161 - acc: 0.7510 - val_loss: 0.8814 - val_acc: 0.6444\n",
      "Epoch 296/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.6812 - acc: 0.7765 - val_loss: 0.8775 - val_acc: 0.6444\n",
      "Epoch 297/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.6664 - acc: 0.8000 - val_loss: 0.8737 - val_acc: 0.6444\n",
      "Epoch 298/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 0.6937 - acc: 0.7667 - val_loss: 0.8786 - val_acc: 0.6333\n",
      "Epoch 299/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.6551 - acc: 0.8039 - val_loss: 0.8688 - val_acc: 0.6444\n",
      "Epoch 300/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6834 - acc: 0.7746Epoch 00300: val_loss improved from 0.87849 to 0.87523, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.6727 - acc: 0.7824 - val_loss: 0.8752 - val_acc: 0.6556\n",
      "Epoch 301/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.6867 - acc: 0.7804 - val_loss: 0.8737 - val_acc: 0.6444\n",
      "Epoch 302/3000\n",
      "510/510 [==============================] - 0s 706us/step - loss: 0.7017 - acc: 0.7588 - val_loss: 0.8752 - val_acc: 0.6444\n",
      "Epoch 303/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.6547 - acc: 0.8000 - val_loss: 0.8732 - val_acc: 0.6444\n",
      "Epoch 304/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.6935 - acc: 0.7608 - val_loss: 0.8699 - val_acc: 0.6444\n",
      "Epoch 305/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6602 - acc: 0.7991Epoch 00305: val_loss improved from 0.87523 to 0.86805, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.6579 - acc: 0.7941 - val_loss: 0.8680 - val_acc: 0.6444\n",
      "Epoch 306/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.6608 - acc: 0.7843 - val_loss: 0.8694 - val_acc: 0.6333\n",
      "Epoch 307/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.6714 - acc: 0.7980 - val_loss: 0.8682 - val_acc: 0.6444\n",
      "Epoch 308/3000\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.6512 - acc: 0.7882 - val_loss: 0.8672 - val_acc: 0.6444\n",
      "Epoch 309/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.6911 - acc: 0.7725 - val_loss: 0.8631 - val_acc: 0.6444\n",
      "Epoch 310/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6568 - acc: 0.7991Epoch 00310: val_loss improved from 0.86805 to 0.86056, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.6653 - acc: 0.7863 - val_loss: 0.8606 - val_acc: 0.6444\n",
      "Epoch 311/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.6496 - acc: 0.7784 - val_loss: 0.8638 - val_acc: 0.6444\n",
      "Epoch 312/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.6826 - acc: 0.7784 - val_loss: 0.8605 - val_acc: 0.6444\n",
      "Epoch 313/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.6616 - acc: 0.7922 - val_loss: 0.8605 - val_acc: 0.6444\n",
      "Epoch 314/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.6532 - acc: 0.7804 - val_loss: 0.8580 - val_acc: 0.6444\n",
      "Epoch 315/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6472 - acc: 0.8058Epoch 00315: val_loss improved from 0.86056 to 0.85552, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.6632 - acc: 0.7922 - val_loss: 0.8555 - val_acc: 0.6444\n",
      "Epoch 316/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.6455 - acc: 0.7902 - val_loss: 0.8551 - val_acc: 0.6444\n",
      "Epoch 317/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.6452 - acc: 0.8118 - val_loss: 0.8531 - val_acc: 0.6444\n",
      "Epoch 318/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.6452 - acc: 0.8137 - val_loss: 0.8492 - val_acc: 0.6667\n",
      "Epoch 319/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.6454 - acc: 0.8039 - val_loss: 0.8524 - val_acc: 0.6556\n",
      "Epoch 320/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6663 - acc: 0.7879Epoch 00320: val_loss improved from 0.85552 to 0.85057, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.6759 - acc: 0.7843 - val_loss: 0.8506 - val_acc: 0.6667\n",
      "Epoch 321/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.6497 - acc: 0.7882 - val_loss: 0.8520 - val_acc: 0.6556\n",
      "Epoch 322/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.6489 - acc: 0.7941 - val_loss: 0.8522 - val_acc: 0.6556\n",
      "Epoch 323/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.6416 - acc: 0.8196 - val_loss: 0.8497 - val_acc: 0.6667\n",
      "Epoch 324/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.6236 - acc: 0.8020 - val_loss: 0.8559 - val_acc: 0.6556\n",
      "Epoch 325/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.6441 - acc: 0.8125Epoch 00325: val_loss did not improve\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.6642 - acc: 0.7941 - val_loss: 0.8507 - val_acc: 0.6778\n",
      "Epoch 326/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.6425 - acc: 0.7667 - val_loss: 0.8470 - val_acc: 0.6778\n",
      "Epoch 327/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.6425 - acc: 0.8078 - val_loss: 0.8504 - val_acc: 0.6667\n",
      "Epoch 328/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.6275 - acc: 0.8000 - val_loss: 0.8509 - val_acc: 0.6667\n",
      "Epoch 329/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.7068 - acc: 0.7667 - val_loss: 0.8473 - val_acc: 0.6444\n",
      "Epoch 330/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6315 - acc: 0.8192Epoch 00330: val_loss improved from 0.85057 to 0.84673, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 672us/step - loss: 0.6372 - acc: 0.8157 - val_loss: 0.8467 - val_acc: 0.6556\n",
      "Epoch 331/3000\n",
      "510/510 [==============================] - 0s 660us/step - loss: 0.6220 - acc: 0.8059 - val_loss: 0.8478 - val_acc: 0.6444\n",
      "Epoch 332/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.6038 - acc: 0.8196 - val_loss: 0.8450 - val_acc: 0.6556\n",
      "Epoch 333/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.6132 - acc: 0.8020 - val_loss: 0.8405 - val_acc: 0.6556\n",
      "Epoch 334/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.6169 - acc: 0.8196 - val_loss: 0.8372 - val_acc: 0.6444\n",
      "Epoch 335/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6320 - acc: 0.8125Epoch 00335: val_loss improved from 0.84673 to 0.83732, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.6445 - acc: 0.8020 - val_loss: 0.8373 - val_acc: 0.6556\n",
      "Epoch 336/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.6139 - acc: 0.8216 - val_loss: 0.8346 - val_acc: 0.6778\n",
      "Epoch 337/3000\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.6201 - acc: 0.8255 - val_loss: 0.8369 - val_acc: 0.6667\n",
      "Epoch 338/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.6194 - acc: 0.8078 - val_loss: 0.8363 - val_acc: 0.6778\n",
      "Epoch 339/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.6099 - acc: 0.8333 - val_loss: 0.8360 - val_acc: 0.6667\n",
      "Epoch 340/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6197 - acc: 0.8036Epoch 00340: val_loss improved from 0.83732 to 0.83668, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.6107 - acc: 0.8039 - val_loss: 0.8367 - val_acc: 0.6667\n",
      "Epoch 341/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.6199 - acc: 0.7941 - val_loss: 0.8268 - val_acc: 0.6667\n",
      "Epoch 342/3000\n",
      "510/510 [==============================] - 0s 541us/step - loss: 0.6382 - acc: 0.8039 - val_loss: 0.8274 - val_acc: 0.6556\n",
      "Epoch 343/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.6266 - acc: 0.8039 - val_loss: 0.8286 - val_acc: 0.6556\n",
      "Epoch 344/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.6299 - acc: 0.7980 - val_loss: 0.8313 - val_acc: 0.6556\n",
      "Epoch 345/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6159 - acc: 0.8326Epoch 00345: val_loss improved from 0.83668 to 0.83382, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.6212 - acc: 0.8353 - val_loss: 0.8338 - val_acc: 0.6556\n",
      "Epoch 346/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.6388 - acc: 0.8020 - val_loss: 0.8288 - val_acc: 0.6556\n",
      "Epoch 347/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.6246 - acc: 0.8020 - val_loss: 0.8274 - val_acc: 0.6556\n",
      "Epoch 348/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.6040 - acc: 0.8314 - val_loss: 0.8265 - val_acc: 0.6667\n",
      "Epoch 349/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.6034 - acc: 0.8118 - val_loss: 0.8218 - val_acc: 0.6556\n",
      "Epoch 350/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5611 - acc: 0.8549Epoch 00350: val_loss improved from 0.83382 to 0.82684, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 604us/step - loss: 0.5634 - acc: 0.8510 - val_loss: 0.8268 - val_acc: 0.6667\n",
      "Epoch 351/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.5930 - acc: 0.8275 - val_loss: 0.8248 - val_acc: 0.6667\n",
      "Epoch 352/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.5936 - acc: 0.8216 - val_loss: 0.8237 - val_acc: 0.6667\n",
      "Epoch 353/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.5988 - acc: 0.8255 - val_loss: 0.8227 - val_acc: 0.6778\n",
      "Epoch 354/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.5789 - acc: 0.8431 - val_loss: 0.8226 - val_acc: 0.6778\n",
      "Epoch 355/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6516 - acc: 0.7701Epoch 00355: val_loss improved from 0.82684 to 0.82080, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.6397 - acc: 0.7824 - val_loss: 0.8208 - val_acc: 0.6778\n",
      "Epoch 356/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.6173 - acc: 0.8078 - val_loss: 0.8183 - val_acc: 0.6778\n",
      "Epoch 357/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.6185 - acc: 0.8216 - val_loss: 0.8165 - val_acc: 0.6778\n",
      "Epoch 358/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.5884 - acc: 0.8490 - val_loss: 0.8214 - val_acc: 0.6778\n",
      "Epoch 359/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.5957 - acc: 0.8255 - val_loss: 0.8192 - val_acc: 0.6667\n",
      "Epoch 360/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6035 - acc: 0.8058Epoch 00360: val_loss did not improve\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.6096 - acc: 0.7980 - val_loss: 0.8240 - val_acc: 0.6667\n",
      "Epoch 361/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.6037 - acc: 0.8196 - val_loss: 0.8210 - val_acc: 0.6778\n",
      "Epoch 362/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.5852 - acc: 0.8392 - val_loss: 0.8199 - val_acc: 0.6667\n",
      "Epoch 363/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.5904 - acc: 0.8333 - val_loss: 0.8161 - val_acc: 0.6778\n",
      "Epoch 364/3000\n",
      "510/510 [==============================] - 0s 547us/step - loss: 0.5960 - acc: 0.8216 - val_loss: 0.8126 - val_acc: 0.6778\n",
      "Epoch 365/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.6166 - acc: 0.7969Epoch 00365: val_loss improved from 0.82080 to 0.81258, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.6203 - acc: 0.7980 - val_loss: 0.8126 - val_acc: 0.6667\n",
      "Epoch 366/3000\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.5951 - acc: 0.8373 - val_loss: 0.8084 - val_acc: 0.6778\n",
      "Epoch 367/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.5944 - acc: 0.8118 - val_loss: 0.8103 - val_acc: 0.6778\n",
      "Epoch 368/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.5891 - acc: 0.8059 - val_loss: 0.8061 - val_acc: 0.6778\n",
      "Epoch 369/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.5961 - acc: 0.8157 - val_loss: 0.8016 - val_acc: 0.6667\n",
      "Epoch 370/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5777 - acc: 0.8080Epoch 00370: val_loss improved from 0.81258 to 0.80264, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 675us/step - loss: 0.5764 - acc: 0.8078 - val_loss: 0.8026 - val_acc: 0.6667\n",
      "Epoch 371/3000\n",
      "510/510 [==============================] - 0s 645us/step - loss: 0.5714 - acc: 0.8255 - val_loss: 0.8053 - val_acc: 0.6778\n",
      "Epoch 372/3000\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.5793 - acc: 0.8235 - val_loss: 0.8087 - val_acc: 0.6667\n",
      "Epoch 373/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.5618 - acc: 0.8235 - val_loss: 0.8075 - val_acc: 0.6667\n",
      "Epoch 374/3000\n",
      "510/510 [==============================] - 0s 545us/step - loss: 0.5708 - acc: 0.8314 - val_loss: 0.8075 - val_acc: 0.6556\n",
      "Epoch 375/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5628 - acc: 0.8259Epoch 00375: val_loss did not improve\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.5724 - acc: 0.8176 - val_loss: 0.8031 - val_acc: 0.6556\n",
      "Epoch 376/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.5879 - acc: 0.8137 - val_loss: 0.8011 - val_acc: 0.6778\n",
      "Epoch 377/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.5751 - acc: 0.8294 - val_loss: 0.8056 - val_acc: 0.6667\n",
      "Epoch 378/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.5754 - acc: 0.8333 - val_loss: 0.7971 - val_acc: 0.6889\n",
      "Epoch 379/3000\n",
      "510/510 [==============================] - 0s 540us/step - loss: 0.5426 - acc: 0.8412 - val_loss: 0.7967 - val_acc: 0.6778\n",
      "Epoch 380/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5554 - acc: 0.8393Epoch 00380: val_loss improved from 0.80264 to 0.79680, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 653us/step - loss: 0.5651 - acc: 0.8314 - val_loss: 0.7968 - val_acc: 0.6889\n",
      "Epoch 381/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.5777 - acc: 0.8059 - val_loss: 0.7982 - val_acc: 0.6667\n",
      "Epoch 382/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.5759 - acc: 0.8098 - val_loss: 0.7980 - val_acc: 0.6556\n",
      "Epoch 383/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.5595 - acc: 0.8490 - val_loss: 0.7954 - val_acc: 0.6556\n",
      "Epoch 384/3000\n",
      "510/510 [==============================] - 0s 612us/step - loss: 0.5740 - acc: 0.8039 - val_loss: 0.7963 - val_acc: 0.6667\n",
      "Epoch 385/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5716 - acc: 0.7991Epoch 00385: val_loss did not improve\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.5950 - acc: 0.7902 - val_loss: 0.7968 - val_acc: 0.6444\n",
      "Epoch 386/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.5861 - acc: 0.8235 - val_loss: 0.8011 - val_acc: 0.6222\n",
      "Epoch 387/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.5470 - acc: 0.8392 - val_loss: 0.7954 - val_acc: 0.6222\n",
      "Epoch 388/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 580us/step - loss: 0.5540 - acc: 0.8216 - val_loss: 0.7961 - val_acc: 0.6556\n",
      "Epoch 389/3000\n",
      "510/510 [==============================] - 0s 640us/step - loss: 0.5497 - acc: 0.8314 - val_loss: 0.8028 - val_acc: 0.6222\n",
      "Epoch 390/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5825 - acc: 0.8326Epoch 00390: val_loss improved from 0.79680 to 0.78725, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 681us/step - loss: 0.5798 - acc: 0.8333 - val_loss: 0.7873 - val_acc: 0.6778\n",
      "Epoch 391/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.5755 - acc: 0.8157 - val_loss: 0.7920 - val_acc: 0.6778\n",
      "Epoch 392/3000\n",
      "510/510 [==============================] - 0s 637us/step - loss: 0.5634 - acc: 0.8333 - val_loss: 0.7865 - val_acc: 0.6778\n",
      "Epoch 393/3000\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.5858 - acc: 0.8216 - val_loss: 0.7881 - val_acc: 0.6667\n",
      "Epoch 394/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.5335 - acc: 0.8529 - val_loss: 0.7855 - val_acc: 0.6778\n",
      "Epoch 395/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5373 - acc: 0.8594Epoch 00395: val_loss improved from 0.78725 to 0.78425, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 654us/step - loss: 0.5401 - acc: 0.8549 - val_loss: 0.7842 - val_acc: 0.6667\n",
      "Epoch 396/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.5880 - acc: 0.8157 - val_loss: 0.7830 - val_acc: 0.6556\n",
      "Epoch 397/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.5501 - acc: 0.8314 - val_loss: 0.7753 - val_acc: 0.7000\n",
      "Epoch 398/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.5119 - acc: 0.8667 - val_loss: 0.7732 - val_acc: 0.6778\n",
      "Epoch 399/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.5531 - acc: 0.8314 - val_loss: 0.7780 - val_acc: 0.6778\n",
      "Epoch 400/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5489 - acc: 0.8393Epoch 00400: val_loss improved from 0.78425 to 0.77796, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.5479 - acc: 0.8353 - val_loss: 0.7780 - val_acc: 0.6889\n",
      "Epoch 401/3000\n",
      "510/510 [==============================] - 0s 568us/step - loss: 0.5651 - acc: 0.8333 - val_loss: 0.7820 - val_acc: 0.7111\n",
      "Epoch 402/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.5416 - acc: 0.8294 - val_loss: 0.7787 - val_acc: 0.7000\n",
      "Epoch 403/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.5536 - acc: 0.8275 - val_loss: 0.7730 - val_acc: 0.7000\n",
      "Epoch 404/3000\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.5442 - acc: 0.8549 - val_loss: 0.7744 - val_acc: 0.6778\n",
      "Epoch 405/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5561 - acc: 0.8214Epoch 00405: val_loss improved from 0.77796 to 0.77748, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.5568 - acc: 0.8235 - val_loss: 0.7775 - val_acc: 0.6889\n",
      "Epoch 406/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.5780 - acc: 0.8353 - val_loss: 0.7795 - val_acc: 0.6778\n",
      "Epoch 407/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.5293 - acc: 0.8667 - val_loss: 0.7778 - val_acc: 0.6667\n",
      "Epoch 408/3000\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.5489 - acc: 0.8176 - val_loss: 0.7868 - val_acc: 0.6556\n",
      "Epoch 409/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.5597 - acc: 0.8451 - val_loss: 0.7765 - val_acc: 0.6556\n",
      "Epoch 410/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5766 - acc: 0.8259Epoch 00410: val_loss did not improve\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.5826 - acc: 0.8176 - val_loss: 0.7796 - val_acc: 0.6444\n",
      "Epoch 411/3000\n",
      "510/510 [==============================] - 0s 546us/step - loss: 0.5374 - acc: 0.8196 - val_loss: 0.7819 - val_acc: 0.6444\n",
      "Epoch 412/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.5483 - acc: 0.8314 - val_loss: 0.7751 - val_acc: 0.6667\n",
      "Epoch 413/3000\n",
      "510/510 [==============================] - 0s 555us/step - loss: 0.5250 - acc: 0.8490 - val_loss: 0.7777 - val_acc: 0.6667\n",
      "Epoch 414/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.5310 - acc: 0.8588 - val_loss: 0.7747 - val_acc: 0.6778\n",
      "Epoch 415/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5083 - acc: 0.8661Epoch 00415: val_loss improved from 0.77748 to 0.77349, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.5163 - acc: 0.8627 - val_loss: 0.7735 - val_acc: 0.7000\n",
      "Epoch 416/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.5347 - acc: 0.8255 - val_loss: 0.7708 - val_acc: 0.7000\n",
      "Epoch 417/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.5347 - acc: 0.8431 - val_loss: 0.7641 - val_acc: 0.6889\n",
      "Epoch 418/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.5147 - acc: 0.8549 - val_loss: 0.7626 - val_acc: 0.6889\n",
      "Epoch 419/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.5335 - acc: 0.8314 - val_loss: 0.7604 - val_acc: 0.6889\n",
      "Epoch 420/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5328 - acc: 0.8482Epoch 00420: val_loss improved from 0.77349 to 0.75995, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.5341 - acc: 0.8412 - val_loss: 0.7599 - val_acc: 0.6889\n",
      "Epoch 421/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.5149 - acc: 0.8490 - val_loss: 0.7573 - val_acc: 0.6778\n",
      "Epoch 422/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.5545 - acc: 0.8235 - val_loss: 0.7559 - val_acc: 0.7000\n",
      "Epoch 423/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.5200 - acc: 0.8569 - val_loss: 0.7625 - val_acc: 0.7000\n",
      "Epoch 424/3000\n",
      "510/510 [==============================] - 0s 555us/step - loss: 0.5266 - acc: 0.8667 - val_loss: 0.7613 - val_acc: 0.6889\n",
      "Epoch 425/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5551 - acc: 0.8214Epoch 00425: val_loss did not improve\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.5690 - acc: 0.8157 - val_loss: 0.7606 - val_acc: 0.6778\n",
      "Epoch 426/3000\n",
      "510/510 [==============================] - 0s 545us/step - loss: 0.5035 - acc: 0.8529 - val_loss: 0.7618 - val_acc: 0.6889\n",
      "Epoch 427/3000\n",
      "510/510 [==============================] - 0s 554us/step - loss: 0.5471 - acc: 0.8471 - val_loss: 0.7546 - val_acc: 0.6556\n",
      "Epoch 428/3000\n",
      "510/510 [==============================] - 0s 555us/step - loss: 0.5170 - acc: 0.8471 - val_loss: 0.7575 - val_acc: 0.6556\n",
      "Epoch 429/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.5265 - acc: 0.8353 - val_loss: 0.7591 - val_acc: 0.6778\n",
      "Epoch 430/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5185 - acc: 0.8504Epoch 00430: val_loss improved from 0.75995 to 0.75140, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.5163 - acc: 0.8510 - val_loss: 0.7514 - val_acc: 0.6778\n",
      "Epoch 431/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.5243 - acc: 0.8471 - val_loss: 0.7530 - val_acc: 0.6778\n",
      "Epoch 432/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.5246 - acc: 0.8627 - val_loss: 0.7523 - val_acc: 0.6889\n",
      "Epoch 433/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.5207 - acc: 0.8490 - val_loss: 0.7539 - val_acc: 0.6778\n",
      "Epoch 434/3000\n",
      "510/510 [==============================] - 0s 556us/step - loss: 0.5020 - acc: 0.8412 - val_loss: 0.7548 - val_acc: 0.6889\n",
      "Epoch 435/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5484 - acc: 0.8237Epoch 00435: val_loss did not improve\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.5339 - acc: 0.8353 - val_loss: 0.7558 - val_acc: 0.6778\n",
      "Epoch 436/3000\n",
      "510/510 [==============================] - 0s 555us/step - loss: 0.5055 - acc: 0.8451 - val_loss: 0.7575 - val_acc: 0.6667\n",
      "Epoch 437/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.5247 - acc: 0.8549 - val_loss: 0.7485 - val_acc: 0.6778\n",
      "Epoch 438/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.5270 - acc: 0.8314 - val_loss: 0.7457 - val_acc: 0.6667\n",
      "Epoch 439/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.5574 - acc: 0.8078 - val_loss: 0.7433 - val_acc: 0.6889\n",
      "Epoch 440/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5004 - acc: 0.8616Epoch 00440: val_loss improved from 0.75140 to 0.74919, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.5087 - acc: 0.8608 - val_loss: 0.7492 - val_acc: 0.6889\n",
      "Epoch 441/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.5382 - acc: 0.8255 - val_loss: 0.7472 - val_acc: 0.7000\n",
      "Epoch 442/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.5181 - acc: 0.8392 - val_loss: 0.7446 - val_acc: 0.6889\n",
      "Epoch 443/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.5283 - acc: 0.8412 - val_loss: 0.7381 - val_acc: 0.6778\n",
      "Epoch 444/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.5199 - acc: 0.8314 - val_loss: 0.7412 - val_acc: 0.6889\n",
      "Epoch 445/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4973 - acc: 0.8438Epoch 00445: val_loss improved from 0.74919 to 0.73492, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.4926 - acc: 0.8490 - val_loss: 0.7349 - val_acc: 0.6889\n",
      "Epoch 446/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.4960 - acc: 0.8569 - val_loss: 0.7322 - val_acc: 0.6778\n",
      "Epoch 447/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.5090 - acc: 0.8353 - val_loss: 0.7333 - val_acc: 0.6778\n",
      "Epoch 448/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.5365 - acc: 0.8373 - val_loss: 0.7403 - val_acc: 0.6778\n",
      "Epoch 449/3000\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.4987 - acc: 0.8529 - val_loss: 0.7384 - val_acc: 0.6778\n",
      "Epoch 450/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4923 - acc: 0.8594Epoch 00450: val_loss improved from 0.73492 to 0.73422, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.4924 - acc: 0.8588 - val_loss: 0.7342 - val_acc: 0.6778\n",
      "Epoch 451/3000\n",
      "510/510 [==============================] - 0s 567us/step - loss: 0.5087 - acc: 0.8373 - val_loss: 0.7322 - val_acc: 0.6889\n",
      "Epoch 452/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.4870 - acc: 0.8588 - val_loss: 0.7367 - val_acc: 0.6667\n",
      "Epoch 453/3000\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.5146 - acc: 0.8373 - val_loss: 0.7314 - val_acc: 0.6778\n",
      "Epoch 454/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.5005 - acc: 0.8294 - val_loss: 0.7305 - val_acc: 0.6778\n",
      "Epoch 455/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4732 - acc: 0.8638Epoch 00455: val_loss improved from 0.73422 to 0.72859, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.4802 - acc: 0.8647 - val_loss: 0.7286 - val_acc: 0.7000\n",
      "Epoch 456/3000\n",
      "510/510 [==============================] - 0s 596us/step - loss: 0.4883 - acc: 0.8686 - val_loss: 0.7281 - val_acc: 0.6889\n",
      "Epoch 457/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.5030 - acc: 0.8588 - val_loss: 0.7314 - val_acc: 0.6667\n",
      "Epoch 458/3000\n",
      "510/510 [==============================] - 0s 561us/step - loss: 0.5170 - acc: 0.8510 - val_loss: 0.7286 - val_acc: 0.6778\n",
      "Epoch 459/3000\n",
      "510/510 [==============================] - 0s 543us/step - loss: 0.4929 - acc: 0.8510 - val_loss: 0.7254 - val_acc: 0.7000\n",
      "Epoch 460/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5183 - acc: 0.8371Epoch 00460: val_loss improved from 0.72859 to 0.72771, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.5206 - acc: 0.8314 - val_loss: 0.7277 - val_acc: 0.6889\n",
      "Epoch 461/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.5082 - acc: 0.8392 - val_loss: 0.7263 - val_acc: 0.7111\n",
      "Epoch 462/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.5040 - acc: 0.8627 - val_loss: 0.7269 - val_acc: 0.7111\n",
      "Epoch 463/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.4624 - acc: 0.8686 - val_loss: 0.7260 - val_acc: 0.6778\n",
      "Epoch 464/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.4607 - acc: 0.8745 - val_loss: 0.7249 - val_acc: 0.6889\n",
      "Epoch 465/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.5049 - acc: 0.8348Epoch 00465: val_loss improved from 0.72771 to 0.72540, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.5129 - acc: 0.8314 - val_loss: 0.7254 - val_acc: 0.7000\n",
      "Epoch 466/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.4866 - acc: 0.8608 - val_loss: 0.7252 - val_acc: 0.6778\n",
      "Epoch 467/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.5334 - acc: 0.8412 - val_loss: 0.7255 - val_acc: 0.6889\n",
      "Epoch 468/3000\n",
      "510/510 [==============================] - 0s 658us/step - loss: 0.4705 - acc: 0.8725 - val_loss: 0.7285 - val_acc: 0.6889\n",
      "Epoch 469/3000\n",
      "510/510 [==============================] - 0s 561us/step - loss: 0.4758 - acc: 0.8765 - val_loss: 0.7291 - val_acc: 0.7000\n",
      "Epoch 470/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4855 - acc: 0.8638Epoch 00470: val_loss did not improve\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.4823 - acc: 0.8686 - val_loss: 0.7260 - val_acc: 0.6889\n",
      "Epoch 471/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.4740 - acc: 0.8784 - val_loss: 0.7241 - val_acc: 0.6889\n",
      "Epoch 472/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.4649 - acc: 0.8608 - val_loss: 0.7212 - val_acc: 0.7000\n",
      "Epoch 473/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.4865 - acc: 0.8510 - val_loss: 0.7183 - val_acc: 0.7000\n",
      "Epoch 474/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 0.5056 - acc: 0.8451 - val_loss: 0.7213 - val_acc: 0.6778\n",
      "Epoch 475/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4402 - acc: 0.8862Epoch 00475: val_loss improved from 0.72540 to 0.71969, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.4424 - acc: 0.8902 - val_loss: 0.7197 - val_acc: 0.6778\n",
      "Epoch 476/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.5091 - acc: 0.8314 - val_loss: 0.7215 - val_acc: 0.6889\n",
      "Epoch 477/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.4947 - acc: 0.8667 - val_loss: 0.7199 - val_acc: 0.6889\n",
      "Epoch 478/3000\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.4844 - acc: 0.8510 - val_loss: 0.7213 - val_acc: 0.6889\n",
      "Epoch 479/3000\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.4758 - acc: 0.8647 - val_loss: 0.7159 - val_acc: 0.7000\n",
      "Epoch 480/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4584 - acc: 0.8571Epoch 00480: val_loss improved from 0.71969 to 0.71404, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.4854 - acc: 0.8471 - val_loss: 0.7140 - val_acc: 0.7000\n",
      "Epoch 481/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.5198 - acc: 0.8333 - val_loss: 0.7106 - val_acc: 0.6889\n",
      "Epoch 482/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.4755 - acc: 0.8471 - val_loss: 0.7150 - val_acc: 0.6889\n",
      "Epoch 483/3000\n",
      "510/510 [==============================] - 0s 553us/step - loss: 0.4767 - acc: 0.8725 - val_loss: 0.7139 - val_acc: 0.6889\n",
      "Epoch 484/3000\n",
      "510/510 [==============================] - 0s 568us/step - loss: 0.4918 - acc: 0.8412 - val_loss: 0.7140 - val_acc: 0.7000\n",
      "Epoch 485/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4708 - acc: 0.8571Epoch 00485: val_loss improved from 0.71404 to 0.71347, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.4717 - acc: 0.8588 - val_loss: 0.7135 - val_acc: 0.6889\n",
      "Epoch 486/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.4788 - acc: 0.8333 - val_loss: 0.7103 - val_acc: 0.6889\n",
      "Epoch 487/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.4813 - acc: 0.8667 - val_loss: 0.7129 - val_acc: 0.6889\n",
      "Epoch 488/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.4979 - acc: 0.8529 - val_loss: 0.7124 - val_acc: 0.6778\n",
      "Epoch 489/3000\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.4601 - acc: 0.8706 - val_loss: 0.7097 - val_acc: 0.7000\n",
      "Epoch 490/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4461 - acc: 0.8661Epoch 00490: val_loss improved from 0.71347 to 0.71171, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.4467 - acc: 0.8686 - val_loss: 0.7117 - val_acc: 0.6889\n",
      "Epoch 491/3000\n",
      "510/510 [==============================] - 0s 593us/step - loss: 0.4579 - acc: 0.8627 - val_loss: 0.7020 - val_acc: 0.6889\n",
      "Epoch 492/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.4415 - acc: 0.8804 - val_loss: 0.7029 - val_acc: 0.6889\n",
      "Epoch 493/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.4718 - acc: 0.8549 - val_loss: 0.7030 - val_acc: 0.7000\n",
      "Epoch 494/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.4530 - acc: 0.8667 - val_loss: 0.7006 - val_acc: 0.7000\n",
      "Epoch 495/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4502 - acc: 0.8750Epoch 00495: val_loss improved from 0.71171 to 0.69750, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.4510 - acc: 0.8745 - val_loss: 0.6975 - val_acc: 0.7000\n",
      "Epoch 496/3000\n",
      "510/510 [==============================] - 0s 580us/step - loss: 0.4529 - acc: 0.8608 - val_loss: 0.7032 - val_acc: 0.6889\n",
      "Epoch 497/3000\n",
      "510/510 [==============================] - 0s 561us/step - loss: 0.4603 - acc: 0.8667 - val_loss: 0.7095 - val_acc: 0.6778\n",
      "Epoch 498/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.4672 - acc: 0.8667 - val_loss: 0.7031 - val_acc: 0.6889\n",
      "Epoch 499/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.4697 - acc: 0.8706 - val_loss: 0.6998 - val_acc: 0.6889\n",
      "Epoch 500/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4877 - acc: 0.8304Epoch 00500: val_loss did not improve\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.4753 - acc: 0.8392 - val_loss: 0.7053 - val_acc: 0.6889\n",
      "Epoch 501/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.4538 - acc: 0.8588 - val_loss: 0.7117 - val_acc: 0.7000\n",
      "Epoch 502/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.4683 - acc: 0.8471 - val_loss: 0.7047 - val_acc: 0.6889\n",
      "Epoch 503/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.4598 - acc: 0.8608 - val_loss: 0.7028 - val_acc: 0.7000\n",
      "Epoch 504/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.4340 - acc: 0.9000 - val_loss: 0.7051 - val_acc: 0.7000\n",
      "Epoch 505/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4359 - acc: 0.8728Epoch 00505: val_loss did not improve\n",
      "510/510 [==============================] - 0s 556us/step - loss: 0.4425 - acc: 0.8667 - val_loss: 0.7086 - val_acc: 0.6889\n",
      "Epoch 506/3000\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.4395 - acc: 0.8686 - val_loss: 0.7010 - val_acc: 0.6778\n",
      "Epoch 507/3000\n",
      "510/510 [==============================] - 0s 537us/step - loss: 0.4655 - acc: 0.8667 - val_loss: 0.7000 - val_acc: 0.6889\n",
      "Epoch 508/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.4398 - acc: 0.8647 - val_loss: 0.7014 - val_acc: 0.7000\n",
      "Epoch 509/3000\n",
      "510/510 [==============================] - 0s 560us/step - loss: 0.4424 - acc: 0.8706 - val_loss: 0.6985 - val_acc: 0.7000\n",
      "Epoch 510/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4557 - acc: 0.8728Epoch 00510: val_loss did not improve\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.4463 - acc: 0.8804 - val_loss: 0.6977 - val_acc: 0.7000\n",
      "Epoch 511/3000\n",
      "510/510 [==============================] - 0s 682us/step - loss: 0.4514 - acc: 0.8608 - val_loss: 0.6996 - val_acc: 0.7111\n",
      "Epoch 512/3000\n",
      "510/510 [==============================] - 0s 635us/step - loss: 0.4793 - acc: 0.8569 - val_loss: 0.6969 - val_acc: 0.7111\n",
      "Epoch 513/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.4722 - acc: 0.8529 - val_loss: 0.6971 - val_acc: 0.7000\n",
      "Epoch 514/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.4313 - acc: 0.8745 - val_loss: 0.6936 - val_acc: 0.7000\n",
      "Epoch 515/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4428 - acc: 0.8683Epoch 00515: val_loss did not improve\n",
      "510/510 [==============================] - 0s 643us/step - loss: 0.4464 - acc: 0.8686 - val_loss: 0.7009 - val_acc: 0.7000\n",
      "Epoch 516/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.4533 - acc: 0.8510 - val_loss: 0.6971 - val_acc: 0.7111\n",
      "Epoch 517/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.4515 - acc: 0.8627 - val_loss: 0.6972 - val_acc: 0.7222\n",
      "Epoch 518/3000\n",
      "510/510 [==============================] - 0s 748us/step - loss: 0.4391 - acc: 0.8627 - val_loss: 0.6986 - val_acc: 0.7111\n",
      "Epoch 519/3000\n",
      "510/510 [==============================] - 0s 676us/step - loss: 0.4441 - acc: 0.8882 - val_loss: 0.6978 - val_acc: 0.7111\n",
      "Epoch 520/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4461 - acc: 0.8661Epoch 00520: val_loss did not improve\n",
      "510/510 [==============================] - 0s 665us/step - loss: 0.4459 - acc: 0.8667 - val_loss: 0.7022 - val_acc: 0.7000\n",
      "Epoch 521/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.4557 - acc: 0.8765 - val_loss: 0.7102 - val_acc: 0.7000\n",
      "Epoch 522/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.4304 - acc: 0.8725 - val_loss: 0.7017 - val_acc: 0.7111\n",
      "Epoch 523/3000\n",
      "510/510 [==============================] - 0s 677us/step - loss: 0.4510 - acc: 0.8569 - val_loss: 0.7001 - val_acc: 0.7111\n",
      "Epoch 524/3000\n",
      "510/510 [==============================] - 0s 675us/step - loss: 0.4336 - acc: 0.8647 - val_loss: 0.6927 - val_acc: 0.7222\n",
      "Epoch 525/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4239 - acc: 0.8862Epoch 00525: val_loss improved from 0.69750 to 0.69365, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 753us/step - loss: 0.4274 - acc: 0.8824 - val_loss: 0.6937 - val_acc: 0.7111\n",
      "Epoch 526/3000\n",
      "510/510 [==============================] - 0s 650us/step - loss: 0.4602 - acc: 0.8549 - val_loss: 0.6953 - val_acc: 0.7000\n",
      "Epoch 527/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.4577 - acc: 0.8784 - val_loss: 0.6924 - val_acc: 0.7111\n",
      "Epoch 528/3000\n",
      "510/510 [==============================] - 0s 651us/step - loss: 0.4502 - acc: 0.8804 - val_loss: 0.6938 - val_acc: 0.7111\n",
      "Epoch 529/3000\n",
      "510/510 [==============================] - 0s 660us/step - loss: 0.4432 - acc: 0.8647 - val_loss: 0.6931 - val_acc: 0.7222\n",
      "Epoch 530/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4534 - acc: 0.8616Epoch 00530: val_loss improved from 0.69365 to 0.69091, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 651us/step - loss: 0.4479 - acc: 0.8647 - val_loss: 0.6909 - val_acc: 0.7111\n",
      "Epoch 531/3000\n",
      "510/510 [==============================] - 0s 671us/step - loss: 0.4463 - acc: 0.8608 - val_loss: 0.6843 - val_acc: 0.7222\n",
      "Epoch 532/3000\n",
      "510/510 [==============================] - 0s 581us/step - loss: 0.4463 - acc: 0.8765 - val_loss: 0.6942 - val_acc: 0.7222\n",
      "Epoch 533/3000\n",
      "510/510 [==============================] - 0s 729us/step - loss: 0.4383 - acc: 0.8784 - val_loss: 0.6914 - val_acc: 0.7111\n",
      "Epoch 534/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.4234 - acc: 0.8765 - val_loss: 0.6929 - val_acc: 0.7111\n",
      "Epoch 535/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4467 - acc: 0.8795Epoch 00535: val_loss did not improve\n",
      "510/510 [==============================] - 0s 591us/step - loss: 0.4416 - acc: 0.8765 - val_loss: 0.6928 - val_acc: 0.7222\n",
      "Epoch 536/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.4265 - acc: 0.8706 - val_loss: 0.6931 - val_acc: 0.7222\n",
      "Epoch 537/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.4294 - acc: 0.8824 - val_loss: 0.6851 - val_acc: 0.7000\n",
      "Epoch 538/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.4222 - acc: 0.8902 - val_loss: 0.6892 - val_acc: 0.7000\n",
      "Epoch 539/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.4147 - acc: 0.9020 - val_loss: 0.6919 - val_acc: 0.7111\n",
      "Epoch 540/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4296 - acc: 0.8616Epoch 00540: val_loss improved from 0.69091 to 0.68791, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 568us/step - loss: 0.4272 - acc: 0.8667 - val_loss: 0.6879 - val_acc: 0.7111\n",
      "Epoch 541/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.4485 - acc: 0.8490 - val_loss: 0.6876 - val_acc: 0.7111\n",
      "Epoch 542/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.4402 - acc: 0.8608 - val_loss: 0.6824 - val_acc: 0.7111\n",
      "Epoch 543/3000\n",
      "510/510 [==============================] - 0s 566us/step - loss: 0.4394 - acc: 0.8765 - val_loss: 0.6799 - val_acc: 0.7333\n",
      "Epoch 544/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.4535 - acc: 0.8725 - val_loss: 0.6795 - val_acc: 0.7111\n",
      "Epoch 545/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4398 - acc: 0.8750Epoch 00545: val_loss improved from 0.68791 to 0.68298, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.4342 - acc: 0.8804 - val_loss: 0.6830 - val_acc: 0.7111\n",
      "Epoch 546/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.4375 - acc: 0.8784 - val_loss: 0.6822 - val_acc: 0.7111\n",
      "Epoch 547/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.4222 - acc: 0.8882 - val_loss: 0.6799 - val_acc: 0.7222\n",
      "Epoch 548/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.4341 - acc: 0.8667 - val_loss: 0.6798 - val_acc: 0.7333\n",
      "Epoch 549/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.4426 - acc: 0.8765 - val_loss: 0.6787 - val_acc: 0.7333\n",
      "Epoch 550/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4438 - acc: 0.8750Epoch 00550: val_loss improved from 0.68298 to 0.68061, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 573us/step - loss: 0.4481 - acc: 0.8667 - val_loss: 0.6806 - val_acc: 0.7111\n",
      "Epoch 551/3000\n",
      "510/510 [==============================] - 0s 539us/step - loss: 0.4231 - acc: 0.8725 - val_loss: 0.6781 - val_acc: 0.7111\n",
      "Epoch 552/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.4166 - acc: 0.8843 - val_loss: 0.6804 - val_acc: 0.7222\n",
      "Epoch 553/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.4174 - acc: 0.8706 - val_loss: 0.6861 - val_acc: 0.7111\n",
      "Epoch 554/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.4222 - acc: 0.8745 - val_loss: 0.6790 - val_acc: 0.7111\n",
      "Epoch 555/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4152 - acc: 0.8772Epoch 00555: val_loss improved from 0.68061 to 0.67533, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 576us/step - loss: 0.4172 - acc: 0.8784 - val_loss: 0.6753 - val_acc: 0.7111\n",
      "Epoch 556/3000\n",
      "510/510 [==============================] - 0s 620us/step - loss: 0.4107 - acc: 0.8804 - val_loss: 0.6812 - val_acc: 0.7111\n",
      "Epoch 557/3000\n",
      "510/510 [==============================] - 0s 549us/step - loss: 0.4384 - acc: 0.8647 - val_loss: 0.6793 - val_acc: 0.7111\n",
      "Epoch 558/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.4229 - acc: 0.8843 - val_loss: 0.6771 - val_acc: 0.7000\n",
      "Epoch 559/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.4162 - acc: 0.8647 - val_loss: 0.6787 - val_acc: 0.7000\n",
      "Epoch 560/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3918 - acc: 0.8996Epoch 00560: val_loss improved from 0.67533 to 0.67023, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 562us/step - loss: 0.3943 - acc: 0.8941 - val_loss: 0.6702 - val_acc: 0.7222\n",
      "Epoch 561/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.4155 - acc: 0.8843 - val_loss: 0.6722 - val_acc: 0.7111\n",
      "Epoch 562/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.4625 - acc: 0.8529 - val_loss: 0.6728 - val_acc: 0.7111\n",
      "Epoch 563/3000\n",
      "510/510 [==============================] - 0s 624us/step - loss: 0.3976 - acc: 0.8843 - val_loss: 0.6707 - val_acc: 0.7111\n",
      "Epoch 564/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.4171 - acc: 0.8843 - val_loss: 0.6722 - val_acc: 0.7111\n",
      "Epoch 565/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4095 - acc: 0.8750Epoch 00565: val_loss improved from 0.67023 to 0.66942, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 682us/step - loss: 0.4005 - acc: 0.8765 - val_loss: 0.6694 - val_acc: 0.7111\n",
      "Epoch 566/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.4147 - acc: 0.8706 - val_loss: 0.6766 - val_acc: 0.7111\n",
      "Epoch 567/3000\n",
      "510/510 [==============================] - 0s 583us/step - loss: 0.4162 - acc: 0.8863 - val_loss: 0.6777 - val_acc: 0.7111\n",
      "Epoch 568/3000\n",
      "510/510 [==============================] - 0s 639us/step - loss: 0.3888 - acc: 0.8784 - val_loss: 0.6703 - val_acc: 0.7222\n",
      "Epoch 569/3000\n",
      "510/510 [==============================] - 0s 677us/step - loss: 0.3936 - acc: 0.9137 - val_loss: 0.6700 - val_acc: 0.7111\n",
      "Epoch 570/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4336 - acc: 0.8683Epoch 00570: val_loss improved from 0.66942 to 0.66829, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.4253 - acc: 0.8784 - val_loss: 0.6683 - val_acc: 0.7111\n",
      "Epoch 571/3000\n",
      "510/510 [==============================] - 0s 599us/step - loss: 0.4115 - acc: 0.8882 - val_loss: 0.6706 - val_acc: 0.7111\n",
      "Epoch 572/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.4131 - acc: 0.8706 - val_loss: 0.6691 - val_acc: 0.7222\n",
      "Epoch 573/3000\n",
      "510/510 [==============================] - 0s 664us/step - loss: 0.4072 - acc: 0.8922 - val_loss: 0.6615 - val_acc: 0.7111\n",
      "Epoch 574/3000\n",
      "510/510 [==============================] - 0s 651us/step - loss: 0.4146 - acc: 0.8765 - val_loss: 0.6638 - val_acc: 0.7111\n",
      "Epoch 575/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4336 - acc: 0.8594Epoch 00575: val_loss improved from 0.66829 to 0.66194, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.4201 - acc: 0.8647 - val_loss: 0.6619 - val_acc: 0.7222\n",
      "Epoch 576/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.4022 - acc: 0.8922 - val_loss: 0.6658 - val_acc: 0.7111\n",
      "Epoch 577/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.4152 - acc: 0.8784 - val_loss: 0.6595 - val_acc: 0.7222\n",
      "Epoch 578/3000\n",
      "510/510 [==============================] - 0s 595us/step - loss: 0.4006 - acc: 0.8863 - val_loss: 0.6713 - val_acc: 0.7111\n",
      "Epoch 579/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.3956 - acc: 0.8902 - val_loss: 0.6644 - val_acc: 0.7111\n",
      "Epoch 580/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.4185 - acc: 0.8705Epoch 00580: val_loss did not improve\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.4195 - acc: 0.8686 - val_loss: 0.6654 - val_acc: 0.7111\n",
      "Epoch 581/3000\n",
      "510/510 [==============================] - 0s 669us/step - loss: 0.3936 - acc: 0.8941 - val_loss: 0.6653 - val_acc: 0.7111\n",
      "Epoch 582/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 0s 574us/step - loss: 0.3861 - acc: 0.8980 - val_loss: 0.6629 - val_acc: 0.7111\n",
      "Epoch 583/3000\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.3962 - acc: 0.8922 - val_loss: 0.6596 - val_acc: 0.7333\n",
      "Epoch 584/3000\n",
      "510/510 [==============================] - 0s 592us/step - loss: 0.4003 - acc: 0.8941 - val_loss: 0.6657 - val_acc: 0.7333\n",
      "Epoch 585/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3854 - acc: 0.8884Epoch 00585: val_loss did not improve\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.3878 - acc: 0.8941 - val_loss: 0.6676 - val_acc: 0.7333\n",
      "Epoch 586/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.3898 - acc: 0.8882 - val_loss: 0.6622 - val_acc: 0.7444\n",
      "Epoch 587/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 0.4234 - acc: 0.8725 - val_loss: 0.6644 - val_acc: 0.7444\n",
      "Epoch 588/3000\n",
      "510/510 [==============================] - 0s 598us/step - loss: 0.4066 - acc: 0.8765 - val_loss: 0.6670 - val_acc: 0.7333\n",
      "Epoch 589/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.4081 - acc: 0.8941 - val_loss: 0.6626 - val_acc: 0.7333\n",
      "Epoch 590/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3997 - acc: 0.8862Epoch 00590: val_loss improved from 0.66194 to 0.65976, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 675us/step - loss: 0.3951 - acc: 0.8902 - val_loss: 0.6598 - val_acc: 0.7444\n",
      "Epoch 591/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.3997 - acc: 0.8725 - val_loss: 0.6606 - val_acc: 0.7444\n",
      "Epoch 592/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.3855 - acc: 0.8824 - val_loss: 0.6590 - val_acc: 0.7556\n",
      "Epoch 593/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.4027 - acc: 0.8941 - val_loss: 0.6593 - val_acc: 0.7444\n",
      "Epoch 594/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.3729 - acc: 0.9059 - val_loss: 0.6572 - val_acc: 0.7444\n",
      "Epoch 595/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3849 - acc: 0.8862Epoch 00595: val_loss improved from 0.65976 to 0.65827, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.3954 - acc: 0.8843 - val_loss: 0.6583 - val_acc: 0.7667\n",
      "Epoch 596/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.4114 - acc: 0.8725 - val_loss: 0.6579 - val_acc: 0.7667\n",
      "Epoch 597/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.3815 - acc: 0.8922 - val_loss: 0.6550 - val_acc: 0.7556\n",
      "Epoch 598/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.3735 - acc: 0.9039 - val_loss: 0.6479 - val_acc: 0.7556\n",
      "Epoch 599/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.3895 - acc: 0.8804 - val_loss: 0.6487 - val_acc: 0.7556\n",
      "Epoch 600/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3614 - acc: 0.9085Epoch 00600: val_loss improved from 0.65827 to 0.64879, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 647us/step - loss: 0.3687 - acc: 0.9059 - val_loss: 0.6488 - val_acc: 0.7556\n",
      "Epoch 601/3000\n",
      "510/510 [==============================] - 0s 633us/step - loss: 0.4073 - acc: 0.8784 - val_loss: 0.6506 - val_acc: 0.7556\n",
      "Epoch 602/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.4114 - acc: 0.8843 - val_loss: 0.6494 - val_acc: 0.7556\n",
      "Epoch 603/3000\n",
      "510/510 [==============================] - 0s 558us/step - loss: 0.4040 - acc: 0.8863 - val_loss: 0.6448 - val_acc: 0.7556\n",
      "Epoch 604/3000\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.3922 - acc: 0.8902 - val_loss: 0.6454 - val_acc: 0.7667\n",
      "Epoch 605/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3796 - acc: 0.8839Epoch 00605: val_loss improved from 0.64879 to 0.64253, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.3837 - acc: 0.8804 - val_loss: 0.6425 - val_acc: 0.7778\n",
      "Epoch 606/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.3780 - acc: 0.9059 - val_loss: 0.6468 - val_acc: 0.7778\n",
      "Epoch 607/3000\n",
      "510/510 [==============================] - 0s 574us/step - loss: 0.4013 - acc: 0.8725 - val_loss: 0.6434 - val_acc: 0.7778\n",
      "Epoch 608/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.3618 - acc: 0.9176 - val_loss: 0.6437 - val_acc: 0.7778\n",
      "Epoch 609/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.3891 - acc: 0.8882 - val_loss: 0.6397 - val_acc: 0.7889\n",
      "Epoch 610/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3787 - acc: 0.8973Epoch 00610: val_loss did not improve\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.3732 - acc: 0.9000 - val_loss: 0.6434 - val_acc: 0.7667\n",
      "Epoch 611/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.3935 - acc: 0.8843 - val_loss: 0.6418 - val_acc: 0.8000\n",
      "Epoch 612/3000\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.3773 - acc: 0.8804 - val_loss: 0.6403 - val_acc: 0.7556\n",
      "Epoch 613/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.3906 - acc: 0.8765 - val_loss: 0.6412 - val_acc: 0.7778\n",
      "Epoch 614/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.3958 - acc: 0.8863 - val_loss: 0.6414 - val_acc: 0.7778\n",
      "Epoch 615/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3906 - acc: 0.8884Epoch 00615: val_loss did not improve\n",
      "510/510 [==============================] - 0s 667us/step - loss: 0.3845 - acc: 0.8922 - val_loss: 0.6477 - val_acc: 0.7667\n",
      "Epoch 616/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.3903 - acc: 0.9000 - val_loss: 0.6445 - val_acc: 0.7778\n",
      "Epoch 617/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.3856 - acc: 0.8647 - val_loss: 0.6447 - val_acc: 0.7778\n",
      "Epoch 618/3000\n",
      "510/510 [==============================] - 0s 614us/step - loss: 0.4352 - acc: 0.8784 - val_loss: 0.6433 - val_acc: 0.7667\n",
      "Epoch 619/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.3904 - acc: 0.8902 - val_loss: 0.6440 - val_acc: 0.7556\n",
      "Epoch 620/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3907 - acc: 0.8906Epoch 00620: val_loss did not improve\n",
      "510/510 [==============================] - 0s 627us/step - loss: 0.3866 - acc: 0.8922 - val_loss: 0.6440 - val_acc: 0.7667\n",
      "Epoch 621/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.3921 - acc: 0.8922 - val_loss: 0.6364 - val_acc: 0.7889\n",
      "Epoch 622/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.4017 - acc: 0.8922 - val_loss: 0.6355 - val_acc: 0.7778\n",
      "Epoch 623/3000\n",
      "510/510 [==============================] - 0s 626us/step - loss: 0.3919 - acc: 0.8882 - val_loss: 0.6365 - val_acc: 0.7778\n",
      "Epoch 624/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.4034 - acc: 0.8804 - val_loss: 0.6394 - val_acc: 0.7778\n",
      "Epoch 625/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3988 - acc: 0.8705Epoch 00625: val_loss improved from 0.64253 to 0.63674, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.3823 - acc: 0.8784 - val_loss: 0.6367 - val_acc: 0.7667\n",
      "Epoch 626/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.3899 - acc: 0.8843 - val_loss: 0.6398 - val_acc: 0.7667\n",
      "Epoch 627/3000\n",
      "510/510 [==============================] - 0s 630us/step - loss: 0.3883 - acc: 0.8863 - val_loss: 0.6380 - val_acc: 0.7889\n",
      "Epoch 628/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.3552 - acc: 0.9098 - val_loss: 0.6351 - val_acc: 0.7889\n",
      "Epoch 629/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.3814 - acc: 0.8804 - val_loss: 0.6345 - val_acc: 0.7778\n",
      "Epoch 630/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3539 - acc: 0.9085Epoch 00630: val_loss improved from 0.63674 to 0.63322, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.3603 - acc: 0.9059 - val_loss: 0.6332 - val_acc: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 631/3000\n",
      "510/510 [==============================] - 0s 572us/step - loss: 0.3570 - acc: 0.9059 - val_loss: 0.6324 - val_acc: 0.7778\n",
      "Epoch 632/3000\n",
      "510/510 [==============================] - 0s 590us/step - loss: 0.3593 - acc: 0.9000 - val_loss: 0.6297 - val_acc: 0.7778\n",
      "Epoch 633/3000\n",
      "510/510 [==============================] - 0s 577us/step - loss: 0.3660 - acc: 0.9000 - val_loss: 0.6380 - val_acc: 0.7667\n",
      "Epoch 634/3000\n",
      "510/510 [==============================] - 0s 561us/step - loss: 0.3696 - acc: 0.8902 - val_loss: 0.6413 - val_acc: 0.7667\n",
      "Epoch 635/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3811 - acc: 0.8906Epoch 00635: val_loss did not improve\n",
      "510/510 [==============================] - 0s 534us/step - loss: 0.3736 - acc: 0.8980 - val_loss: 0.6388 - val_acc: 0.7778\n",
      "Epoch 636/3000\n",
      "510/510 [==============================] - 0s 557us/step - loss: 0.3585 - acc: 0.9039 - val_loss: 0.6365 - val_acc: 0.7667\n",
      "Epoch 637/3000\n",
      "510/510 [==============================] - 0s 564us/step - loss: 0.3709 - acc: 0.8980 - val_loss: 0.6349 - val_acc: 0.7778\n",
      "Epoch 638/3000\n",
      "510/510 [==============================] - 0s 563us/step - loss: 0.3638 - acc: 0.9039 - val_loss: 0.6409 - val_acc: 0.7667\n",
      "Epoch 639/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.3647 - acc: 0.9039 - val_loss: 0.6352 - val_acc: 0.7889\n",
      "Epoch 640/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3533 - acc: 0.8973Epoch 00640: val_loss did not improve\n",
      "510/510 [==============================] - 0s 578us/step - loss: 0.3619 - acc: 0.8882 - val_loss: 0.6337 - val_acc: 0.7667\n",
      "Epoch 641/3000\n",
      "510/510 [==============================] - 0s 584us/step - loss: 0.3727 - acc: 0.9020 - val_loss: 0.6313 - val_acc: 0.7667\n",
      "Epoch 642/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.3544 - acc: 0.9039 - val_loss: 0.6340 - val_acc: 0.7667\n",
      "Epoch 643/3000\n",
      "510/510 [==============================] - 0s 565us/step - loss: 0.3645 - acc: 0.8902 - val_loss: 0.6349 - val_acc: 0.7667\n",
      "Epoch 644/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.3452 - acc: 0.9039 - val_loss: 0.6370 - val_acc: 0.7667\n",
      "Epoch 645/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3334 - acc: 0.9219Epoch 00645: val_loss did not improve\n",
      "510/510 [==============================] - 0s 642us/step - loss: 0.3517 - acc: 0.9098 - val_loss: 0.6398 - val_acc: 0.7667\n",
      "Epoch 646/3000\n",
      "510/510 [==============================] - 0s 685us/step - loss: 0.3615 - acc: 0.9039 - val_loss: 0.6387 - val_acc: 0.7667\n",
      "Epoch 647/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.3526 - acc: 0.9157 - val_loss: 0.6376 - val_acc: 0.7778\n",
      "Epoch 648/3000\n",
      "510/510 [==============================] - 0s 613us/step - loss: 0.3532 - acc: 0.8961 - val_loss: 0.6342 - val_acc: 0.7667\n",
      "Epoch 649/3000\n",
      "510/510 [==============================] - 0s 587us/step - loss: 0.3512 - acc: 0.9000 - val_loss: 0.6321 - val_acc: 0.7889\n",
      "Epoch 650/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3532 - acc: 0.9129Epoch 00650: val_loss improved from 0.63322 to 0.62637, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.3555 - acc: 0.9137 - val_loss: 0.6264 - val_acc: 0.7889\n",
      "Epoch 651/3000\n",
      "510/510 [==============================] - 0s 601us/step - loss: 0.3391 - acc: 0.9137 - val_loss: 0.6336 - val_acc: 0.7889\n",
      "Epoch 652/3000\n",
      "510/510 [==============================] - 0s 608us/step - loss: 0.3774 - acc: 0.8882 - val_loss: 0.6348 - val_acc: 0.7889\n",
      "Epoch 653/3000\n",
      "510/510 [==============================] - 0s 571us/step - loss: 0.3711 - acc: 0.8784 - val_loss: 0.6299 - val_acc: 0.7889\n",
      "Epoch 654/3000\n",
      "510/510 [==============================] - 0s 628us/step - loss: 0.3729 - acc: 0.9039 - val_loss: 0.6367 - val_acc: 0.7889\n",
      "Epoch 655/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3552 - acc: 0.9062Epoch 00655: val_loss did not improve\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.3543 - acc: 0.9059 - val_loss: 0.6336 - val_acc: 0.7889\n",
      "Epoch 656/3000\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.3527 - acc: 0.8961 - val_loss: 0.6300 - val_acc: 0.7889\n",
      "Epoch 657/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.3381 - acc: 0.9020 - val_loss: 0.6313 - val_acc: 0.7889\n",
      "Epoch 658/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.3911 - acc: 0.8882 - val_loss: 0.6300 - val_acc: 0.7889\n",
      "Epoch 659/3000\n",
      "510/510 [==============================] - 0s 631us/step - loss: 0.3737 - acc: 0.8941 - val_loss: 0.6297 - val_acc: 0.7889\n",
      "Epoch 660/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3123 - acc: 0.9241Epoch 00660: val_loss did not improve\n",
      "510/510 [==============================] - 0s 605us/step - loss: 0.3208 - acc: 0.9196 - val_loss: 0.6317 - val_acc: 0.7889\n",
      "Epoch 661/3000\n",
      "510/510 [==============================] - 0s 609us/step - loss: 0.3601 - acc: 0.9039 - val_loss: 0.6323 - val_acc: 0.7778\n",
      "Epoch 662/3000\n",
      "510/510 [==============================] - 0s 623us/step - loss: 0.3469 - acc: 0.8902 - val_loss: 0.6349 - val_acc: 0.7889\n",
      "Epoch 663/3000\n",
      "510/510 [==============================] - 0s 589us/step - loss: 0.3544 - acc: 0.9098 - val_loss: 0.6338 - val_acc: 0.7889\n",
      "Epoch 664/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.3407 - acc: 0.9176 - val_loss: 0.6327 - val_acc: 0.7889\n",
      "Epoch 665/3000\n",
      "384/510 [=====================>........] - ETA: 0s - loss: 0.3290 - acc: 0.9245Epoch 00665: val_loss did not improve\n",
      "510/510 [==============================] - 0s 617us/step - loss: 0.3270 - acc: 0.9118 - val_loss: 0.6285 - val_acc: 0.7889\n",
      "Epoch 666/3000\n",
      "510/510 [==============================] - 0s 616us/step - loss: 0.3732 - acc: 0.8824 - val_loss: 0.6278 - val_acc: 0.7889\n",
      "Epoch 667/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.3550 - acc: 0.9020 - val_loss: 0.6274 - val_acc: 0.8000\n",
      "Epoch 668/3000\n",
      "510/510 [==============================] - 0s 629us/step - loss: 0.3739 - acc: 0.8804 - val_loss: 0.6281 - val_acc: 0.7889\n",
      "Epoch 669/3000\n",
      "510/510 [==============================] - 0s 648us/step - loss: 0.3503 - acc: 0.8980 - val_loss: 0.6262 - val_acc: 0.7889\n",
      "Epoch 670/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3527 - acc: 0.8906Epoch 00670: val_loss did not improve\n",
      "510/510 [==============================] - 0s 621us/step - loss: 0.3646 - acc: 0.8824 - val_loss: 0.6265 - val_acc: 0.7889\n",
      "Epoch 671/3000\n",
      "510/510 [==============================] - 0s 622us/step - loss: 0.3832 - acc: 0.8686 - val_loss: 0.6251 - val_acc: 0.8000\n",
      "Epoch 672/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.3618 - acc: 0.8902 - val_loss: 0.6270 - val_acc: 0.7889\n",
      "Epoch 673/3000\n",
      "510/510 [==============================] - 0s 603us/step - loss: 0.3575 - acc: 0.9020 - val_loss: 0.6303 - val_acc: 0.8000\n",
      "Epoch 674/3000\n",
      "510/510 [==============================] - 0s 657us/step - loss: 0.3412 - acc: 0.9059 - val_loss: 0.6286 - val_acc: 0.8000\n",
      "Epoch 675/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3708 - acc: 0.9040Epoch 00675: val_loss did not improve\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.3885 - acc: 0.8922 - val_loss: 0.6313 - val_acc: 0.7889\n",
      "Epoch 676/3000\n",
      "510/510 [==============================] - 0s 606us/step - loss: 0.3276 - acc: 0.9255 - val_loss: 0.6308 - val_acc: 0.7889\n",
      "Epoch 677/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.3342 - acc: 0.9235 - val_loss: 0.6310 - val_acc: 0.7778\n",
      "Epoch 678/3000\n",
      "510/510 [==============================] - 0s 649us/step - loss: 0.3409 - acc: 0.9118 - val_loss: 0.6283 - val_acc: 0.7889\n",
      "Epoch 679/3000\n",
      "510/510 [==============================] - 0s 597us/step - loss: 0.3359 - acc: 0.9157 - val_loss: 0.6231 - val_acc: 0.7889\n",
      "Epoch 680/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3370 - acc: 0.8929Epoch 00680: val_loss improved from 0.62637 to 0.62206, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 646us/step - loss: 0.3372 - acc: 0.8980 - val_loss: 0.6221 - val_acc: 0.8000\n",
      "Epoch 681/3000\n",
      "510/510 [==============================] - 0s 618us/step - loss: 0.3372 - acc: 0.9176 - val_loss: 0.6295 - val_acc: 0.7889\n",
      "Epoch 682/3000\n",
      "510/510 [==============================] - 0s 718us/step - loss: 0.3465 - acc: 0.9078 - val_loss: 0.6227 - val_acc: 0.8000\n",
      "Epoch 683/3000\n",
      "510/510 [==============================] - 0s 708us/step - loss: 0.3629 - acc: 0.8961 - val_loss: 0.6211 - val_acc: 0.8000\n",
      "Epoch 684/3000\n",
      "510/510 [==============================] - 0s 619us/step - loss: 0.3415 - acc: 0.9000 - val_loss: 0.6194 - val_acc: 0.8111\n",
      "Epoch 685/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3850 - acc: 0.8839Epoch 00685: val_loss did not improve\n",
      "510/510 [==============================] - 0s 548us/step - loss: 0.3705 - acc: 0.8922 - val_loss: 0.6239 - val_acc: 0.7889\n",
      "Epoch 686/3000\n",
      "510/510 [==============================] - 0s 582us/step - loss: 0.3339 - acc: 0.9078 - val_loss: 0.6185 - val_acc: 0.8111\n",
      "Epoch 687/3000\n",
      "510/510 [==============================] - 0s 570us/step - loss: 0.3315 - acc: 0.9176 - val_loss: 0.6145 - val_acc: 0.8222\n",
      "Epoch 688/3000\n",
      "510/510 [==============================] - 0s 569us/step - loss: 0.3412 - acc: 0.9157 - val_loss: 0.6163 - val_acc: 0.8000\n",
      "Epoch 689/3000\n",
      "510/510 [==============================] - 0s 636us/step - loss: 0.3430 - acc: 0.8863 - val_loss: 0.6171 - val_acc: 0.8000\n",
      "Epoch 690/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3605 - acc: 0.8996Epoch 00690: val_loss improved from 0.62206 to 0.61785, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 658us/step - loss: 0.3545 - acc: 0.9039 - val_loss: 0.6179 - val_acc: 0.8000\n",
      "Epoch 691/3000\n",
      "510/510 [==============================] - 0s 602us/step - loss: 0.3191 - acc: 0.9176 - val_loss: 0.6125 - val_acc: 0.8222\n",
      "Epoch 692/3000\n",
      "510/510 [==============================] - 0s 673us/step - loss: 0.3454 - acc: 0.9137 - val_loss: 0.6075 - val_acc: 0.8111\n",
      "Epoch 693/3000\n",
      "510/510 [==============================] - 0s 634us/step - loss: 0.3616 - acc: 0.8843 - val_loss: 0.6127 - val_acc: 0.8111\n",
      "Epoch 694/3000\n",
      "510/510 [==============================] - 0s 730us/step - loss: 0.3655 - acc: 0.8980 - val_loss: 0.6188 - val_acc: 0.8000\n",
      "Epoch 695/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3035 - acc: 0.9219Epoch 00695: val_loss improved from 0.61785 to 0.60940, saving model to top_weight.h5\n",
      "510/510 [==============================] - 0s 666us/step - loss: 0.3089 - acc: 0.9176 - val_loss: 0.6094 - val_acc: 0.8111\n",
      "Epoch 696/3000\n",
      "510/510 [==============================] - 0s 585us/step - loss: 0.3338 - acc: 0.9078 - val_loss: 0.6143 - val_acc: 0.8000\n",
      "Epoch 697/3000\n",
      "510/510 [==============================] - 0s 586us/step - loss: 0.3407 - acc: 0.9059 - val_loss: 0.6167 - val_acc: 0.7889\n",
      "Epoch 698/3000\n",
      "510/510 [==============================] - 0s 644us/step - loss: 0.3240 - acc: 0.9216 - val_loss: 0.6141 - val_acc: 0.8000\n",
      "Epoch 699/3000\n",
      "510/510 [==============================] - 0s 559us/step - loss: 0.3562 - acc: 0.8961 - val_loss: 0.6197 - val_acc: 0.8000\n",
      "Epoch 700/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3313 - acc: 0.8929Epoch 00700: val_loss did not improve\n",
      "510/510 [==============================] - 0s 654us/step - loss: 0.3364 - acc: 0.8941 - val_loss: 0.6152 - val_acc: 0.8222\n",
      "Epoch 701/3000\n",
      "510/510 [==============================] - 0s 579us/step - loss: 0.3305 - acc: 0.9157 - val_loss: 0.6197 - val_acc: 0.8111\n",
      "Epoch 702/3000\n",
      "510/510 [==============================] - 0s 588us/step - loss: 0.3144 - acc: 0.9137 - val_loss: 0.6119 - val_acc: 0.8111\n",
      "Epoch 703/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.3262 - acc: 0.9078 - val_loss: 0.6124 - val_acc: 0.8222\n",
      "Epoch 704/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.3322 - acc: 0.9078 - val_loss: 0.6129 - val_acc: 0.8222\n",
      "Epoch 705/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3240 - acc: 0.9085Epoch 00705: val_loss did not improve\n",
      "510/510 [==============================] - 0s 575us/step - loss: 0.3122 - acc: 0.9176 - val_loss: 0.6171 - val_acc: 0.8111\n",
      "Epoch 706/3000\n",
      "510/510 [==============================] - 0s 615us/step - loss: 0.3294 - acc: 0.9137 - val_loss: 0.6135 - val_acc: 0.8222\n",
      "Epoch 707/3000\n",
      "510/510 [==============================] - 0s 594us/step - loss: 0.3358 - acc: 0.9098 - val_loss: 0.6144 - val_acc: 0.8222\n",
      "Epoch 708/3000\n",
      "510/510 [==============================] - 0s 611us/step - loss: 0.3615 - acc: 0.8882 - val_loss: 0.6112 - val_acc: 0.8222\n",
      "Epoch 709/3000\n",
      "510/510 [==============================] - 0s 607us/step - loss: 0.3372 - acc: 0.8980 - val_loss: 0.6143 - val_acc: 0.8000\n",
      "Epoch 710/3000\n",
      "448/510 [=========================>....] - ETA: 0s - loss: 0.3242 - acc: 0.9330Epoch 00710: val_loss did not improve\n",
      "510/510 [==============================] - 0s 666us/step - loss: 0.3272 - acc: 0.9294 - val_loss: 0.6137 - val_acc: 0.8111\n",
      "Epoch 711/3000\n",
      "510/510 [==============================] - 0s 632us/step - loss: 0.3237 - acc: 0.9078 - val_loss: 0.6137 - val_acc: 0.8222\n",
      "Epoch 712/3000\n",
      "510/510 [==============================] - 0s 600us/step - loss: 0.3256 - acc: 0.9118 - val_loss: 0.6124 - val_acc: 0.8222\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(patience=20, mode='auto', monitor='val_loss')\n",
    "checkPoint = ModelCheckpoint(filepath=\"top_weight.h5\", verbose=1, save_best_only=True, save_weights_only=True, period=5)\n",
    "Logs = CSVLogger('logs.csv', separator=',', append=True)\n",
    "history = model.fit(X_train, y_train, epochs=3000, batch_size=64, shuffle=True, validation_split=0.15, callbacks=[checkPoint, Logs, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc28b383590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc289f08b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting 了 QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 506us/step\n",
      "loss: 0.61\n",
      "acc: 77.69%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print 'loss: %.2f'%(loss)\n",
    "print 'acc: %.2f%%'%(acc*100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  6  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  8  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  7  4  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  2  9  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  1  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2 13  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  3  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 12  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  3  9]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD8CAYAAAD5TVjyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc2a4066e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "real = np.argmax(y_test, axis=1)\n",
    "confuse = np.asarray(confusion_matrix(real,pred))\n",
    "print confuse\n",
    "print classification_report(real,pred)\n",
    "confuse = confuse / float(confuse.sum())\n",
    "plt.imshow(confuse, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
