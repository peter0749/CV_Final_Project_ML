{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDNN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('./dataset.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__globals__', 'test_data', 'test_label', '__version__', 'train_label', 'train_data']\n"
     ]
    }
   ],
   "source": [
    "print list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 林敬翔 之銘言：\n",
    "跟助教要的 dataset\n",
    "\n",
    "第一維是數量 第二維是時間序 第三維是資料量\n",
    "\n",
    "Label 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ....\n",
    "\n",
    "1-10左手腕 11-20右手腕 21-30左手臂 31-40右手臂\n",
    "\n",
    "220代表frame數\n",
    "\n",
    "我是取一秒25frame\n",
    "\n",
    "動作數量是以動作為單位沒錯 1*220*40表是某一個動作的完整資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下資料維度\n",
    "\n",
    "data: (?, 220, 40) = (幾筆資料, 時間步, 資料維度)\n",
    "\n",
    "label: (?, 1) = (幾筆資料, 動作label)\n",
    "\n",
    "動作 label: 1~3是第一個動作 1是動作評估為好 2是動作評估為普通 3是動作評估為差 以此類推 4~6是第二個動作 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: __header__\n",
      "not an array\n",
      "key: __globals__\n",
      "not an array\n",
      "key: test_data\n",
      "(790, 220, 40)\n",
      "key: test_label\n",
      "(790, 1)\n",
      "key: __version__\n",
      "not an array\n",
      "key: train_label\n",
      "(3300, 1)\n",
      "key: train_data\n",
      "(3300, 220, 40)\n"
     ]
    }
   ],
   "source": [
    "for key, value in data.iteritems():\n",
    "    print 'key: %s'%key\n",
    "    if hasattr(value , 'shape'):\n",
    "        print value.shape\n",
    "    else:\n",
    "        print 'not an array'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列資料 + 預測 -> RNN? / HMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Keras CuDNNLSTM\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "import keras\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Flatten, Conv1D, MaxPool1D\n",
    "from keras.layers import LSTM, CuDNNLSTM, RepeatVector, TimeDistributed, Bidirectional\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.utils import to_categorical # one-hot encoding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, actions=40, cell_size=128, lstm_layer_n = 3, conv_filters=64, \n",
    "                conv_kernel=7, pooling_step=2, learning_rate=0.01, dropout_r=0.14, \n",
    "                optimizer=keras.optimizers.RMSprop, clipnorm=1., \n",
    "                tLSTM=LSTM, use_temporal_subsampling=False, gaussian_noise_std=1., reg_l2=0.01):\n",
    "    if lstm_layer_n<1: raise ValueError('lstm_layer_n must >= 1')\n",
    "    optimizer = optimizer(lr=learning_rate, clipnorm=clipnorm)\n",
    "    model = Sequential()\n",
    "    model.add(GaussianNoise(gaussian_noise_std, input_shape=input_shape))\n",
    "    if use_temporal_subsampling:\n",
    "        model.add(Conv1D(filters=conv_filters, kernel_size=conv_kernel, padding='same', activation='relu'))\n",
    "        model.add(MaxPool1D(pool_size=pooling_step))\n",
    "        model.add(Dropout(dropout_r))\n",
    "    for _ in xrange(1, lstm_layer_n):\n",
    "        model.add(#Bidirectional(\n",
    "                                tLSTM(cell_size, return_sequences=True,\n",
    "                                      unit_forget_bias=True, recurrent_regularizer=regularizers.l2(reg_l2))#,\n",
    "                                #merge_mode='sum'\n",
    "                               )#)\n",
    "        model.add(Dropout(dropout_r))\n",
    "    model.add(#Bidirectional(\n",
    "                            tLSTM(cell_size, return_sequences=False, unit_forget_bias=True, \n",
    "                                  recurrent_regularizer=regularizers.l2(reg_l2))#,\n",
    "                            #merge_mode='sum'\n",
    "                           )#)\n",
    "    model.add(Dropout(dropout_r))\n",
    "    model.add(Dense(actions, activation='softmax'))\n",
    "    model.compile(\n",
    "            loss = 'categorical_crossentropy',\n",
    "            optimizer=optimizer, metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info for 0 pca: 1.00\n",
      "info for 1 pca: 1.00\n",
      "info for 2 pca: 1.00\n",
      "info for 3 pca: 1.00\n",
      "info for 4 pca: 1.00\n",
      "info for 5 pca: 1.00\n",
      "info for 6 pca: 1.00\n",
      "info for 7 pca: 1.00\n",
      "info for 8 pca: 1.00\n",
      "info for 9 pca: 1.00\n",
      "info for 10 pca: 1.00\n",
      "info for 11 pca: 1.00\n",
      "info for 12 pca: 1.00\n",
      "info for 13 pca: 1.00\n",
      "info for 14 pca: 1.00\n",
      "info for 15 pca: 1.00\n",
      "info for 16 pca: 1.00\n",
      "info for 17 pca: 1.00\n",
      "info for 18 pca: 1.00\n",
      "info for 19 pca: 1.00\n",
      "info for 20 pca: 1.00\n",
      "info for 21 pca: 1.00\n",
      "info for 22 pca: 1.00\n",
      "info for 23 pca: 1.00\n",
      "info for 24 pca: 1.00\n",
      "info for 25 pca: 1.00\n",
      "info for 26 pca: 1.00\n",
      "info for 27 pca: 1.00\n",
      "info for 28 pca: 1.00\n",
      "info for 29 pca: 1.00\n",
      "info for 30 pca: 1.00\n",
      "info for 31 pca: 1.00\n",
      "info for 32 pca: 1.00\n",
      "info for 33 pca: 1.00\n",
      "info for 34 pca: 1.00\n",
      "info for 35 pca: 1.00\n",
      "info for 36 pca: 1.00\n",
      "info for 37 pca: 1.00\n",
      "info for 38 pca: 1.00\n",
      "info for 39 pca: 1.00\n",
      "info for 40 pca: 1.00\n",
      "info for 41 pca: 1.00\n",
      "info for 42 pca: 1.00\n",
      "info for 43 pca: 1.00\n",
      "info for 44 pca: 1.00\n",
      "info for 45 pca: 1.00\n",
      "info for 46 pca: 1.00\n",
      "info for 47 pca: 1.00\n",
      "info for 48 pca: 1.00\n",
      "info for 49 pca: 1.00\n",
      "info for 50 pca: 1.00\n",
      "info for 51 pca: 1.00\n",
      "info for 52 pca: 1.00\n",
      "info for 53 pca: 1.00\n",
      "info for 54 pca: 1.00\n",
      "info for 55 pca: 1.00\n",
      "info for 56 pca: 1.00\n",
      "info for 57 pca: 1.00\n",
      "info for 58 pca: 1.00\n",
      "info for 59 pca: 1.00\n",
      "info for 60 pca: 1.00\n",
      "info for 61 pca: 1.00\n",
      "info for 62 pca: 1.00\n",
      "info for 63 pca: 1.00\n",
      "info for 64 pca: 1.00\n",
      "info for 65 pca: 1.00\n",
      "info for 66 pca: 1.00\n",
      "info for 67 pca: 1.00\n",
      "info for 68 pca: 1.00\n",
      "info for 69 pca: 1.00\n",
      "info for 70 pca: 1.00\n",
      "info for 71 pca: 1.00\n",
      "info for 72 pca: 1.00\n",
      "info for 73 pca: 1.00\n",
      "info for 74 pca: 1.00\n",
      "info for 75 pca: 1.00\n",
      "info for 76 pca: 1.00\n",
      "info for 77 pca: 1.00\n",
      "info for 78 pca: 1.00\n",
      "info for 79 pca: 1.00\n",
      "info for 80 pca: 1.00\n",
      "info for 81 pca: 1.00\n",
      "info for 82 pca: 1.00\n",
      "info for 83 pca: 1.00\n",
      "info for 84 pca: 1.00\n",
      "info for 85 pca: 1.00\n",
      "info for 86 pca: 1.00\n",
      "info for 87 pca: 1.00\n",
      "info for 88 pca: 1.00\n",
      "info for 89 pca: 1.00\n",
      "info for 90 pca: 1.00\n",
      "info for 91 pca: 1.00\n",
      "info for 92 pca: 1.00\n",
      "info for 93 pca: 1.00\n",
      "info for 94 pca: 1.00\n",
      "info for 95 pca: 1.00\n",
      "info for 96 pca: 1.00\n",
      "info for 97 pca: 1.00\n",
      "info for 98 pca: 1.00\n",
      "info for 99 pca: 1.00\n",
      "info for 100 pca: 1.00\n",
      "info for 101 pca: 1.00\n",
      "info for 102 pca: 1.00\n",
      "info for 103 pca: 1.00\n",
      "info for 104 pca: 1.00\n",
      "info for 105 pca: 1.00\n",
      "info for 106 pca: 1.00\n",
      "info for 107 pca: 1.00\n",
      "info for 108 pca: 1.00\n",
      "info for 109 pca: 1.00\n",
      "info for 110 pca: 1.00\n",
      "info for 111 pca: 1.00\n",
      "info for 112 pca: 1.00\n",
      "info for 113 pca: 1.00\n",
      "info for 114 pca: 1.00\n",
      "info for 115 pca: 1.00\n",
      "info for 116 pca: 1.00\n",
      "info for 117 pca: 1.00\n",
      "info for 118 pca: 1.00\n",
      "info for 119 pca: 1.00\n",
      "info for 120 pca: 1.00\n",
      "info for 121 pca: 1.00\n",
      "info for 122 pca: 1.00\n",
      "info for 123 pca: 1.00\n",
      "info for 124 pca: 1.00\n",
      "info for 125 pca: 1.00\n",
      "info for 126 pca: 1.00\n",
      "info for 127 pca: 1.00\n",
      "info for 128 pca: 1.00\n",
      "info for 129 pca: 1.00\n",
      "info for 130 pca: 1.00\n",
      "info for 131 pca: 1.00\n",
      "info for 132 pca: 1.00\n",
      "info for 133 pca: 1.00\n",
      "info for 134 pca: 1.00\n",
      "info for 135 pca: 1.00\n",
      "info for 136 pca: 1.00\n",
      "info for 137 pca: 1.00\n",
      "info for 138 pca: 1.00\n",
      "info for 139 pca: 1.00\n",
      "info for 140 pca: 1.00\n",
      "info for 141 pca: 1.00\n",
      "info for 142 pca: 1.00\n",
      "info for 143 pca: 1.00\n",
      "info for 144 pca: 1.00\n",
      "info for 145 pca: 1.00\n",
      "info for 146 pca: 1.00\n",
      "info for 147 pca: 1.00\n",
      "info for 148 pca: 1.00\n",
      "info for 149 pca: 1.00\n",
      "info for 150 pca: 1.00\n",
      "info for 151 pca: 1.00\n",
      "info for 152 pca: 1.00\n",
      "info for 153 pca: 1.00\n",
      "info for 154 pca: 1.00\n",
      "info for 155 pca: 1.00\n",
      "info for 156 pca: 1.00\n",
      "info for 157 pca: 1.00\n",
      "info for 158 pca: 1.00\n",
      "info for 159 pca: 1.00\n",
      "info for 160 pca: 1.00\n",
      "info for 161 pca: 1.00\n",
      "info for 162 pca: 1.00\n",
      "info for 163 pca: 1.00\n",
      "info for 164 pca: 1.00\n",
      "info for 165 pca: 1.00\n",
      "info for 166 pca: 1.00\n",
      "info for 167 pca: 1.00\n",
      "info for 168 pca: 1.00\n",
      "info for 169 pca: 1.00\n",
      "info for 170 pca: 1.00\n",
      "info for 171 pca: 1.00\n",
      "info for 172 pca: 1.00\n",
      "info for 173 pca: 1.00\n",
      "info for 174 pca: 1.00\n",
      "info for 175 pca: 1.00\n",
      "info for 176 pca: 1.00\n",
      "info for 177 pca: 1.00\n",
      "info for 178 pca: 1.00\n",
      "info for 179 pca: 1.00\n",
      "info for 180 pca: 1.00\n",
      "info for 181 pca: 1.00\n",
      "info for 182 pca: 1.00\n",
      "info for 183 pca: 1.00\n",
      "info for 184 pca: 1.00\n",
      "info for 185 pca: 1.00\n",
      "info for 186 pca: 1.00\n",
      "info for 187 pca: 1.00\n",
      "info for 188 pca: 1.00\n",
      "info for 189 pca: 1.00\n",
      "info for 190 pca: 1.00\n",
      "info for 191 pca: 1.00\n",
      "info for 192 pca: 1.00\n",
      "info for 193 pca: 1.00\n",
      "info for 194 pca: 1.00\n",
      "info for 195 pca: 1.00\n",
      "info for 196 pca: 1.00\n",
      "info for 197 pca: 1.00\n",
      "info for 198 pca: 1.00\n",
      "info for 199 pca: 1.00\n",
      "info for 200 pca: 1.00\n",
      "info for 201 pca: 1.00\n",
      "info for 202 pca: 1.00\n",
      "info for 203 pca: 1.00\n",
      "info for 204 pca: 1.00\n",
      "info for 205 pca: 1.00\n",
      "info for 206 pca: 1.00\n",
      "info for 207 pca: 1.00\n",
      "info for 208 pca: 1.00\n",
      "info for 209 pca: 1.00\n",
      "info for 210 pca: 1.00\n",
      "info for 211 pca: 1.00\n",
      "info for 212 pca: 1.00\n",
      "info for 213 pca: 1.00\n",
      "info for 214 pca: 1.00\n",
      "info for 215 pca: 1.00\n",
      "info for 216 pca: 1.00\n",
      "info for 217 pca: 1.00\n",
      "info for 218 pca: 1.00\n",
      "info for 219 pca: 1.00\n",
      "(3300, 220, 30)\n",
      "(790, 220, 30)\n",
      "(3300, 12)\n",
      "(790, 12)\n",
      "49.25015918\n",
      "5.01250496832e-09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = data['train_data'], data['test_data'], data['train_label'], data['test_label']\n",
    "label_enc = OHE(sparse=False) # One-hot encoder, which has attribute [transform, inverse_transform]\n",
    "y_test = label_enc.fit_transform(y_test)\n",
    "y_train = label_enc.transform(y_train)\n",
    "\n",
    "pcas = []\n",
    "n_comp = 30\n",
    "X_train_new = np.zeros((X_train.shape[0], X_train.shape[1], n_comp))\n",
    "X_test_new = np.zeros((X_test.shape[0], X_test.shape[1], n_comp))\n",
    "for i in xrange(X_train.shape[-2]):\n",
    "    pca = PCA(n_components=n_comp).fit(X_train[:,i,:])\n",
    "    pcas.append( (pca, np.sum(pca.explained_variance_ratio_)) )\n",
    "    X_train_new[:,i,:] = pcas[-1][0].transform(X_train[:,i,:])\n",
    "    X_test_new[:,i,:] = pcas[-1][0].transform(X_test[:,i,:])\n",
    "\n",
    "X_train = X_train_new\n",
    "X_test = X_test_new\n",
    "del X_train_new\n",
    "del X_test_new\n",
    "\n",
    "for i, pca in enumerate(pcas):\n",
    "    print 'info for %d pca: %.2f'%(i,pca[1])\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape\n",
    "print X_train.std()\n",
    "print np.abs(X_train).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pcas.pkl','wb') as pak:\n",
    "    pickle.dump(pcas, pak, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_1 (GaussianNo (None, 220, 30)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 220, 100)          52800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 220, 100)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 100)               80800     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 134,812\n",
      "Trainable params: 134,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(input_shape=X_test.shape[1:] ,actions=y_test.shape[-1], cell_size=100, lstm_layer_n=2,\n",
    "                    learning_rate=0.0001, dropout_r=0.5,\n",
    "                    tLSTM=CuDNNLSTM if USE_CUDNN else LSTM, use_temporal_subsampling=False,\n",
    "                    gaussian_noise_std=1e-5, reg_l2=0.04\n",
    "                   )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2805 samples, validate on 495 samples\n",
      "Epoch 1/3000\n",
      "2805/2805 [==============================] - 5s 2ms/step - loss: 9.8000 - acc: 0.1244 - val_loss: 9.1708 - val_acc: 0.1717\n",
      "Epoch 2/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 8.7567 - acc: 0.1847 - val_loss: 8.1954 - val_acc: 0.2303\n",
      "Epoch 3/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 7.8262 - acc: 0.2385 - val_loss: 7.3186 - val_acc: 0.2929\n",
      "Epoch 4/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 7.0163 - acc: 0.2756 - val_loss: 6.5638 - val_acc: 0.3313\n",
      "Epoch 5/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 6.3068 - acc: 0.2954Epoch 00005: val_loss improved from inf to 5.86626, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 6.3005 - acc: 0.2973 - val_loss: 5.8663 - val_acc: 0.3576\n",
      "Epoch 6/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 5.6571 - acc: 0.3112 - val_loss: 5.2590 - val_acc: 0.3697\n",
      "Epoch 7/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 5.0884 - acc: 0.3519 - val_loss: 4.7443 - val_acc: 0.3838\n",
      "Epoch 8/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 4.5919 - acc: 0.3540 - val_loss: 4.2681 - val_acc: 0.3879\n",
      "Epoch 9/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 4.1419 - acc: 0.3815 - val_loss: 3.8688 - val_acc: 0.3879\n",
      "Epoch 10/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 3.7854 - acc: 0.3746Epoch 00010: val_loss improved from 5.86626 to 3.51032, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 3.7802 - acc: 0.3761 - val_loss: 3.5103 - val_acc: 0.3980\n",
      "Epoch 11/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 3.4220 - acc: 0.4036 - val_loss: 3.1984 - val_acc: 0.4182\n",
      "Epoch 12/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 3.1145 - acc: 0.4168 - val_loss: 2.9159 - val_acc: 0.4343\n",
      "Epoch 13/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 2.8721 - acc: 0.4103 - val_loss: 2.6797 - val_acc: 0.4505\n",
      "Epoch 14/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 2.6361 - acc: 0.4228 - val_loss: 2.4788 - val_acc: 0.4667\n",
      "Epoch 15/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 2.4455 - acc: 0.4357Epoch 00015: val_loss improved from 3.51032 to 2.29853, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 2.4427 - acc: 0.4371 - val_loss: 2.2985 - val_acc: 0.4687\n",
      "Epoch 16/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 2.2844 - acc: 0.4585 - val_loss: 2.1403 - val_acc: 0.4707\n",
      "Epoch 17/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 2.0950 - acc: 0.4852 - val_loss: 1.9994 - val_acc: 0.4848\n",
      "Epoch 18/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.9721 - acc: 0.4952 - val_loss: 1.8825 - val_acc: 0.4788\n",
      "Epoch 19/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.8750 - acc: 0.4795 - val_loss: 1.7890 - val_acc: 0.4848\n",
      "Epoch 20/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 1.7805 - acc: 0.4880Epoch 00020: val_loss improved from 2.29853 to 1.70107, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.7815 - acc: 0.4859 - val_loss: 1.7011 - val_acc: 0.4949\n",
      "Epoch 21/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.6779 - acc: 0.5187 - val_loss: 1.6274 - val_acc: 0.4909\n",
      "Epoch 22/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.5985 - acc: 0.5234 - val_loss: 1.5530 - val_acc: 0.5172\n",
      "Epoch 23/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.5503 - acc: 0.5241 - val_loss: 1.4961 - val_acc: 0.5131\n",
      "Epoch 24/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.4823 - acc: 0.5433 - val_loss: 1.4465 - val_acc: 0.5192\n",
      "Epoch 25/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 1.4366 - acc: 0.5447Epoch 00025: val_loss improved from 1.70107 to 1.39859, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.4363 - acc: 0.5462 - val_loss: 1.3986 - val_acc: 0.5333\n",
      "Epoch 26/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.3911 - acc: 0.5529 - val_loss: 1.3626 - val_acc: 0.5253\n",
      "Epoch 27/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.3499 - acc: 0.5640 - val_loss: 1.3184 - val_acc: 0.5293\n",
      "Epoch 28/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.3005 - acc: 0.5736 - val_loss: 1.2851 - val_acc: 0.5394\n",
      "Epoch 29/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.2636 - acc: 0.5879 - val_loss: 1.2606 - val_acc: 0.5394\n",
      "Epoch 30/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 1.2471 - acc: 0.5843Epoch 00030: val_loss improved from 1.39859 to 1.23412, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.2466 - acc: 0.5847 - val_loss: 1.2341 - val_acc: 0.5636\n",
      "Epoch 31/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.2074 - acc: 0.5971 - val_loss: 1.2028 - val_acc: 0.5677\n",
      "Epoch 32/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.1773 - acc: 0.6093 - val_loss: 1.1821 - val_acc: 0.5677\n",
      "Epoch 33/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.1628 - acc: 0.6185 - val_loss: 1.1549 - val_acc: 0.5838\n",
      "Epoch 34/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.1325 - acc: 0.6182 - val_loss: 1.1411 - val_acc: 0.5879\n",
      "Epoch 35/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 1.1091 - acc: 0.6279Epoch 00035: val_loss improved from 1.23412 to 1.11537, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.1073 - acc: 0.6299 - val_loss: 1.1154 - val_acc: 0.6000\n",
      "Epoch 36/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.0709 - acc: 0.6485 - val_loss: 1.0935 - val_acc: 0.6081\n",
      "Epoch 37/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.0567 - acc: 0.6531 - val_loss: 1.0759 - val_acc: 0.6182\n",
      "Epoch 38/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.0318 - acc: 0.6635 - val_loss: 1.0636 - val_acc: 0.6081\n",
      "Epoch 39/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.0301 - acc: 0.6638 - val_loss: 1.0460 - val_acc: 0.6202\n",
      "Epoch 40/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 1.0049 - acc: 0.6653Epoch 00040: val_loss improved from 1.11537 to 1.03054, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 1.0055 - acc: 0.6645 - val_loss: 1.0305 - val_acc: 0.6323\n",
      "Epoch 41/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.9649 - acc: 0.7005 - val_loss: 1.0199 - val_acc: 0.6404\n",
      "Epoch 42/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.9673 - acc: 0.6809 - val_loss: 1.0082 - val_acc: 0.6626\n",
      "Epoch 43/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.9528 - acc: 0.7002 - val_loss: 0.9998 - val_acc: 0.6566\n",
      "Epoch 44/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.9377 - acc: 0.6959 - val_loss: 0.9926 - val_acc: 0.6485\n",
      "Epoch 45/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.9269 - acc: 0.7126Epoch 00045: val_loss improved from 1.03054 to 0.98135, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.9249 - acc: 0.7141 - val_loss: 0.9814 - val_acc: 0.6525\n",
      "Epoch 46/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.8954 - acc: 0.7112 - val_loss: 0.9719 - val_acc: 0.6566\n",
      "Epoch 47/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.8728 - acc: 0.7187 - val_loss: 0.9390 - val_acc: 0.6808\n",
      "Epoch 48/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.8680 - acc: 0.7180 - val_loss: 0.9358 - val_acc: 0.6869\n",
      "Epoch 49/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.8307 - acc: 0.7440 - val_loss: 0.9103 - val_acc: 0.6848\n",
      "Epoch 50/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.8372 - acc: 0.7409Epoch 00050: val_loss improved from 0.98135 to 0.90865, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.8372 - acc: 0.7415 - val_loss: 0.9087 - val_acc: 0.6909\n",
      "Epoch 51/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.8252 - acc: 0.7383 - val_loss: 0.9162 - val_acc: 0.6909\n",
      "Epoch 52/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.8219 - acc: 0.7419 - val_loss: 0.9008 - val_acc: 0.7051\n",
      "Epoch 53/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7891 - acc: 0.7554 - val_loss: 0.9048 - val_acc: 0.6828\n",
      "Epoch 54/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7777 - acc: 0.7540 - val_loss: 0.9041 - val_acc: 0.7051\n",
      "Epoch 55/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.7891 - acc: 0.7536Epoch 00055: val_loss improved from 0.90865 to 0.89767, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7891 - acc: 0.7540 - val_loss: 0.8977 - val_acc: 0.7071\n",
      "Epoch 56/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7639 - acc: 0.7643 - val_loss: 0.8863 - val_acc: 0.7091\n",
      "Epoch 57/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7575 - acc: 0.7658 - val_loss: 0.9052 - val_acc: 0.6929\n",
      "Epoch 58/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7396 - acc: 0.7686 - val_loss: 0.8752 - val_acc: 0.7091\n",
      "Epoch 59/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7234 - acc: 0.7790 - val_loss: 0.8596 - val_acc: 0.7192\n",
      "Epoch 60/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.7271 - acc: 0.7812Epoch 00060: val_loss improved from 0.89767 to 0.87364, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7271 - acc: 0.7818 - val_loss: 0.8736 - val_acc: 0.7212\n",
      "Epoch 61/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7156 - acc: 0.7818 - val_loss: 0.8744 - val_acc: 0.7152\n",
      "Epoch 62/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6921 - acc: 0.7968 - val_loss: 0.8633 - val_acc: 0.7152\n",
      "Epoch 63/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.7004 - acc: 0.7943 - val_loss: 0.8429 - val_acc: 0.7313\n",
      "Epoch 64/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6702 - acc: 0.7897 - val_loss: 0.8504 - val_acc: 0.7232\n",
      "Epoch 65/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.6709 - acc: 0.8038Epoch 00065: val_loss improved from 0.87364 to 0.85155, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6697 - acc: 0.8050 - val_loss: 0.8515 - val_acc: 0.7313\n",
      "Epoch 66/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.6675 - acc: 0.7911 - val_loss: 0.8600 - val_acc: 0.7212\n",
      "Epoch 67/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6510 - acc: 0.8150 - val_loss: 0.8710 - val_acc: 0.7293\n",
      "Epoch 68/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6435 - acc: 0.8171 - val_loss: 0.8442 - val_acc: 0.7172\n",
      "Epoch 69/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6399 - acc: 0.8057 - val_loss: 0.8207 - val_acc: 0.7374\n",
      "Epoch 70/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.6201 - acc: 0.8205Epoch 00070: val_loss improved from 0.85155 to 0.83323, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6186 - acc: 0.8214 - val_loss: 0.8332 - val_acc: 0.7293\n",
      "Epoch 71/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6146 - acc: 0.8210 - val_loss: 0.8345 - val_acc: 0.7212\n",
      "Epoch 72/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6170 - acc: 0.8217 - val_loss: 0.8167 - val_acc: 0.7475\n",
      "Epoch 73/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6003 - acc: 0.8353 - val_loss: 0.7901 - val_acc: 0.7535\n",
      "Epoch 74/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.6078 - acc: 0.8217 - val_loss: 0.8049 - val_acc: 0.7414\n",
      "Epoch 75/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.8412Epoch 00075: val_loss improved from 0.83323 to 0.81869, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5709 - acc: 0.8414 - val_loss: 0.8187 - val_acc: 0.7293\n",
      "Epoch 76/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5691 - acc: 0.8339 - val_loss: 0.7942 - val_acc: 0.7475\n",
      "Epoch 77/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5749 - acc: 0.8403 - val_loss: 0.7975 - val_acc: 0.7414\n",
      "Epoch 78/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5764 - acc: 0.8364 - val_loss: 0.8186 - val_acc: 0.7394\n",
      "Epoch 79/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5483 - acc: 0.8535 - val_loss: 0.8077 - val_acc: 0.7374\n",
      "Epoch 80/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.8532Epoch 00080: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5395 - acc: 0.8531 - val_loss: 0.8254 - val_acc: 0.7273\n",
      "Epoch 81/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5311 - acc: 0.8549 - val_loss: 0.8030 - val_acc: 0.7414\n",
      "Epoch 82/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5518 - acc: 0.8478 - val_loss: 0.7810 - val_acc: 0.7475\n",
      "Epoch 83/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5315 - acc: 0.8503 - val_loss: 0.8021 - val_acc: 0.7394\n",
      "Epoch 84/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5133 - acc: 0.8642 - val_loss: 0.8267 - val_acc: 0.7273\n",
      "Epoch 85/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.8583Epoch 00085: val_loss improved from 0.81869 to 0.77954, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5147 - acc: 0.8585 - val_loss: 0.7795 - val_acc: 0.7455\n",
      "Epoch 86/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5016 - acc: 0.8617 - val_loss: 0.8205 - val_acc: 0.7394\n",
      "Epoch 87/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4994 - acc: 0.8602 - val_loss: 0.8158 - val_acc: 0.7434\n",
      "Epoch 88/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.5174 - acc: 0.8588 - val_loss: 0.7832 - val_acc: 0.7556\n",
      "Epoch 89/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4824 - acc: 0.8709 - val_loss: 0.7707 - val_acc: 0.7495\n",
      "Epoch 90/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.4861 - acc: 0.8685Epoch 00090: val_loss improved from 0.77954 to 0.77808, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.4857 - acc: 0.8699 - val_loss: 0.7781 - val_acc: 0.7576\n",
      "Epoch 91/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4790 - acc: 0.8720 - val_loss: 0.7718 - val_acc: 0.7596\n",
      "Epoch 92/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4907 - acc: 0.8720 - val_loss: 0.7838 - val_acc: 0.7576\n",
      "Epoch 93/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4786 - acc: 0.8649 - val_loss: 0.7735 - val_acc: 0.7434\n",
      "Epoch 94/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4675 - acc: 0.8766 - val_loss: 0.7800 - val_acc: 0.7455\n",
      "Epoch 95/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.4723 - acc: 0.8717Epoch 00095: val_loss improved from 0.77808 to 0.74716, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4739 - acc: 0.8713 - val_loss: 0.7472 - val_acc: 0.7596\n",
      "Epoch 96/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4683 - acc: 0.8706 - val_loss: 0.7659 - val_acc: 0.7677\n",
      "Epoch 97/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4626 - acc: 0.8766 - val_loss: 0.7476 - val_acc: 0.7758\n",
      "Epoch 98/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4540 - acc: 0.8813 - val_loss: 0.7828 - val_acc: 0.7535\n",
      "Epoch 99/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4540 - acc: 0.8806 - val_loss: 0.7804 - val_acc: 0.7515\n",
      "Epoch 100/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8859Epoch 00100: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4316 - acc: 0.8859 - val_loss: 0.7964 - val_acc: 0.7475\n",
      "Epoch 101/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4238 - acc: 0.8988 - val_loss: 0.7664 - val_acc: 0.7556\n",
      "Epoch 102/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4186 - acc: 0.8973 - val_loss: 0.7903 - val_acc: 0.7374\n",
      "Epoch 103/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4426 - acc: 0.8834 - val_loss: 0.7768 - val_acc: 0.7596\n",
      "Epoch 104/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4247 - acc: 0.8927 - val_loss: 0.7994 - val_acc: 0.7535\n",
      "Epoch 105/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8932Epoch 00105: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4088 - acc: 0.8920 - val_loss: 0.7602 - val_acc: 0.7677\n",
      "Epoch 106/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4149 - acc: 0.8952 - val_loss: 0.8029 - val_acc: 0.7535\n",
      "Epoch 107/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4074 - acc: 0.8973 - val_loss: 0.8221 - val_acc: 0.7475\n",
      "Epoch 108/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3969 - acc: 0.9012 - val_loss: 0.7646 - val_acc: 0.7758\n",
      "Epoch 109/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4011 - acc: 0.9020 - val_loss: 0.7681 - val_acc: 0.7697\n",
      "Epoch 110/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8953Epoch 00110: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3929 - acc: 0.8966 - val_loss: 0.7783 - val_acc: 0.7778\n",
      "Epoch 111/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.4003 - acc: 0.8995 - val_loss: 0.8167 - val_acc: 0.7596\n",
      "Epoch 112/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3813 - acc: 0.9027 - val_loss: 0.7940 - val_acc: 0.7576\n",
      "Epoch 113/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3845 - acc: 0.9084 - val_loss: 0.7910 - val_acc: 0.7838\n",
      "Epoch 114/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3803 - acc: 0.9080 - val_loss: 0.7917 - val_acc: 0.7697\n",
      "Epoch 115/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.9033Epoch 00115: val_loss improved from 0.74716 to 0.74368, saving model to top_weight.h5\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3823 - acc: 0.9034 - val_loss: 0.7437 - val_acc: 0.7899\n",
      "Epoch 116/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3733 - acc: 0.9080 - val_loss: 0.7383 - val_acc: 0.7859\n",
      "Epoch 117/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3748 - acc: 0.9105 - val_loss: 0.7617 - val_acc: 0.7939\n",
      "Epoch 118/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3631 - acc: 0.9141 - val_loss: 0.7773 - val_acc: 0.7737\n",
      "Epoch 119/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3487 - acc: 0.9194 - val_loss: 0.7499 - val_acc: 0.7960\n",
      "Epoch 120/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.9088Epoch 00120: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3646 - acc: 0.9091 - val_loss: 0.7684 - val_acc: 0.7859\n",
      "Epoch 121/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3564 - acc: 0.9144 - val_loss: 0.7918 - val_acc: 0.7717\n",
      "Epoch 122/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3569 - acc: 0.9112 - val_loss: 0.7623 - val_acc: 0.7879\n",
      "Epoch 123/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3383 - acc: 0.9205 - val_loss: 0.7897 - val_acc: 0.7778\n",
      "Epoch 124/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3438 - acc: 0.9194 - val_loss: 0.7877 - val_acc: 0.7859\n",
      "Epoch 125/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.9164Epoch 00125: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3490 - acc: 0.9159 - val_loss: 0.7879 - val_acc: 0.7758\n",
      "Epoch 126/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3449 - acc: 0.9144 - val_loss: 0.7781 - val_acc: 0.7939\n",
      "Epoch 127/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3402 - acc: 0.9176 - val_loss: 0.7820 - val_acc: 0.7879\n",
      "Epoch 128/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3505 - acc: 0.9180 - val_loss: 0.7718 - val_acc: 0.8020\n",
      "Epoch 129/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3456 - acc: 0.9166 - val_loss: 0.7976 - val_acc: 0.7879\n",
      "Epoch 130/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9273Epoch 00130: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3182 - acc: 0.9273 - val_loss: 0.8022 - val_acc: 0.7859\n",
      "Epoch 131/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3175 - acc: 0.9251 - val_loss: 0.8172 - val_acc: 0.7778\n",
      "Epoch 132/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3286 - acc: 0.9166 - val_loss: 0.7826 - val_acc: 0.7919\n",
      "Epoch 133/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3206 - acc: 0.9258 - val_loss: 0.7625 - val_acc: 0.7939\n",
      "Epoch 134/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3165 - acc: 0.9234 - val_loss: 0.8059 - val_acc: 0.7879\n",
      "Epoch 135/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9331Epoch 00135: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2994 - acc: 0.9326 - val_loss: 0.7756 - val_acc: 0.8040\n",
      "Epoch 136/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3135 - acc: 0.9280 - val_loss: 0.7819 - val_acc: 0.8000\n",
      "Epoch 137/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3189 - acc: 0.9294 - val_loss: 0.8025 - val_acc: 0.7899\n",
      "Epoch 138/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3074 - acc: 0.9308 - val_loss: 0.7744 - val_acc: 0.7879\n",
      "Epoch 139/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3154 - acc: 0.9255 - val_loss: 0.7609 - val_acc: 0.7879\n",
      "Epoch 140/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9313Epoch 00140: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3036 - acc: 0.9312 - val_loss: 0.7684 - val_acc: 0.8000\n",
      "Epoch 141/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.3086 - acc: 0.9223 - val_loss: 0.7861 - val_acc: 0.8000\n",
      "Epoch 142/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2783 - acc: 0.9387 - val_loss: 0.8033 - val_acc: 0.7919\n",
      "Epoch 143/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2951 - acc: 0.9365 - val_loss: 0.7451 - val_acc: 0.7879\n",
      "Epoch 144/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2919 - acc: 0.9283 - val_loss: 0.7403 - val_acc: 0.8162\n",
      "Epoch 145/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9379Epoch 00145: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2846 - acc: 0.9373 - val_loss: 0.8532 - val_acc: 0.7758\n",
      "Epoch 146/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2946 - acc: 0.9316 - val_loss: 0.7987 - val_acc: 0.7778\n",
      "Epoch 147/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2874 - acc: 0.9326 - val_loss: 0.8340 - val_acc: 0.7818\n",
      "Epoch 148/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2822 - acc: 0.9362 - val_loss: 0.7949 - val_acc: 0.7899\n",
      "Epoch 149/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2814 - acc: 0.9348 - val_loss: 0.8056 - val_acc: 0.7939\n",
      "Epoch 150/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9382Epoch 00150: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2845 - acc: 0.9369 - val_loss: 0.8049 - val_acc: 0.7899\n",
      "Epoch 151/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2966 - acc: 0.9301 - val_loss: 0.7855 - val_acc: 0.7939\n",
      "Epoch 152/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2755 - acc: 0.9355 - val_loss: 0.7906 - val_acc: 0.7899\n",
      "Epoch 153/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2840 - acc: 0.9337 - val_loss: 0.7591 - val_acc: 0.7960\n",
      "Epoch 154/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2814 - acc: 0.9365 - val_loss: 0.8466 - val_acc: 0.7798\n",
      "Epoch 155/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.9328Epoch 00155: val_loss did not improve\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2763 - acc: 0.9330 - val_loss: 0.8244 - val_acc: 0.7919\n",
      "Epoch 156/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2700 - acc: 0.9444 - val_loss: 0.8094 - val_acc: 0.7899\n",
      "Epoch 157/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2620 - acc: 0.9405 - val_loss: 0.8565 - val_acc: 0.7879\n",
      "Epoch 158/3000\n",
      "2805/2805 [==============================] - 3s 1ms/step - loss: 0.2697 - acc: 0.9415 - val_loss: 0.7598 - val_acc: 0.7960\n",
      "Epoch 159/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2579 - acc: 0.9451 - val_loss: 0.7668 - val_acc: 0.8081\n",
      "Epoch 160/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9448Epoch 00160: val_loss did not improve\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2516 - acc: 0.9430 - val_loss: 0.8183 - val_acc: 0.8000\n",
      "Epoch 161/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2603 - acc: 0.9440 - val_loss: 0.8225 - val_acc: 0.7980\n",
      "Epoch 162/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2735 - acc: 0.9362 - val_loss: 0.8027 - val_acc: 0.8000\n",
      "Epoch 163/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2471 - acc: 0.9469 - val_loss: 0.8552 - val_acc: 0.7818\n",
      "Epoch 164/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2693 - acc: 0.9376 - val_loss: 0.8418 - val_acc: 0.7919\n",
      "Epoch 165/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9408Epoch 00165: val_loss did not improve\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2576 - acc: 0.9405 - val_loss: 0.8086 - val_acc: 0.8000\n",
      "Epoch 166/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2467 - acc: 0.9483 - val_loss: 0.8849 - val_acc: 0.7818\n",
      "Epoch 167/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2624 - acc: 0.9426 - val_loss: 0.8617 - val_acc: 0.7879\n",
      "Epoch 168/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2570 - acc: 0.9437 - val_loss: 0.8229 - val_acc: 0.7899\n",
      "Epoch 169/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2434 - acc: 0.9451 - val_loss: 0.7887 - val_acc: 0.8020\n",
      "Epoch 170/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9437Epoch 00170: val_loss did not improve\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2511 - acc: 0.9433 - val_loss: 0.8028 - val_acc: 0.8101\n",
      "Epoch 171/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2415 - acc: 0.9494 - val_loss: 0.8485 - val_acc: 0.7899\n",
      "Epoch 172/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2339 - acc: 0.9447 - val_loss: 0.8156 - val_acc: 0.8081\n",
      "Epoch 173/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2514 - acc: 0.9394 - val_loss: 0.7555 - val_acc: 0.8263\n",
      "Epoch 174/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2443 - acc: 0.9444 - val_loss: 0.7905 - val_acc: 0.8081\n",
      "Epoch 175/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9419Epoch 00175: val_loss did not improve\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2497 - acc: 0.9426 - val_loss: 0.7968 - val_acc: 0.8081\n",
      "Epoch 176/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2528 - acc: 0.9422 - val_loss: 0.8010 - val_acc: 0.8061\n",
      "Epoch 177/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2362 - acc: 0.9469 - val_loss: 0.8363 - val_acc: 0.7798\n",
      "Epoch 178/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2502 - acc: 0.9405 - val_loss: 0.8059 - val_acc: 0.8121\n",
      "Epoch 179/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2222 - acc: 0.9590 - val_loss: 0.7948 - val_acc: 0.8141\n",
      "Epoch 180/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9549- ETA: 1s - loss: 0.2299 - Epoch 00180: val_loss did not improve\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2391 - acc: 0.9544 - val_loss: 0.7844 - val_acc: 0.8000\n",
      "Epoch 181/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2355 - acc: 0.9504 - val_loss: 0.8232 - val_acc: 0.8040\n",
      "Epoch 182/3000\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2256 - acc: 0.9529 - val_loss: 0.8852 - val_acc: 0.7939\n",
      "Epoch 183/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2348 - acc: 0.9487 - val_loss: 0.8143 - val_acc: 0.8101\n",
      "Epoch 184/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2293 - acc: 0.9458 - val_loss: 0.8496 - val_acc: 0.8020\n",
      "Epoch 185/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9473Epoch 00185: val_loss did not improve\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2356 - acc: 0.9469 - val_loss: 0.8476 - val_acc: 0.8000\n",
      "Epoch 186/3000\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2271 - acc: 0.9483 - val_loss: 0.8489 - val_acc: 0.8040\n",
      "Epoch 187/3000\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2261 - acc: 0.9512 - val_loss: 0.8138 - val_acc: 0.8162\n",
      "Epoch 188/3000\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2207 - acc: 0.9497 - val_loss: 0.8144 - val_acc: 0.8061\n",
      "Epoch 189/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2172 - acc: 0.9501 - val_loss: 0.7896 - val_acc: 0.8182\n",
      "Epoch 190/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9499Epoch 00190: val_loss did not improve\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2193 - acc: 0.9497 - val_loss: 0.8508 - val_acc: 0.8000\n",
      "Epoch 191/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2173 - acc: 0.9529 - val_loss: 0.7973 - val_acc: 0.8121\n",
      "Epoch 192/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2039 - acc: 0.9583 - val_loss: 0.8512 - val_acc: 0.8081\n",
      "Epoch 193/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2285 - acc: 0.9490 - val_loss: 0.8855 - val_acc: 0.7838\n",
      "Epoch 194/3000\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2129 - acc: 0.9576 - val_loss: 0.8296 - val_acc: 0.8020\n",
      "Epoch 195/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9520Epoch 00195: val_loss did not improve\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2213 - acc: 0.9519 - val_loss: 0.8600 - val_acc: 0.8061\n",
      "Epoch 196/3000\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2077 - acc: 0.9583 - val_loss: 0.8812 - val_acc: 0.8000\n",
      "Epoch 197/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2040 - acc: 0.9547 - val_loss: 0.8624 - val_acc: 0.8040\n",
      "Epoch 198/3000\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2202 - acc: 0.9508 - val_loss: 0.8134 - val_acc: 0.8141\n",
      "Epoch 199/3000\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2192 - acc: 0.9540 - val_loss: 0.8130 - val_acc: 0.8121\n",
      "Epoch 200/3000\n",
      "2752/2805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9578Epoch 00200: val_loss did not improve\n",
      "2805/2805 [==============================] - 4s 2ms/step - loss: 0.2036 - acc: 0.9572 - val_loss: 0.7688 - val_acc: 0.8242\n",
      "Epoch 201/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.1959 - acc: 0.9547 - val_loss: 0.8780 - val_acc: 0.7859\n",
      "Epoch 202/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.1977 - acc: 0.9586 - val_loss: 0.8121 - val_acc: 0.8081\n",
      "Epoch 203/3000\n",
      "2805/2805 [==============================] - 4s 1ms/step - loss: 0.2215 - acc: 0.9497 - val_loss: 0.7790 - val_acc: 0.8222\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(patience=30, mode='auto', monitor='val_acc')\n",
    "checkPoint = ModelCheckpoint(filepath=\"top_weight.h5\", verbose=1, save_best_only=True, save_weights_only=True, period=5)\n",
    "Logs = CSVLogger('logs.csv', separator=',', append=True)\n",
    "idx = np.random.permutation(X_train.shape[0]) ## random shuffle\n",
    "history = model.fit(X_train[idx], y_train[idx], epochs=3000, batch_size=64, shuffle=False, validation_split=0.15, callbacks=[checkPoint, Logs, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efaac0a6ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xmc3HWd5/HXp44+qu8r3UnnDgkEQghJwEAYBkEZDkEdRHDBUZwd1lkVdHVmcZ0dnV13l1lnZ7zwgBFPBBVkYBREgmJUzhAgBwm5yNFJ+r7v6qrv/vH9JemE7tA5un6drvfz8cgj1b/6Vf0+9auq37u+39/v9/2Zcw4REclekbALEBGRcCkIRESynIJARCTLKQhERLKcgkBEJMspCEREspyCQOQozOx7ZvbFMc6708zecaLPI5JpCgIRkSynIBARyXIKAjnlBV0yf2Nm68ysx8y+Y2bVZva4mXWZ2SozKxs2/7VmttHM2s3saTNbOOy+c81sbfC4nwB5RyzrXWb2SvDYZ8xs8XHW/Fdmts3MWs3sUTObFkw3M/sXM2s0s04zW29mi4L7rjKz14La9prZZ45rhYkcQUEgk8V1wDuBBcA1wOPAfwOq8J/z2wDMbAFwP/DJ4L7HgH83sxwzywH+DfghUA78LHhegseeC9wL/CegAvg28KiZ5R5LoWZ2KfB/gPcDU4FdwAPB3ZcDFwevoySYpyW47zvAf3LOFQGLgN8cy3JFRqMgkMnia865BufcXuD3wPPOuZedc/3Aw8C5wXw3AL90zj3pnEsC/wTkAxcCK4A48GXnXNI59yDw4rBl3Ap82zn3vHMu5Zz7PjAQPO5Y3ATc65xb65wbAD4LXGBms4EkUAScAZhzbpNzbn/wuCRwppkVO+fanHNrj3G5IiNSEMhk0TDsdt8IfxcGt6fhf4ED4JxLA3uA2uC+ve7wkRh3Dbs9C/h00C3UbmbtwIzgccfiyBq68b/6a51zvwG+DtwFNJrZ3WZWHMx6HXAVsMvMfmdmFxzjckVGpCCQbLMPv0EHfJ88fmO+F9gP1AbTDpg57PYe4H8550qH/Us45+4/wRoK8F1NewGcc191zi0DzsR3Ef1NMP1F59y7gSn4LqyfHuNyRUakIJBs81PgajO7zMziwKfx3TvPAM8CQ8BtZhY3sz8Hzh/22HuAj5rZ24KdugVmdrWZFR1jDfcDt5jZkmD/wv/Gd2XtNLPzguePAz1AP5AO9mHcZGYlQZdWJ5A+gfUgcpCCQLKKc+514Gbga0AzfsfyNc65QefcIPDnwIeBVvz+hJ8Pe+wa4K/wXTdtwLZg3mOtYRXw34GH8K2QecCNwd3F+MBpw3cftQBfCu77ILDTzDqBj+L3NYicMNOFaUREsptaBCIiWU5BICKS5RQEIiJZTkEgIpLlYmEXMBaVlZVu9uzZYZchInJKeemll5qdc1VvNd8pEQSzZ89mzZo1YZchInJKMbNdbz2XuoZERLLeuAWBmd0bDKW7Ydi0cjN70sy2Bv+XHe05RERk/I1ni+B7wBVHTLsDeMo5Nx94KvhbRERCNG77CJxzq4NhdYd7N3BJcPv7wNPAfx2vGkQkeyWTSerq6ujv7w+7lHGXl5fH9OnTicfjx/X4TO8srh42tno9UD3ajGZ2K378d2bOnDnabCIiI6qrq6OoqIjZs2dz+ICyk4tzjpaWFurq6pgzZ85xPUdoO4uDMd9HHejIOXe3c265c255VdVbHv0kInKY/v5+KioqJnUIAJgZFRUVJ9TyyXQQNJjZVIDg/8YML19EsshkD4EDTvR1ZjoIHgU+FNz+EPDIeC7s4Zfr+NFzYzqMVkQka43n4aP34y/0cbqZ1ZnZXwJ3Au80s63AO4K/x80v19Vz3/O7x3MRIiIjam9v5xvf+MYxP+6qq66ivb19HCoa3bgFgXPuA865qc65uHNuunPuO865FufcZc65+c65dzjnWsdr+QCliTidfcnxXISIyIhGC4KhoaGjPu6xxx6jtLR0vMoa0SkxxMTxKsmP0947GHYZIpKF7rjjDrZv386SJUuIx+Pk5eVRVlbG5s2b2bJlC+95z3vYs2cP/f393H777dx6663AoSF1uru7ufLKK7nooot45plnqK2t5ZFHHiE/P/+k1zqpg6A0P07PYIrBoTQ5MY2mIZKt/uHfN/Lavs6T+pxnTivm89ecNer9d955Jxs2bOCVV17h6aef5uqrr2bDhg0HD/G89957KS8vp6+vj/POO4/rrruOioqKw55j69at3H///dxzzz28//3v56GHHuLmm28+qa8DJvlYQ6UJf3JFh7qHRCRk559//mHH+X/1q1/lnHPOYcWKFezZs4etW7e+6TFz5sxhyZIlACxbtoydO3eOS22TukVQksgBoKNvkKqi3JCrEZGwHO2Xe6YUFBQcvP3000+zatUqnn32WRKJBJdccsmI5wHk5h7abkWjUfr6+saltsndIsj3LYL2XrUIRCSzioqK6OrqGvG+jo4OysrKSCQSbN68meeeey7D1R1uUrcIDnQNKQhEJNMqKipYuXIlixYtIj8/n+rqQyPqXHHFFXzrW99i4cKFnH766axYsSLESid7EOT7rqF27SMQkRD8+Mc/HnF6bm4ujz/++Ij3HdgPUFlZyYYNB0fx5zOf+cxJr++ASd01VHKwRaBDSEVERjOpg6AoN0bEdNSQiMjRTOogiEQsOKlMQSAiMppJHQS89D3+Kva49hGIiBzF5A6CLU9wZeq32kcgInIUkzsIEuWU0KV9BCIiRzG5gyC/nKJ0B+09ahGIyMRWWFgIwL59+3jf+9434jyXXHIJa9asOenLntxBkCgn7pIM9I18dp+IyEQzbdo0HnzwwYwuc3IHQX45ANH+dlLpUS+PLCJy0t1xxx3cddddB//+whe+wBe/+EUuu+wyli5dytlnn80jj7z5Io07d+5k0aJFAPT19XHjjTeycOFC3vve947bWEOT+sxiEj4Iyqybzr4kZQU5IRckIqF4/A6oX39yn7PmbLhy9Iss3nDDDXzyk5/kYx/7GAA//elPeeKJJ7jtttsoLi6mubmZFStWcO211456zeFvfvObJBIJNm3axLp161i6dOnJfQ2BSR4EfmzvUuuiXUEgIhl07rnn0tjYyL59+2hqaqKsrIyamho+9alPsXr1aiKRCHv37qWhoYGampoRn2P16tXcdtttACxevJjFixePS62TOwiCrqFyuoJDSAuOPr+ITE5H+eU+nq6//noefPBB6uvrueGGG7jvvvtoamripZdeIh6PM3v27BGHn860yb2PIOgaKrVunV0sIhl3ww038MADD/Dggw9y/fXX09HRwZQpU4jH4/z2t79l165dR338xRdffHDgug0bNrBu3bpxqXOStwjKACijmzadVCYiGXbWWWfR1dVFbW0tU6dO5aabbuKaa67h7LPPZvny5ZxxxhlHffxf//Vfc8stt7Bw4UIWLlzIsmXLxqXOyR0E0Tgup4iyoS5adS6BiIRg/fpDO6krKyt59tlnR5yvu7sb8BevPzD8dH5+Pg888MC41zi5u4YACiooN7UIRERGM+mDwPLLqYr10NqjfQQiIiOZ9EFAopyKSDdt6hoSyTrOZceJpCf6Oid/EOSXU0o3reoaEskqeXl5tLS0TPowcM7R0tJCXl7ecT/H5N5ZDJAop9h1qUUgkmWmT59OXV0dTU1NYZcy7vLy8pg+ffpxPz4LgqCC/HQPnd09YVciIhkUj8eZM2dO2GWcErKga8ifS+D62khr4DkRkTeZ/EEQnF1cQjed/TpySETkSJM/CILxhsrQSWUiIiOZ/EEQjEBaZl06qUxEZAShBIGZfcrMNprZBjO738yO/7int1JQCUCFdemkMhGREWQ8CMysFrgNWO6cWwREgRvHbYGJIAjooLVnYNwWIyJyqgqraygG5JtZDEgA+8ZvSTm43GLK1SIQERlRxoPAObcX+CdgN7Af6HDO/frI+czsVjNbY2ZrTviEkIJKpkQ6tY9ARGQEYXQNlQHvBuYA04ACM7v5yPmcc3c755Y755ZXVVWd2DILqqiOduuoIRGREYTRNfQO4A3nXJNzLgn8HLhwXJeYqKQiomEmRERGEkYQ7AZWmFnCzAy4DNg0rkssqKTMddCiIBAReZMw9hE8DzwIrAXWBzXcPa4LLaikKN1Ba3ffuC5GRORUFMqgc865zwOfz9gCE5VESZPsbsvYIkVEThWT/8xigAK/szkx1EbPwFDIxYiITCxZEgR+mIlyumju1kllIiLDZUcQHDi72DoVBCIiR8iOIAi6hiqsk6YuBYGIyHDZEQTBCKQVdNLUrUNIRUSGy44giOXg8koot06a1SIQETlMdgQBYIlKpsa6tY9AROQIWRMEFFRSHdVRQyIiR8qiIKgKjhrSPgIRkeGyJwgKqylPt6lFICJyhOwJgqIaCtKddHR1h12JiMiEkj1BUDgFgILBFnoHNcyEiMgBWRQENQBUWQfNXdpPICJyQPYEQVE1AFOsjSbtJxAROSh7giBoEUyxdg0zISIyTPYEQUEVDqPK2mnq6g+7GhGRCSN7giAag4Iqaqydhk61CEREDsieIACsqJraWBcNnWoRiIgckFVBQGE1NdF2GrSPQETkoCwLghoqXBsNHWoRiIgckF1BUFRNcaqdps6esCsREZkwsisICmuIkiLS10Z/MhV2NSIiE0J2BUFwUlmVziUQETkou4Jg2EllOnJIRMTLriAYNsxEvYJARATIuiCYCkANrTqpTEQkkF1BEMvFJSqYFm2nUS0CEREg24IAsKJpzIxpH4GIyAFZFwQUT2VqpE1dQyIigewLgqKpVLpW7SwWEQlkXxAUT6M41UZTexfOubCrEREJXfYFQXDkUMlQC229yZCLEREJXyhBYGalZvagmW02s01mdkHGFh4EQbW1sa+9L2OLFRGZqMJqEXwF+JVz7gzgHGBTxpZcHJxLYK3s1yikIiLEMr1AMysBLgY+DOCcGwQGM1ZA0TTAtwj2d6hFICISRotgDtAEfNfMXjazfzWzgiNnMrNbzWyNma1pamo6eUtPlOOiuUyLtLFXXUMiIqEEQQxYCnzTOXcu0APcceRMzrm7nXPLnXPLq6qqTt7SzbCiGmbldLK/XV1DIiJhBEEdUOecez74+0F8MGRO8TRqo+oaEhGBEILAOVcP7DGz04NJlwGvZbSI4mlUuxb2qUUgIpL5ncWBTwD3mVkOsAO4JaNLL5lO6VATjT29pNKOaMQyungRkYkklCBwzr0CLA9j2QCUzCDmkpSl22nuHqC6OC+0UkREwpZ9ZxYDlM4EYLo16aQyEcl62RkEJTMAqLVm7ScQkayXnUFQeigI9rT1hlyMiEi4sjMIcosgr5S58Vb2tCoIRCS7ZWcQAJTMYE68jT1t2kcgItkte4OgdAbTrJk6tQhEJMtlbxCUzKByqIG6tl7SaV2gRkSyV/YGQekMctO95KW6aOzS9YtFJHtlbxAEh5BO15FDIpLlxhQEZna7mRWb9x0zW2tml493ceNq+CGk2k8gIllsrC2CjzjnOoHLgTLgg8Cd41ZVJpTNAWCmNbCnVUcOiUj2GmsQHBiV7Srgh865jcOmnZoS5ZBXypm5TeoaEpGsNtYgeMnMfo0PgifMrAhIj19ZGVI+l9NiTeoaEpGsNtbRR/8SWALscM71mlk5mR46ejxUzGN60+/ZrSAQkSw21hbBBcDrzrl2M7sZ+DugY/zKypDyuZQlG2np6KJvMBV2NSIioRhrEHwT6DWzc4BPA9uBH4xbVZlSPo8IaWZYIztbesKuRkQkFGMNgiHnnAPeDXzdOXcXUDR+ZWVI+VwAZlkDO5sVBCKSnca6j6DLzD6LP2z0T8wsAsTHr6wMCYJgjtWzQ0EgIllqrC2CG4AB/PkE9cB04EvjVlWmJMohr4Qzcpt5Q0EgIllqTEEQbPzvA0rM7F1Av3Pu1N9HYAblc1kQa1TXkIhkrbEOMfF+4AXgeuD9wPNm9r7xLCxjyucx0+1Ti0BEstZY9xF8DjjPOdcIYGZVwCrgwfEqLGMq51O64SG6+7vp6E1Skjj1d32IiByLse4jiBwIgUDLMTx2Yqucj+GYY/W8oUNIRSQLjbVF8CszewK4P/j7BuCx8SkpwyrmAzDX9rGjqZslM0pDLkhEJLPGFATOub8xs+uAlcGku51zD49fWRlUMQ+A+ZH9bGvsDrkYEZHMG2uLAOfcQ8BD41hLOHIKoGQGZ/c28lMFgYhkoaMGgZl1ASNd0NcA55wrHpeqMq3iNOb371WLQESy0lGDwDl36g8jMRaVC5i683l2dnYzMJQiNxYNuyIRkYyZHEf+nKjK+eSke6lybexs1pDUIpJdFAQAlcGRQ5H9bG3sCrkYEZHMUhAAVJ4OwOlWx9YG7ScQkeyiIAAoqoFEBcvz97KtSUEgItkltCAws6iZvWxmvwirhmHFQPUizorsZku9uoZEJLuE2SK4HdgU4vIPV3M2M4Z2sbOpQ5etFJGsEkoQmNl04GrgX8NY/oiqzyKWHmAm9by2/9S/HLOIyFiF1SL4MvC3QDqk5b9Z9SIAzrRdrKtTEIhI9sh4EAQXtml0zr30FvPdamZrzGxNU1PT+BdWdTpEYizN26cgEJGsEkaLYCVwrZntBB4ALjWzHx05k3Pubufccufc8qqqqvGvKpYLlaezLHcv6+rax395IiITRMaDwDn3WefcdOfcbOBG4DfOuZszXceIas5mbmo7O5p76OpPhl2NiEhG6DyC4aadS+FgM1WujY37OsOuRkQkI0INAufc0865d4VZw2GmnQvA4sgO1ms/gYhkCbUIhqs5GyzKyvzdrNurIBCR7KAgGC4nAVMWcl7OLtZrh7GIZAkFwZGmLWHe0BZ2tvTQ0asdxiIy+SkIjjTtXPKT7dTSzIZ96h4SkclPQXCkaUsBOCeyXSeWiUhWUBAcqXoRxPK5JPEG6/dqP4GITH4KgiPFcmD6ct4WfZ1XdisIRGTyUxCMZOYKZgxso6Ojjb3tfWFXIyIyrhQEI5m5gghplkS28eIbrWFXIyIyrhQEI5l+Ps4irIxv5cWdCgIRmdwUBCPJK8aqF/GnedsUBCIy6SkIRjPrQhYkN7GzoY22nsGwqxERGTcKgtHMuZh4eoAlplaBiExuCoLRzFqJswhvz9nEb1/PwBXSRERCoiAYTX4pNnUJ78x/nVWbGkinXdgViYiMCwXB0cy5mLkDm+ju6tCw1CIyaSkIjmbOxUTcEBdEX2fVaw1hVyMiMi4UBEcz60KI5XNDyWs8qSAQkUlKQXA08XyYdykrUy/wekMnu1t6w65IROSkUxC8lTOupnCggUX2Bk9uUqtARCYfBcFbWXAFWIQPFK/XfgIRmZQUBG+loAJmXsDlkTW8sLNVl68UkUlHQTAWZ76Hqr7tzHe7WKXuIRGZZBQEY7HoOlwkxocKnuOhtXVhVyMiclIpCMaioAKbfznXRP7Ac9ub2NOqo4dEZPJQEIzV4hsoHGzmTyLr+fnavWFXIyJy0igIxmrBFZBfzsdK/shP1+xhKJUOuyIRkZNCQTBW8Tw49yaW9z/LYPs+HttQH3ZFIiInhYLgWCy7hYhL8dHiZ/jW09txTiOSisipT0FwLCrmwdxL+EBkFVv2t/KHbc1hVyQicsIUBMdqxX8m0d/AjflruP+F3WFXIyJywhQEx+q0d0LVQj6R90uefK2elu6BsCsSETkhCoJjFYnAytup7tvORe5lHn5Zh5KKyKkt40FgZjPM7Ldm9pqZbTSz2zNdwwk7+31QOou/SzzMj5/fpUNJReSUFkaLYAj4tHPuTGAF8DEzOzOEOo5fNA6X3MG8oW0saH2a+57XvgIROXVlPAicc/udc2uD213AJqA203WcsMU34CoX8PeJB/nyrzfSrH0FInKKCnUfgZnNBs4Fnh/hvlvNbI2ZrWlqasp0aW8tEsUu/yLThuq4OfUIdzy0TucViMgpKbQgMLNC4CHgk865ziPvd87d7Zxb7pxbXlVVlfkCx2LBn8GZ7+b2+L+xbfOrfOcPb4RdkYjIMQslCMwsjg+B+5xzPw+jhpPmin8kmpPPj4q+zlcef4WXd7eFXZGIyDEJ46ghA74DbHLO/XOml3/SFU/F3ncvtcldfCX/Hj5+31pdxUxETilhtAhWAh8ELjWzV4J/V4VQx8lz2mXYO/6BS1PP8N7en/CZB1/V/gIROWXEMr1A59wfAMv0csfdhZ+A/a/y6Q0/5aObp/LdP1bwkYvmhF2ViMhb0pnFJ4sZXPs1mL6cu3K+xnOP/5BVr+n6xiIy8SkITqacBHbzQ9jUc7gr/hV+dv89PL5+f9hViYgclYLgZMsrIfoXD2PVi/h69F/45f13cfsDL9OfTIVdmYjIiBQE4yG/lNiH/43ojGV8PedrnLPhTm699w909utoIhGZeBQE4yW/jMiHfgFv+ygfif2Kz+/7KJ/6p3t4bP1+HVEkIhOKgmA8xXLgyn+ED/4bMwqNu4c+x/6ffIrbv/Mke1p7w65ORARQEGTGvLeT84nnsKV/wUdiv+LOPTfzyJc/zt1PbSSpIaxFJGQKgkzJKyZy7Vewj72ALfgzPh55iKtWX8s9X/pbntm4Q91FIhIaOxU2QMuXL3dr1qwJu4yTa8fvaHvsC5Q1r2XAxVibs5z6me9i0aU3Mr92gg6yJyKnFDN7yTm3/C3nUxCEa3D3GravupeauscoS7fR4op4tfJdzLrgOuYtvRQi0bBLFJFTlILgVJNO0bXpKfY9+VXmtf2RmKVpoox1JW9n6pkrOWPF1URKpoZdpYicQhQEp7DO9mZeffrnlG17mPndL5JLkjRGY8k5lCy9jvwz3gFVp6u1ICJHpSCYJAYHBvjjs7+n4cWHOadrNQsj/vrIA9EChqqXkDf7fCIzlmHVi6B0FkS0/19EPAXBJLRxXwdP/P450rueo7prPefYNhbabuLmh69IxQqIVi+E8rlQPgfKZvtwKJ0JxdPUghDJMgqCSa6le4BVmxpo6+gktW89LW+8zIzkTs5LNFCT2k95qokIw85RiMSgZAbMuxSW3wJFUyG/XC0IkUlMQZBlegeHuHv1Dn61oZ5oxKhv7aR4YD+11sysSBNLCts5r6iNWa2/x1KD/kHF0+H0KyCvFAY6YaAbahZBohJSg5BOQm6xD5DSGdBRBy3bYP7lUFAZ7gsWyQTn/BDzYT/HcVIQZDnnHHVtfWzc18HGfZ2s3tLEq3UdTLEO/kPFFqpzBlgZWc+MjrUw1Ae5RVgsD7rHcA2FaA7MuRgq5vtg6G0Gi0LZLJh2LsxYAYPd/l+yHzp2Q9tO6GmBuX8KZ74HiqrHfR1IBrS+AakkVC04tsc1b4OS6RDP83/Xr4fBHpi5YvTHtGyH/a/ClIVQdQbsfQnuu953gS66DpZ9GHILD83f3+k/m+Vzj/VVeXtegJ/d4n8sXfklqF8H3Y1+o24R/1lPlB/+mIaNvrWdKIfdz8Pq/wt1L8K7vuy7aV+9H7b/xtd52jvgwtsgv/TQ43f+EX7zP/3QNFPPOb66h1EQyJts2t/J4xvqWbOzlcauAbY1dpMbizCYSlOYE2N2ZQG18S6W18R5z/LZVJYUQl8bqbbdRDvrIFHhWwbrfgZvrIbW7T4Mimp866Flh9/oj6RgCuQUQNsb/ks0dQn0NPl/FvEf+nmXwRlXQ3e9/8K3bIeZF8C0JdDbAjueht5W/0VrWA/pNCy4HBZcAbFc2PokbP6lD6pL7oDXH4O6NZBfBguv8d1iJ/rLrK/Nh1vxVP9LLzXol93TAv3tUDHP397wkN8AnHEVnPXeQ4/vboI19/ov/5yLIZ7w+27ySiC36K2Xn+yHaNw/JpWEZJ/v9otE/es+8vW17/HvU3ej35CVzIRzb/LvRX8n7PojzLrQLz+dgn0v+zqqTj/8eZyD9t3QvgtmXgjRGPS1wzdW+PdwxX+G3c+BS8PK2/x72tfu103tMr+xTvb5DeDaH8Cjn4B4Acy+yL8/63/qX8dHnoDapdC5D1Z/yf+fU+Af/+w3/I8WgOpF0LHH152o8HUnKvyVAhdeC7uegaf+wdc2ayVccSdMXQyDvf7HTl+rb+G+cA80vQ5nXw9zL/HvX2E1vPRdeOp/Qk7Cv+clM9/82Y4XwKI/9+ur5my/rFVf8C3q06/0r7Og0n8/6tf7x8Ty/fs+2AO7n4HCGv8c+WW+hu9d7V9XThFc82X/2TmBfXsKAnlLz25v4YmN9RTnxWjvS7KrpZeOviSv1rXjHEwryWNgKE1HX5KL5ldy3uxyphTlsmRGKfOqColERtiodtT5L2V+me9WiuVCce2hX2qNm2Hjz32QlEz3G/XUoN9o7n3p8OfKLYGBjkN/WxTyiv0Xs3QmOA59OS0KLuW/xANdkAwG9SubDb1t/nmKa/0vyWQfDPX7jXFHne8SK53pH5NbDEv/wneV9bbAlDN9vbue8cvo2uefd9ZK/4Vt33Oo28ylYe7bYd9a6O/wzzXQ6b/MuUV+nt3PQ7Lnzestmgtv/6z/VTvYC1se9xvW7kY4411+A7j1Sdj9rH+NM1f4vwc6Dz1HTpHf6CT7/AYyluPfiwMicR/YeaUw7+3+12dPow+jsjnBugjW99QlfhnxhG/N7XkeOvf6++b8qd9I/f6f4ZX7/IZtx9P+OVzah8VoZl4IdS/4gK+c79dr6w445wP+l7KZbzG+/EP/OioX+HXQXe9/KFzyWR9oL9zt368P/9K/d3tehN/dCdtWHVrWtKU+iF+4x4fewmtg06P+vT+gaKqfb+sTkB46vNbT3gnv/Tas/R68cj+c9x9h+nIfislev6Hf+qR/3IH3dMGVfn01bYKlH4Ir/o8P6Bfu8e/H4hsOBf7el+DfPwnNW/37a8H+uuu/50Owfr1fpzf+GKrPHH2dHoWCQI7bzuYefrl+P9sau8mJRkjkRlm1qYE9rX0H5ynJj7OotphU2lGYG2dBdSFvP2MKU0vy6BlIMX/KKEFxNO17/AaldKb/hZVf5jeq7bv9RrV2md94D/b6X2rOQeNr8Prj/ou54Eo/T8duWPNd/wtv3tthaAA2/By2Pem7snKKfED1tfnBDbGFAAAPeklEQVSjqXKL/UY9noCWrX7DNFxuMZx+lf+iVsz1LZEND/musJrFh1pGkajfQNUshsu/6H9VP/l5ePlHvgukeJr/Jbvydv9ce9f6DXN6CLY8AZt/cfhyi6b6jUbzFv939SL/ehpeg71r/OutPssHYHrItza66/3r6O/wv8jnvxOmn+d/mVac5ltIL30XdvzO13/Bx/wGuLvRL2/2Suiqh9cegfoNkBqAomkwfZnfiLs0PPn3fjr4ro13/g+/0Zpypr9/52p/IEKiwr8vu57xz58eglcf8GF+y+OHukQO9KHveRF+dJ3fKE4/D675KlSe5u/vbvABeKDF45xf1pG/luvW+Fqqz/KfhUjUL/snH/Qb3nNu9C2g/DJf47Qlhz4LTa/7Vmj7Lt9SmXPx2D63zvn3smu//5ykBvzGferiMX7w8TWv+ge//i/6lG+dbf6FD5v3/8C3io6DgkBOuv5kirq2PtbubuOlnW1sbugiNxqhvW+QHU09DKUPfZbmVBZwwbwKcqIRYhGjpiSPc2aUUl2UR1lBnMLcGBbSDrSjSqf8r9+iqX5DVr/Ob4CH9+OOB+dgx2/9Rh78BvxA90zTFr8hKKkd3xqOlA6OOjvyyLKm133LxKKw+P1+Q3osz+nSvmtpJKkhv/E+2Z+NdMq3nvLLTu7zTnAKAsmorv4kT7/eRFf/EBGDh9bW8UZzD8mUI5lK0zt4+KU68+IR3jangrNrS0jkRkmlHKWJOGfVltA7kKKmJI/TphSOsjQRGYuxBsEosSxybIry4lxzzrSDf994/szD7m/s7GfDvg6auwdp7x1kb1sfv9/azOqtTYz2W+TyM6uZW1VI2jl6BoboGRhiSnEe7z23ltOriw52PfUMDDE4lKasIGfcXp/IZKYgkIyYUpzHpcV5b5runGNgKE0sYuzv6GfT/k6K8uI8s72Z+1/YzdNbmogYFObGSOTE2N/Rx92rdxCP+u6m0vwcNtd3kkw5zqgpYuVplZxe7XfGzShPMKM8n4gZ6+raAXjHwmpiUZ1EJzKcuobklNLSPcCTrzWwu7WXve19NHcPsKi2hOIgPF7c2cbg0OhXfastzae62PdplyVy2N/RT18yxdzKAuZNKaQskUN73yC5sSgl+XFK8uOU5scpScQpS8SZXVGgIJFThvYRSFbqT6Zo6vJHtOxo7qG+o4+htG8ttHQPcv8Lu0mmHGnnaO0ZpKYkj0ROlB1NPexo7mFwKE1ONEIynR6xy6o0Eedtc8qpLs6jsjCXjr4kv329kcqCXJbNLmPZzDKiUWNwKE1taT61pfmUJuL0JVPkxaJEIsZQKk00YhNzZ7lMKgoCkWOUSjv6kykSOVHSzu8A7+hL0t7r/2/qGuCP25p5pa6dlu5BOvqSxKPGirkVdPYPsXFvx2FHTh1g5g8KikWMgtwYHX1JakvzOWtaMUNpRzxq5MWjDKUcJYk4VYW5pJ1jSnEeC2uKWFBTRGv3ILtae6kqzCU/J0puLMLUkryjholzTmGT5bSzWOQYRYMNNUDUoDSRQ2kih1kVh+a5btn0g7cHh9KknSMv7o9l7xtMsXFfB2ZGPGrsa++nrs2fpJfIidHZn6S7f4iyghy2NXaxtaGb3HiE5JCjL5kiHjXaepO09gyOqd6pJXmU5Mdp703S3jdIxIyivBhFeXE6+5I0dg1QkBPl7OklXL14GsV5MZq7B2ns6qemOI+Z5QlmlieYUZ6ge2CILQ1dtPYMksiJUluaoLVnkJxYhFkVCfoGU5QX5BxcPzK5qEUgMsGk0w4z2N/Rz+b6TjbXd1GWyGFuZQGtPYMHz/Z+cWcrA0NpSvPjlCbiOAed/Um6+ocozI1RU5JHV/8QT7/eyM6W3oPPH48aydSxf+8TOVEuPWMKdW19dPYng+Xm0J9MUd/ZT1FujIrCXEoTcQxjKJ2mP5nijeYe4tEIF8ytoC+Zoi+ZojA3RmFujJL8OFOKc5lSlEc0YrR0++66WRUJSvPj1Hf2s6Oph33tfSydVUZNSR5rdrZSkp/DvKoCShM5tHQP0Ng1QFkihylFuWM+kTEbWkzqGhIRwG/w9rT2MZhKU5aIU16QQ2vPILtbe9nd2sue1l7y4lHOqCmmqiiX7oEke9v7qSjwG/k9rb0kcmK8uLOV32xuZF5VIVVFubT3DdLWkyQnFmFaqT+jvKVngLaeJOADJx71LYrO/iHW7mqjKM8f/dUdHA48Ulfa0UQjRmrYY4pyY3QNHBoaoiAnSmVRLi3dgxTnxZhWmk9VUS5bG7vZ395HRWEuqbSjsz9Jz8AQsyoKqCnOY3tTNwunFrN8Vhmv7fdHoZXkx5lelk/v4BAdfUmK8/y+nv5k+mBrLD8nSiInSiInRn5OlAOxkhuLMLeqkM31nby2r5PTa4qIRoyegSFOm1JITUk+qbRjfV0Hnf1J8nOinDWtmClFh46s29fex1ObG7np/JnHfpZ+QEEgIhNKOu0O26A55+geGKKxa4DGzgFSaUd5QQ772vuoa+ulvS9JdXEecyoLmFKUy+otTTR3D3LhaRX0J1Nsb+xhV2sPM8sTviurd5BtDV209AxSWZhLZ3+Sfe19NHQOMKsiweyKAtp6B4lFIhTl+Q33tsZuGrsGmFtZwPM7WtjX0c+sigSJnBitPQM0dA6QG4tQmojT2Td0cP9MY9fAYYF0shTlxagszCWZSlPX5od0efTjK1k8/fjObFcQiIgcg1TaB1NJfvzgtMGhNPHom4/wSqUdvYND9A6mgn9D9CcPnT3fPZBia0MX08sSLJ9dxtaGbiIGiZwYWxq6aOkZIO1g0bQS37rqHWT93g52t/bS0jNIPGLMry7iykU1zK06/jPsJ3QQmNkVwFeAKPCvzrk7jza/gkBE5NiNNQgyfmaMmUWBu4ArgTOBD5jZ8Y2xKiIiJyyMUyTPB7Y553Y45waBB4B3h1CHiIgQThDUAnuG/V0XTDuMmd1qZmvMbE1TU1PGihMRyTYTdtAU59zdzrnlzrnlVVVVYZcjIjJphREEe4EZw/6eHkwTEZEQhBEELwLzzWyOmeUANwKPhlCHiIgQwlhDzrkhM/s48AT+8NF7nXMbM12HiIh4oYwg5Zx7DHgsjGWLiMjhTokzi82sCdh1nA+vBJpPYjkny0SsayLWBKrrWEzEmkB1HYuTWdMs59xbHm1zSgTBiTCzNWM5sy7TJmJdE7EmUF3HYiLWBKrrWIRR04Q9fFRERDJDQSAikuWyIQjuDruAUUzEuiZiTaC6jsVErAlU17HIeE2Tfh+BiIgcXTa0CERE5CgUBCIiWW5SB4GZXWFmr5vZNjO7I6QaZpjZb83sNTPbaGa3B9O/YGZ7zeyV4N9VIdS208zWB8tfE0wrN7MnzWxr8H9ZBus5fdj6eMXMOs3sk2GsKzO718wazWzDsGkjrhvzvhp8ztaZ2dIM1/UlM9scLPthMysNps82s75h6+1bGa5r1PfNzD4brK/XzezPMljTT4bVs9PMXgmmZ3JdjbZNCO/z5ZyblP/ww1dsB+YCOcCrwJkh1DEVWBrcLgK24C/I8wXgMyGvo51A5RHT/i9wR3D7DuAfQ3z/6oFZYawr4GJgKbDhrdYNcBXwOGDACuD5DNd1ORALbv/jsLpmD58vhPU14vsWfP5fBXKBOcH3NJqJmo64//8Bfx/CuhptmxDa52sytwgmxAVwnHP7nXNrg9tdwCZGuP7CBPJu4PvB7e8D7wmpjsuA7c654z2j/IQ451YDrUdMHm3dvBv4gfOeA0rNbGqm6nLO/do5NxT8+Rx+RN+MGmV9jebdwAPOuQHn3BvANvz3NWM1mZkB7wfuP9nLfStH2SaE9vmazEEwpgvgZJKZzQbOBZ4PJn08aOrdm8kumGEc8Gsze8nMbg2mVTvn9ge364HqEOoCPyrt8C9p2OsKRl83E+mz9hH8r8cD5pjZy2b2OzP7kxDqGel9mwjr60+ABufc1mHTMr6ujtgmhPb5msxBMKGYWSHwEPBJ51wn8E1gHrAE2I9vpmbaRc65pfjrR3/MzC4efqfz7dKMH19sfnjya4GfBZMmwro6TFjr5mjM7HPAEHBfMGk/MNM5dy7wX4Afm1lxBkuacO/bMB/g8B8aGV9XI2wTDsr052syB8GEuQCOmcXxb/h9zrmfAzjnGpxzKedcGriHcWgavxXn3N7g/0bg4aCGhgPNzuD/xkzXhQ+mtc65hqC+0NdVYLR1E/pnzcw+DLwLuCnYiBB0vbQEt1/C98UvyFRNR3nfQl1fZhYD/hz4ybBaM7quRtomEOLnazIHwYS4AE7QF/kdYJNz7p+HTR/ex/deYMORjx3nugrMrOjAbfwOxw34dfShYLYPAY9ksq7AYb/Wwl5Xw4y2bh4F/iI4umMF0DGsiT/uzOwK4G+Ba51zvcOmV5lZNLg9F5gP7MhgXaO9b48CN5pZrpnNCep6IVN1Ae8ANjvn6g5MyOS6Gm2bQJifr0zsJQ/rH35v+xZ8un8upBouwjfx1gGvBP+uAn4IrA+mPwpMzXBdc/FHbrwKbDywfoAK4ClgK7AKKM9wXQVAC1AybFrG1xU+iPYDSXyf7F+Otm7wR3PcFXzO1gPLM1zXNnwf8oHP17eCea8L3ttXgLXANRmua9T3DfhcsL5eB67MVE3B9O8BHz1i3kyuq9G2CaF9vjTEhIhIlpvMXUMiIjIGCgIRkSynIBARyXIKAhGRLKcgEBHJcgoCkXFmZpeY2S/CrkNkNAoCEZEspyAQCZjZzWb2QjAe/bfNLGpm3Wb2L8G48U+ZWVUw7xIze84OXQPgwNjxp5nZKjN71czWmtm84OkLzexB89cNuC84u1RkQlAQiABmthC4AVjpnFsCpICb8Gc6r3HOnQX8Dvh88JAfAP/VObcYf7bngen3AXc5584BLsSf2Qp+hMlP4sednwusHPcXJTJGsbALEJkgLgOWAS8GP9bz8YN+pTk0ONmPgJ+bWQlQ6pz7XTD9+8DPgrGbap1zDwM45/oBgud7wQVj25i/KtZs4A/j/7JE3pqCQMQz4PvOuc8eNtHsvx8x3/GOyTIw7HYKffdkAlHXkIj3FPA+M5sCB68fOwv/HXlfMM9/AP7gnOsA2oZdvOSDwO+cv9pUnZm9J3iOXDNLZPRViBwH/SoRAZxzr5nZ3+Gv2BbBj1j5MaAHOD+4rxG/HwH8MMHfCjb0O4BbgukfBL5tZv8jeI7rM/gyRI6LRh8VOQoz63bOFYZdh8h4UteQiEiWU4tARCTLqUUgIpLlFAQiIllOQSAikuUUBCIiWU5BICKS5f4/bFa0opkRVzkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efa0c5dc190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting 了 QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790/790 [==============================] - 1s 1ms/step\n",
      "loss: 0.88\n",
      "acc: 81.39%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print 'loss: %.2f'%(loss)\n",
    "print 'acc: %.2f%%'%(acc*100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54  2  0  1  0  0  6  0  0  2  0  0]\n",
      " [ 4 57  2  3  0  0  2  1  0  0  2  0]\n",
      " [ 3  3 61  0  0  0  0  0  2  0  1  6]\n",
      " [ 0  1  0 53  4  0  4  0  0  3  0  0]\n",
      " [ 0  0  0  4 57  3  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  4 55  0  0  3  0  0  4]\n",
      " [ 0  0  0  2  0  0 50  6  2  2  0  0]\n",
      " [ 0  4  0  0  0  0  4 57 11  0  0  0]\n",
      " [ 0  0  3  0  0  4  0  1 52  0  0  1]\n",
      " [ 5  0  0  5  0  0  6  0  0 46  3  3]\n",
      " [ 0  0  0  1  0  0  0  1  0  3 45  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  3 56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.83      0.82        65\n",
      "          1       0.85      0.80      0.83        71\n",
      "          2       0.92      0.80      0.86        76\n",
      "          3       0.77      0.82      0.79        65\n",
      "          4       0.88      0.88      0.88        65\n",
      "          5       0.89      0.83      0.86        66\n",
      "          6       0.68      0.81      0.74        62\n",
      "          7       0.86      0.75      0.80        76\n",
      "          8       0.74      0.85      0.79        61\n",
      "          9       0.82      0.68      0.74        68\n",
      "         10       0.83      0.80      0.82        56\n",
      "         11       0.74      0.95      0.83        59\n",
      "\n",
      "avg / total       0.82      0.81      0.81       790\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efa0c483050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "real = np.argmax(y_test, axis=1)\n",
    "confuse = np.asarray(confusion_matrix(real,pred))\n",
    "print confuse\n",
    "print classification_report(real,pred)\n",
    "confuse = confuse / float(confuse.sum())\n",
    "plt.imshow(confuse, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
